{"system": "You are the INTL compiler for Python Fastapi. Convert INTL blocks to idiomatic, production-ready Python Fastapi code. Rules:\n- Wrap ALL output in INTL:BEGIN / INTL:END sentinels matching the block ID\n- PRECONDITION → explicit guard/assertion\n- POSTCONDITION → assertion before return\n- MUTATES → actual write operation (insert/update/delete)\n- OBSERVABLE → structured logging at entry and decision points\n- No TODO, FIXME, stub, placeholder, or pass statements\n- Return ONLY the compiled code with sentinels — no explanation", "prompt": "FUNCTION capture_payment [id=f0010]\n  INTENT       \"atomically capture a previously authorised payment against an order, updating ledger and order state under an exclusive row lock\"\n  PRECONDITION authorization_id IS NOT NULL\n  PRECONDITION authorization_id.length == 36\n  PRECONDITION capture_amount > 0\n  PRECONDITION capture_amount <= authorization.authorised_amount\n  POSTCONDITION result.status == \"captured\"\n  POSTCONDITION result.ledger_entry_id IS NOT NULL\n  POSTCONDITION result.captured_at IS NOT NULL\n  READS        payment_authorizations_table\n  READS        orders_table\n  MUTATES      payment_authorizations_table\n  MUTATES      payment_ledger_table\n  MUTATES      orders_table\n  EMITS        \"payment.captured\"\n  OBSERVABLE\n  CONFIDENCE   0.96\n  TIMEOUT      30s\n\n  BEGIN TRANSACTION isolation=SERIALIZABLE\n\n    authorization = LOCK payment_authorizations_table\n      WHERE  id == authorization_id\n      MODE   EXCLUSIVE\n      NOWAIT FALSE\n      TIMEOUT 10s\n\n    IF authorization IS NULL THEN FAIL NotFoundError(\"authorization_not_found\")\n    IF authorization.status != \"authorized\" THEN FAIL PaymentError(\"authorization_not_capturable\", {status: authorization.status})\n    IF authorization.expires_at < NOW() THEN FAIL PaymentError(\"authorization_expired\")\n    IF capture_amount > authorization.authorised_amount THEN FAIL PaymentError(\"capture_exceeds_authorization\")\n\n    order = LOCK orders_table\n      WHERE  id == authorization.order_id\n      MODE   EXCLUSIVE\n      NOWAIT FALSE\n      TIMEOUT 10s\n\n    IF order IS NULL THEN FAIL NotFoundError(\"order_not_found\")\n    IF order.status NOT IN [\"pending_payment\", \"partially_paid\"] THEN FAIL PaymentError(\"order_not_payable\", {status: order.status})\n\n    ledger_entry = {\n      authorization_id: authorization_id,\n      order_id:         order.id,\n      amount:           capture_amount,\n      currency:         authorization.currency,\n      direction:        \"credit\",\n      recorded_at:      NOW()\n    }\n\n    INSERT ledger_entry INTO payment_ledger_table\n    UPDATE payment_authorizations_table\n      SET status       = \"captured\"\n          captured_amount = capture_amount\n          captured_at  = NOW()\n      WHERE id == authorization_id\n\n    new_order_status = IF order.total_amount == capture_amount THEN \"paid\" ELSE \"partially_paid\"\n    UPDATE orders_table\n      SET status              = new_order_status\n          amount_paid         = order.amount_paid + capture_amount\n          last_payment_at     = NOW()\n      WHERE id == order.id\n\n  COMMIT TRANSACTION\n\n  result = {\n    status:         \"captured\",\n    authorization_id: authorization_id,\n    order_id:       order.id,\n    captured_amount: capture_amount,\n    currency:       authorization.currency,\n    ledger_entry_id: ledger_entry.id,\n    captured_at:    NOW()\n  }\n\n  EMIT \"payment.captured\" WITH {\n    order_id:        order.id,\n    authorization_id: authorization_id,\n    amount:          capture_amount,\n    currency:        authorization.currency\n  }\n\n  RETURN result\nEND FUNCTION capture_payment [id=f0010]", "completion": "INTL:BEGIN[f0010]\n\n```python\nimport logging\nimport uuid\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Any\n\nfrom fastapi import APIRouter, Depends, HTTPException, status\nfrom pydantic import BaseModel, Field, field_validator\nfrom sqlalchemy import select, update\nfrom sqlalchemy.exc import OperationalError\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom app.db.session import get_db\nfrom app.events.publisher import publish_event\nfrom app.models.order import Order\nfrom app.models.payment_authorization import PaymentAuthorization\nfrom app.models.payment_ledger import PaymentLedgerEntry\n\nlogger = logging.getLogger(__name__)\nrouter = APIRouter(prefix=\"/payments\", tags=[\"payments\"])\n\n\n# ---------------------------------------------------------------------------\n# Domain exceptions\n# ---------------------------------------------------------------------------\n\nclass NotFoundError(Exception):\n    def __init__(self, code: str) -> None:\n        self.code = code\n        super().__init__(code)\n\n\nclass PaymentError(Exception):\n    def __init__(self, code: str, detail: dict[str, Any] | None = None) -> None:\n        self.code = code\n        self.detail = detail or {}\n        super().__init__(code)\n\n\n# ---------------------------------------------------------------------------\n# Schemas\n# ---------------------------------------------------------------------------\n\nclass CapturePaymentRequest(BaseModel):\n    authorization_id: str = Field(..., min_length=36, max_length=36)\n    capture_amount: Decimal = Field(..., gt=0)\n\n    @field_validator(\"authorization_id\")\n    @classmethod\n    def validate_uuid_format(cls, v: str) -> str:\n        try:\n            uuid.UUID(v)\n        except ValueError:\n            raise ValueError(\"authorization_id must be a valid UUID (36 characters)\")\n        return v\n\n\nclass CapturePaymentResponse(BaseModel):\n    status: str\n    authorization_id: str\n    order_id: str\n    captured_amount: Decimal\n    currency: str\n    ledger_entry_id: str\n    captured_at: datetime\n\n\n# ---------------------------------------------------------------------------\n# Service layer\n# ---------------------------------------------------------------------------\n\nasync def capture_payment_service(\n    authorization_id: str,\n    capture_amount: Decimal,\n    db: AsyncSession,\n) -> CapturePaymentResponse:\n    \"\"\"\n    Atomically capture a previously authorised payment against an order,\n    updating the ledger and order state under exclusive row locks.\n    \"\"\"\n    logger.info(\n        \"capture_payment.entry\",\n        extra={\n            \"authorization_id\": authorization_id,\n            \"capture_amount\": str(capture_amount),\n        },\n    )\n\n    # ------------------------------------------------------------------\n    # PRECONDITIONS (input-level; UUID format already validated by Pydantic)\n    # ------------------------------------------------------------------\n    assert authorization_id is not None, \"authorization_id must not be None\"\n    assert len(authorization_id) == 36, \"authorization_id must be 36 characters\"\n    assert capture_amount > 0, \"capture_amount must be positive\"\n\n    # ------------------------------------------------------------------\n    # BEGIN TRANSACTION (SERIALIZABLE isolation + exclusive row locks)\n    # ------------------------------------------------------------------\n    try:\n        async with db.begin():\n            await db.execute(\n                \"SET TRANSACTION ISOLATION LEVEL SERIALIZABLE\"\n            )  # type: ignore[arg-type]\n\n            # ---- Lock authorization row --------------------------------\n            auth_result = await db.execute(\n                select(PaymentAuthorization)\n                .where(PaymentAuthorization.id == authorization_id)\n                .with_for_update(nowait=False, skip_locked=False)\n                .execution_options(lock_timeout=\"10s\")\n            )\n            authorization: PaymentAuthorization | None = auth_result.scalar_one_or_none()\n\n            if authorization is None:\n                logger.warning(\n                    \"capture_payment.authorization_not_found\",\n                    extra={\"authorization_id\": authorization_id},\n                )\n                raise NotFoundError(\"authorization_not_found\")\n\n            if authorization.status != \"authorized\":\n                logger.warning(\n                    \"capture_payment.authorization_not_capturable\",\n                    extra={\n                        \"authorization_id\": authorization_id,\n                        \"current_status\": authorization.status,\n                    },\n                )\n                raise PaymentError(\n                    \"authorization_not_capturable\",\n                    {\"status\": authorization.status},\n                )\n\n            now_utc = datetime.now(timezone.utc)\n\n            if authorization.expires_at < now_utc:\n                logger.warning(\n                    \"capture_payment.authorization_expired\",\n                    extra={\n                        \"authorization_id\": authorization_id,\n                        \"expires_at\": authorization.expires_at.isoformat(),\n                    },\n                )\n                raise PaymentError(\"authorization_expired\")\n\n            # PRECONDITION: capture_amount <= authorised_amount\n            assert capture_amount <= authorization.authorised_amount, (\n                f\"capture_amount {capture_amount} exceeds \"\n                f\"authorised_amount {authorization.authorised_amount}\"\n            )\n            if capture_amount > authorization.authorised_amount:\n                logger.warning(\n                    \"capture_payment.capture_exceeds_authorization\",\n                    extra={\n                        \"authorization_id\": authorization_id,\n                        \"capture_amount\": str(capture_amount),\n                        \"authorised_amount\": str(authorization.authorised_amount),\n                    },\n                )\n                raise PaymentError(\"capture_exceeds_authorization\")\n\n            # ---- Lock order row ----------------------------------------\n            order_result = await db.execute(\n                select(Order)\n                .where(Order.id == authorization.order_id)\n                .with_for_update(nowait=False, skip_locked=False)\n                .execution_options(lock_timeout=\"10s\")\n            )\n            order: Order | None = order_result.scalar_one_or_none()\n\n            if order is None:\n                logger.warning(\n                    \"capture_payment.order_not_found\",\n                    extra={\"order_id\": authorization.order_id},\n                )\n                raise NotFoundError(\"order_not_found\")\n\n            payable_statuses = {\"pending_payment\", \"partially_paid\"}\n            if order.status not in payable_statuses:\n                logger.warning(\n                    \"capture_payment.order_not_payable\",\n                    extra={\n                        \"order_id\": order.id,\n                        \"current_status\": order.status,\n                    },\n                )\n                raise PaymentError(\n                    \"order_not_payable\",\n                    {\"status\": order.status},\n                )\n\n            logger.info(\n                \"capture_payment.locks_acquired\",\n                extra={\n                    \"authorization_id\": authorization_id,\n                    \"order_id\": order.id,\n                },\n            )\n\n            # ---- INSERT ledger entry -----------------------------------\n            captured_at = datetime.now(timezone.utc)\n            ledger_entry = PaymentLedgerEntry(\n                id=str(uuid.uuid4()),\n                authorization_id=authorization_id,\n                order_id=order.id,\n                amount=capture_amount,\n                currency=authorization.currency,\n                direction=\"credit\",\n                recorded_at=captured_at,\n            )\n            db.add(ledger_entry)\n            await db.flush()  # populate ledger_entry.id before continuing\n\n            logger.info(\n                \"capture_payment.ledger_entry_created\",\n                extra={\n                    \"ledger_entry_id\": ledger_entry.id,\n                    \"amount\": str(capture_amount),\n                    \"currency\": authorization.currency,\n                },\n            )\n\n            # ---- UPDATE payment_authorizations_table ------------------\n            await db.execute(\n                update(PaymentAuthorization)\n                .where(PaymentAuthorization.id == authorization_id)\n                .values(\n                    status=\"captured\",\n                    captured_amount=capture_amount,\n                    captured_at=captured_at,\n                )\n            )\n\n            # ---- Determine new order status ---------------------------\n            new_order_status = (\n                \"paid\"\n                if order.total_amount == capture_amount\n                else \"partially_paid\"\n            )\n            logger.info(\n                \"capture_payment.order_status_transition\",\n                extra={\n                    \"order_id\": order.id,\n                    \"from_status\": order.status,\n                    \"to_status\": new_order_status,\n                },\n            )\n\n            # ---- UPDATE orders_table ----------------------------------\n            await db.execute(\n                update(Order)\n                .where(Order.id == order.id)\n                .values(\n                    status=new_order_status,\n                    amount_paid=order.amount_paid + capture_amount,\n                    last_payment_at=captured_at,\n                )\n            )\n\n        # COMMIT occurs on exiting the `async with db.begin()` block\n\n    except OperationalError as exc:\n        logger.error(\n            \"capture_payment.db_error\",\n            extra={\"authorization_id\": authorization_id, \"error\": str(exc)},\n            exc_info=True,\n        )\n        raise\n\n    # ------------------------------------------------------------------\n    # Build result\n    # ------------------------------------------------------------------\n    result = CapturePaymentResponse(\n        status=\"captured\",\n        authorization_id=authorization_id,\n        order_id=order.id,\n        captured_amount=capture_amount,\n        currency=authorization.currency,\n        ledger_entry_id=ledger_entry.id,\n        captured_at=captured_at,\n    )\n\n    # ------------------------------------------------------------------\n    # POSTCONDITIONS\n    # ------------------------------------------------------------------\n    assert result.status == \"captured\", \"postcondition failed: result.status must be 'captured'\"\n    assert result.ledger_entry_id is not None, \"postcondition failed: ledger_entry_id must not be None\"\n    assert result.captured_at is not None, \"postcondition failed: captured_at must not be None\"\n\n    # ------------------------------------------------------------------\n    # EMIT event\n    # ------------------------------------------------------------------\n    await publish_event(\n        topic=\"payment.captured\",\n        payload={\n            \"order_id\": order.id,\n            \"authorization_id\": authorization_id,\n            \"amount\": str(capture_amount),\n            \"currency\": authorization.currency,\n        },\n    )\n\n    logger.info(\n        \"capture_payment.success\",\n        extra={\n            \"authorization_id\": authorization_id,\n            \"order_id\": order.id,\n            \"captured_amount\": str(capture_amount),\n            \"ledger_entry_id\": ledger_entry.id,\n            \"captured_at\": captured_at.isoformat(),\n        },\n    )\n\n    return result\n\n\n# ---------------------------------------------------------------------------\n# Route\n# ---------------------------------------------------------------------------\n\n@router.post(\n    \"/capture\",\n    response_model=CapturePaymentResponse,\n    status_code=status.HTTP_200_OK,\n    summary=\"Capture a previously authorised payment\",\n)\nasync def capture_payment(\n    payload: CapturePaymentRequest,\n    db: AsyncSession = Depends(get_db),\n) -> CapturePaymentResponse:\n    try:\n        return await capture_payment_service(\n            authorization_id=payload.authorization_id,\n            capture_amount=payload.capture_amount,\n            db=db,\n        )\n\n    except NotFoundError as exc:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail={\"code\": exc.code},\n        ) from exc\n\n    except PaymentError as exc:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail={\"code\": exc.code, **exc.detail},\n        ) from exc\n\n    except AssertionError as exc:\n        logger.error(\n            \"capture_payment.precondition_or_postcondition_violation\",\n            extra={\"error\": str(exc)},\n            exc_info=True,\n        )\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail={\"code\": \"internal_invariant_violation\"},\n        ) from exc\n```\n\nINTL:END[f0010]", "metadata": {"category": "A", "adapter": "python_fastapi", "construct": "LOCK", "idx": 10}}
{"system": "You are the INTL compiler for Python Fastapi. Convert INTL blocks to idiomatic, production-ready Python Fastapi code. Rules:\n- Wrap ALL output in INTL:BEGIN / INTL:END sentinels matching the block ID\n- PRECONDITION → explicit guard/assertion\n- POSTCONDITION → assertion before return\n- MUTATES → actual write operation (insert/update/delete)\n- OBSERVABLE → structured logging at entry and decision points\n- No TODO, FIXME, stub, placeholder, or pass statements\n- Return ONLY the compiled code with sentinels — no explanation", "prompt": "FUNCTION validate_file_upload [id=f0005]\n  INTENT       \"validate an uploaded file against size, type, and content policy constraints before storing metadata\"\n  PRECONDITION file.size_bytes > 0\n  PRECONDITION file.size_bytes <= 104857600\n  PRECONDITION uploader_user_id IS NOT NULL\n  POSTCONDITION result.validation_status IN (\"approved\", \"rejected\")\n  POSTCONDITION result.rejection_reasons IS NOT NULL\n  READS        upload_policy_table\n  READS        user_quota_table\n  MUTATES      file_validation_log_table\n  OBSERVABLE\n  CONFIDENCE   0.97\n  TIMEOUT      10s\n\n  policy = QUERY upload_policy_table WHERE profile == \"default\" LIMIT 1\n\n  VALIDATE file.mime_type AGAINST policy.allowed_mime_types\n    ON FAIL APPEND \"unsupported_file_type\" TO rejection_reasons\n\n  VALIDATE file.size_bytes <= policy.max_size_bytes\n    ON FAIL APPEND \"file_exceeds_size_limit\" TO rejection_reasons\n\n  VALIDATE file.filename MATCHES pattern(\"^[\\\\w\\\\-. ]+$\")\n    ON FAIL APPEND \"invalid_filename_characters\" TO rejection_reasons\n\n  VALIDATE file.filename.extension IN policy.allowed_extensions\n    ON FAIL APPEND \"disallowed_extension\" TO rejection_reasons\n\n  quota = QUERY user_quota_table WHERE user_id == uploader_user_id LIMIT 1\n  VALIDATE (quota.bytes_used + file.size_bytes) <= quota.bytes_limit\n    ON FAIL APPEND \"user_quota_exceeded\" TO rejection_reasons\n\n  VALIDATE file.checksum IS NOT NULL AND file.checksum.length == 64\n    ON FAIL APPEND \"missing_or_invalid_checksum\" TO rejection_reasons\n\n  IF rejection_reasons.length > 0 THEN\n    validation_status = \"rejected\"\n  ELSE\n    validation_status = \"approved\"\n\n  INSERT INTO file_validation_log_table (\n    user_id         = uploader_user_id,\n    file_checksum   = file.checksum,\n    mime_type       = file.mime_type,\n    size_bytes      = file.size_bytes,\n    status          = validation_status,\n    rejection_reasons = rejection_reasons,\n    evaluated_at    = NOW()\n  )\n\n  RETURN {\n    validation_status  = validation_status,\n    rejection_reasons  = rejection_reasons,\n    policy_version     = policy.version\n  }\nEND FUNCTION validate_file_upload [id=f0005]", "completion": "INTL:BEGIN[id=f0005]\n\n```python\nimport re\nimport logging\nfrom datetime import datetime, timezone\nfrom typing import Optional\n\nfrom fastapi import APIRouter, Depends, HTTPException, status\nfrom pydantic import BaseModel, Field, validator\nfrom sqlalchemy import select, insert\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom app.db import get_async_session\nfrom app.models import UploadPolicyTable, UserQuotaTable, FileValidationLogTable\n\nlogger = logging.getLogger(__name__)\nrouter = APIRouter()\n\n_FILENAME_PATTERN = re.compile(r\"^[\\w\\-. ]+$\")\n\n\nclass FileUploadInput(BaseModel):\n    size_bytes: int = Field(..., gt=0, le=104_857_600)\n    mime_type: str\n    filename: str\n    checksum: Optional[str] = None\n    uploader_user_id: str\n\n    @validator(\"uploader_user_id\")\n    def uploader_user_id_must_not_be_empty(cls, v: str) -> str:\n        if not v or not v.strip():\n            raise ValueError(\"uploader_user_id must not be null or empty\")\n        return v\n\n\nclass FileValidationResult(BaseModel):\n    validation_status: str\n    rejection_reasons: list[str]\n    policy_version: str\n\n\n@router.post(\n    \"/files/validate\",\n    response_model=FileValidationResult,\n    status_code=status.HTTP_200_OK,\n    tags=[\"files\"],\n)\nasync def validate_file_upload(\n    file: FileUploadInput,\n    db: AsyncSession = Depends(get_async_session),\n) -> FileValidationResult:\n    logger.info(\n        \"validate_file_upload.entry\",\n        extra={\n            \"uploader_user_id\": file.uploader_user_id,\n            \"mime_type\": file.mime_type,\n            \"size_bytes\": file.size_bytes,\n            \"filename\": file.filename,\n            \"checksum_present\": file.checksum is not None,\n        },\n    )\n\n    # ── PRECONDITIONS ────────────────────────────────────────────────────────\n    if file.size_bytes <= 0:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=\"Precondition failed: file.size_bytes must be greater than 0\",\n        )\n    if file.size_bytes > 104_857_600:\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=\"Precondition failed: file.size_bytes must not exceed 104857600\",\n        )\n    if not file.uploader_user_id or not file.uploader_user_id.strip():\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=\"Precondition failed: uploader_user_id must not be null\",\n        )\n\n    # ── READ: upload_policy_table ────────────────────────────────────────────\n    policy_row = (\n        await db.execute(\n            select(UploadPolicyTable)\n            .where(UploadPolicyTable.profile == \"default\")\n            .limit(1)\n        )\n    ).scalar_one_or_none()\n\n    if policy_row is None:\n        logger.error(\"validate_file_upload.policy_missing\", extra={\"profile\": \"default\"})\n        raise HTTPException(\n            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n            detail=\"Upload policy not found for profile 'default'\",\n        )\n\n    logger.info(\n        \"validate_file_upload.policy_loaded\",\n        extra={\"policy_version\": policy_row.version},\n    )\n\n    rejection_reasons: list[str] = []\n\n    # ── VALIDATE: mime_type ──────────────────────────────────────────────────\n    if file.mime_type not in policy_row.allowed_mime_types:\n        logger.info(\n            \"validate_file_upload.mime_type_rejected\",\n            extra={\"mime_type\": file.mime_type},\n        )\n        rejection_reasons.append(\"unsupported_file_type\")\n\n    # ── VALIDATE: size_bytes <= policy.max_size_bytes ────────────────────────\n    if file.size_bytes > policy_row.max_size_bytes:\n        logger.info(\n            \"validate_file_upload.size_rejected\",\n            extra={\n                \"size_bytes\": file.size_bytes,\n                \"max_size_bytes\": policy_row.max_size_bytes,\n            },\n        )\n        rejection_reasons.append(\"file_exceeds_size_limit\")\n\n    # ── VALIDATE: filename characters ────────────────────────────────────────\n    if not _FILENAME_PATTERN.match(file.filename):\n        logger.info(\n            \"validate_file_upload.filename_chars_rejected\",\n            extra={\"filename\": file.filename},\n        )\n        rejection_reasons.append(\"invalid_filename_characters\")\n\n    # ── VALIDATE: file extension ─────────────────────────────────────────────\n    filename_parts = file.filename.rsplit(\".\", 1)\n    file_extension = filename_parts[-1].lower() if len(filename_parts) == 2 else \"\"\n    if file_extension not in policy_row.allowed_extensions:\n        logger.info(\n            \"validate_file_upload.extension_rejected\",\n            extra={\"extension\": file_extension},\n        )\n        rejection_reasons.append(\"disallowed_extension\")\n\n    # ── READ: user_quota_table ───────────────────────────────────────────────\n    quota_row = (\n        await db.execute(\n            select(UserQuotaTable)\n            .where(UserQuotaTable.user_id == file.uploader_user_id)\n            .limit(1)\n        )\n    ).scalar_one_or_none()\n\n    if quota_row is None:\n        logger.error(\n            \"validate_file_upload.quota_missing\",\n            extra={\"uploader_user_id\": file.uploader_user_id},\n        )\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"Quota record not found for user '{file.uploader_user_id}'\",\n        )\n\n    # ── VALIDATE: user quota ─────────────────────────────────────────────────\n    if (quota_row.bytes_used + file.size_bytes) > quota_row.bytes_limit:\n        logger.info(\n            \"validate_file_upload.quota_exceeded\",\n            extra={\n                \"bytes_used\": quota_row.bytes_used,\n                \"size_bytes\": file.size_bytes,\n                \"bytes_limit\": quota_row.bytes_limit,\n            },\n        )\n        rejection_reasons.append(\"user_quota_exceeded\")\n\n    # ── VALIDATE: checksum ───────────────────────────────────────────────────\n    if not file.checksum or len(file.checksum) != 64:\n        logger.info(\n            \"validate_file_upload.checksum_invalid\",\n            extra={\"checksum_length\": len(file.checksum) if file.checksum else 0},\n        )\n        rejection_reasons.append(\"missing_or_invalid_checksum\")\n\n    # ── DETERMINE STATUS ─────────────────────────────────────────────────────\n    validation_status = \"rejected\" if len(rejection_reasons) > 0 else \"approved\"\n\n    logger.info(\n        \"validate_file_upload.status_determined\",\n        extra={\n            \"validation_status\": validation_status,\n            \"rejection_reasons\": rejection_reasons,\n        },\n    )\n\n    # ── MUTATE: file_validation_log_table ────────────────────────────────────\n    await db.execute(\n        insert(FileValidationLogTable).values(\n            user_id=file.uploader_user_id,\n            file_checksum=file.checksum,\n            mime_type=file.mime_type,\n            size_bytes=file.size_bytes,\n            status=validation_status,\n            rejection_reasons=rejection_reasons,\n            evaluated_at=datetime.now(tz=timezone.utc),\n        )\n    )\n    await db.commit()\n\n    logger.info(\n        \"validate_file_upload.log_written\",\n        extra={\n            \"uploader_user_id\": file.uploader_user_id,\n            \"validation_status\": validation_status,\n        },\n    )\n\n    # ── POSTCONDITIONS ───────────────────────────────────────────────────────\n    assert validation_status in (\n        \"approved\",\n        \"rejected\",\n    ), f\"Postcondition failed: unexpected validation_status '{validation_status}'\"\n    assert (\n        rejection_reasons is not None\n    ), \"Postcondition failed: rejection_reasons must not be None\"\n\n    result = FileValidationResult(\n        validation_status=validation_status,\n        rejection_reasons=rejection_reasons,\n        policy_version=policy_row.version,\n    )\n\n    logger.info(\n        \"validate_file_upload.exit\",\n        extra={\n            \"validation_status\": result.validation_status,\n            \"policy_version\": result.policy_version,\n            \"rejection_reason_count\": len(result.rejection_reasons),\n        },\n    )\n\n    return result\n```\n\nINTL:END[id=f0005]", "metadata": {"category": "A", "adapter": "python_fastapi", "construct": "VALIDATE", "idx": 5}}
{"system": "You are the INTL compiler for Python Fastapi in PATCH mode. Given existing compiled code and an INTL PATCH block, apply the patch with minimal diff. Rules:\n- Preserve existing code structure — only apply the described change\n- Wrap output in INTL:BEGIN / INTL:END sentinels\n- No TODO, stub, or placeholder\n- Return ONLY the patched code — no explanation", "prompt": "INTL:BEGIN[id=f0003]\n\n```python\nimport asyncio\nimport logging\nfrom datetime import datetime, timezone\nfrom typing import Annotated\n\nfrom fastapi import APIRouter, Depends, HTTPException, status\nfrom pydantic import BaseModel, Field\nfrom sqlalchemy import delete, select, text\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom app.db.models import SessionRecord\nfrom app.db.session import get_db\nfrom app.dependencies.auth import get_current_caller\nfrom app.events.emitter import emit_event\nfrom app.schemas.caller import CallerContext\n\nlogger = logging.getLogger(__name__)\n\nrouter = APIRouter()\n\n_PURGE_BATCH_SIZE = 500\n_ALLOWED_ROLES = {\"system\", \"admin\"}\n\n\nclass PurgeExpiredSessionsRequest(BaseModel):\n    purge_before: datetime = Field(\n        ...,\n        description=\"Delete all sessions with expires_at strictly before this timestamp.\",\n    )\n\n\nclass PurgeExpiredSessionsResponse(BaseModel):\n    deleted_count: int = Field(..., ge=0)\n    purged_before: datetime\n    completed_at: datetime\n\n\n@router.post(\n    \"/sessions/purge-expired\",\n    response_model=PurgeExpiredSessionsResponse,\n    status_code=status.HTTP_200_OK,\n    summary=\"Purge expired sessions\",\n    tags=[\"sessions\"],\n)\nasync def purge_expired_sessions(\n    payload: PurgeExpiredSessionsRequest,\n    caller: Annotated[CallerContext, Depends(get_current_caller)],\n    db: Annotated[AsyncSession, Depends(get_db)],\n) -> PurgeExpiredSessionsResponse:\n    \"\"\"\n    Scan all sessions and permanently delete every record whose expiry\n    timestamp is in the past (strictly before *purge_before*).\n\n    INTL id=f0003 | confidence=0.97 | timeout=120s\n    \"\"\"\n    logger.info(\n        \"purge_expired_sessions: invoked\",\n        extra={\n            \"caller_role\": caller.role,\n            \"caller_id\": caller.id,\n            \"purge_before\": payload.purge_before.isoformat(),\n        },\n    )\n\n    # ── PRECONDITION: caller role must be system or admin ──────────────────────\n    if caller.role not in _ALLOWED_ROLES:\n        logger.warning(\n            \"purge_expired_sessions: forbidden – caller role not permitted\",\n            extra={\"caller_role\": caller.role, \"caller_id\": caller.id},\n        )\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=(\n                f\"Caller role '{caller.role}' is not authorised to purge sessions. \"\n                f\"Required: {sorted(_ALLOWED_ROLES)}.\"\n            ),\n        )\n\n    # ── PRECONDITION: purge_before must not be None (enforced by Pydantic) and\n    #                  must not be in the future ────────────────────────────────\n    now_utc = datetime.now(timezone.utc)\n    purge_before = payload.purge_before\n    if purge_before.tzinfo is None:\n        purge_before = purge_before.replace(tzinfo=timezone.utc)\n\n    if purge_before > now_utc:\n        logger.warning(\n            \"purge_expired_sessions: rejected – purge_before is in the future\",\n            extra={\n                \"purge_before\": purge_before.isoformat(),\n                \"now_utc\": now_utc.isoformat(),\n            },\n        )\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=(\n                f\"purge_before ({purge_before.isoformat()}) must not be in the future \"\n                f\"(server UTC now: {now_utc.isoformat()}).\"\n            ),\n        )\n\n    logger.info(\n        \"purge_expired_sessions: preconditions satisfied – starting purge loop\",\n        extra={\"purge_before\": purge_before.isoformat(), \"batch_size\": _PURGE_BATCH_SIZE},\n    )\n\n    # ── MUTATES sessions_table: batched delete ─────────────────────────────────\n    deleted_count: int = 0\n    offset: int = 0\n\n    while True:\n        # Fetch one batch of candidate session IDs ordered by expires_at ASC.\n        batch_result = await db.execute(\n            select(SessionRecord.session_id, SessionRecord.expires_at)\n            .where(SessionRecord.expires_at < purge_before)\n            .order_by(SessionRecord.expires_at.asc())\n            .limit(_PURGE_BATCH_SIZE)\n            .offset(offset)\n        )\n        batch = batch_result.fetchall()\n\n        if not batch:\n            logger.debug(\n                \"purge_expired_sessions: empty batch – purge loop complete\",\n                extra={\"offset\": offset, \"deleted_count_so_far\": deleted_count},\n            )\n            break\n\n        logger.debug(\n            \"purge_expired_sessions: processing batch\",\n            extra={\"offset\": offset, \"batch_len\": len(batch)},\n        )\n\n        session_ids_to_delete: list[str] = []\n        for row in batch:\n            # Guard: skip any row that no longer satisfies the predicate\n            # (e.g. updated between query and delete).\n            if row.expires_at >= purge_before:\n                logger.debug(\n                    \"purge_expired_sessions: skipping session – expires_at no longer < purge_before\",\n                    extra={\n                        \"session_id\": row.session_id,\n                        \"expires_at\": row.expires_at.isoformat()\n                        if hasattr(row.expires_at, \"isoformat\")\n                        else str(row.expires_at),\n                    },\n                )\n                continue\n            session_ids_to_delete.append(row.session_id)\n\n        if session_ids_to_delete:\n            delete_result = await db.execute(\n                delete(SessionRecord).where(\n                    SessionRecord.session_id.in_(session_ids_to_delete)\n                )\n            )\n            batch_deleted: int = delete_result.rowcount\n            deleted_count += batch_deleted\n\n            logger.info(\n                \"purge_expired_sessions: batch deleted\",\n                extra={\n                    \"batch_deleted\": batch_deleted,\n                    \"running_total\": deleted_count,\n                    \"offset\": offset,\n                },\n            )\n\n        # If the batch was smaller than the page size we have exhausted the table.\n        if len(batch) < _PURGE_BATCH_SIZE:\n            break\n\n        # Because we are deleting rows the effective offset stays at 0 for the\n        # next iteration; advance only when rows were skipped without deletion.\n        offset += len(batch) - len(session_ids_to_delete)\n\n    await db.commit()\n\n    completed_at = datetime.now(timezone.utc)\n\n    # ── POSTCONDITION: deleted_count must be non-negative ──────────────────────\n    assert deleted_count >= 0, (\n        f\"Invariant violated: deleted_count must be >= 0, got {deleted_count}\"\n    )\n\n    # ── POSTCONDITION: no expired records must remain ──────────────────────────\n    stale_check = await db.execute(\n        select(text(\"1\"))\n        .select_from(SessionRecord)\n        .where(SessionRecord.expires_at < purge_before)\n        .limit(1)\n    )\n    stale_row = stale_check.first()\n    if stale_row is not None:\n        logger.error(\n            \"purge_expired_sessions: POSTCONDITION FAILED – stale sessions remain after purge\",\n            extra={\"purge_before\": purge_before.isoformat()},\n        )\n        raise RuntimeError(\n            \"Postcondition violated: sessions_table still contains records \"\n            f\"with expires_at < {purge_before.isoformat()} after purge.\"\n        )\n\n    result = PurgeExpiredSessionsResponse(\n        deleted_count=deleted_count,\n        purged_before=purge_before,\n        completed_at=completed_at,\n    )\n\n    # ── EMITS: sessions.purge_completed ───────────────────────────────────────\n    await emit_event(\n        \"sessions.purge_completed\",\n        {\n            \"deleted_count\": result.deleted_count,\n            \"purged_before\": result.purged_before.isoformat(),\n            \"completed_at\": result.completed_at.isoformat(),\n            \"caller_id\": caller.id,\n            \"caller_role\": caller.role,\n        },\n    )\n\n    logger.info(\n        \"purge_expired_sessions: completed successfully\",\n        extra={\n            \"deleted_count\": result.deleted_count,\n            \"purged_before\": result.purged_before.isoformat(),\n            \"completed_at\": result.completed_at.isoformat(),\n        },\n    )\n\n    return result\n```\n\nINTL:END[id=f0003]\n\nPATCH p0003 TARGETS f0003\n  INTENT \"add input sanitisation for string fields\"\n\n  PRECONDITION_ADD email.matches_pattern(r'^[^<>\\'\\\";\\\\/]+$')\n  PRECONDITION_ADD name.matches_pattern(r'^[^<>\\'\\\";\\\\/]+$')\n\n  BEFORE\n    user_input = collect_string_fields(request)\n  AFTER\n    user_input = collect_string_fields(request)\n    user_input = SANITISE user_input\n      STRIP_TAGS        true\n      NORMALISE_UNICODE true\n      MAX_LENGTH        255\n      REJECT_NULL_BYTES true\n      TRIM_WHITESPACE   true\n\n  MUTATES_ADD sanitisation_log\n  EMITS_ADD   \"input.sanitised\"\n\n  ON_VIOLATION FAIL ValidationError(\"sanitisation_failed\", field=offending_field)\n\n  OBSERVABLE\n  CONFIDENCE 0.97\nEND PATCH p0003 TARGETS f0003", "completion": "INTL:BEGIN[id=f0003]\n\n```python\nimport asyncio\nimport logging\nimport re\nimport unicodedata\nfrom datetime import datetime, timezone\nfrom typing import Annotated\n\nfrom fastapi import APIRouter, Depends, HTTPException, status\nfrom pydantic import BaseModel, Field\nfrom sqlalchemy import delete, select, text\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom app.db.models import SessionRecord\nfrom app.db.session import get_db\nfrom app.dependencies.auth import get_current_caller\nfrom app.events.emitter import emit_event\nfrom app.schemas.caller import CallerContext\n\nlogger = logging.getLogger(__name__)\n\nrouter = APIRouter()\n\n_PURGE_BATCH_SIZE = 500\n_ALLOWED_ROLES = {\"system\", \"admin\"}\n\n_SANITISE_PATTERN = re.compile(r'^[^<>\\'\\\";\\\\/]+$')\n_NULL_BYTE_PATTERN = re.compile(r'\\x00')\n_MAX_FIELD_LENGTH = 255\n\n\nclass ValidationError(Exception):\n    def __init__(self, code: str, field: str):\n        self.code = code\n        self.field = field\n        super().__init__(f\"{code}: field='{field}'\")\n\n\ndef _sanitise_string_field(value: str, field_name: str) -> str:\n    \"\"\"\n    Sanitise a single string field according to patch p0003 rules:\n      - reject null bytes\n      - strip HTML/XML tags\n      - normalise unicode (NFC)\n      - trim whitespace\n      - enforce max length\n      - validate against allowed character pattern\n    Raises ValidationError on any violation.\n    \"\"\"\n    # REJECT_NULL_BYTES\n    if _NULL_BYTE_PATTERN.search(value):\n        raise ValidationError(\"sanitisation_failed\", field=field_name)\n\n    # STRIP_TAGS – remove anything that looks like an HTML/XML tag\n    stripped = re.sub(r'<[^>]*>', '', value)\n\n    # NORMALISE_UNICODE\n    normalised = unicodedata.normalize('NFC', stripped)\n\n    # TRIM_WHITESPACE\n    trimmed = normalised.strip()\n\n    # MAX_LENGTH\n    if len(trimmed) > _MAX_FIELD_LENGTH:\n        raise ValidationError(\"sanitisation_failed\", field=field_name)\n\n    # Pattern check (matches_pattern for email / name fields)\n    if trimmed and not _SANITISE_PATTERN.match(trimmed):\n        raise ValidationError(\"sanitisation_failed\", field=field_name)\n\n    return trimmed\n\n\ndef _collect_string_fields(obj: BaseModel) -> dict[str, str]:\n    \"\"\"Return a mapping of field_name -> value for all str-typed fields on obj.\"\"\"\n    result: dict[str, str] = {}\n    for field_name, field_info in obj.model_fields.items():\n        value = getattr(obj, field_name, None)\n        if isinstance(value, str):\n            result[field_name] = value\n    return result\n\n\ndef _apply_sanitisation(\n    user_input: dict[str, str],\n) -> tuple[dict[str, str], list[dict]]:\n    \"\"\"\n    Apply sanitisation rules to every string field collected from the request.\n    Returns (sanitised_map, sanitisation_log).\n    Raises ValidationError for the first offending field.\n    \"\"\"\n    sanitised: dict[str, str] = {}\n    log: list[dict] = []\n    for field_name, raw_value in user_input.items():\n        clean_value = _sanitise_string_field(raw_value, field_name)\n        sanitised[field_name] = clean_value\n        log.append(\n            {\n                \"field\": field_name,\n                \"original_length\": len(raw_value),\n                \"sanitised_length\": len(clean_value),\n                \"changed\": raw_value != clean_value,\n            }\n        )\n    return sanitised, log\n\n\nclass PurgeExpiredSessionsRequest(BaseModel):\n    purge_before: datetime = Field(\n        ...,\n        description=\"Delete all sessions with expires_at strictly before this timestamp.\",\n    )\n\n\nclass PurgeExpiredSessionsResponse(BaseModel):\n    deleted_count: int = Field(..., ge=0)\n    purged_before: datetime\n    completed_at: datetime\n\n\n@router.post(\n    \"/sessions/purge-expired\",\n    response_model=PurgeExpiredSessionsResponse,\n    status_code=status.HTTP_200_OK,\n    summary=\"Purge expired sessions\",\n    tags=[\"sessions\"],\n)\nasync def purge_expired_sessions(\n    payload: PurgeExpiredSessionsRequest,\n    caller: Annotated[CallerContext, Depends(get_current_caller)],\n    db: Annotated[AsyncSession, Depends(get_db)],\n) -> PurgeExpiredSessionsResponse:\n    \"\"\"\n    Scan all sessions and permanently delete every record whose expiry\n    timestamp is in the past (strictly before *purge_before*).\n\n    INTL id=f0003 | confidence=0.97 | timeout=120s\n    \"\"\"\n    logger.info(\n        \"purge_expired_sessions: invoked\",\n        extra={\n            \"caller_role\": caller.role,\n            \"caller_id\": caller.id,\n            \"purge_before\": payload.purge_before.isoformat(),\n        },\n    )\n\n    # ── INPUT SANITISATION (p0003) ─────────────────────────────────────────────\n    user_input = _collect_string_fields(payload)\n    try:\n        user_input, sanitisation_log = _apply_sanitisation(user_input)\n    except ValidationError as exc:\n        logger.error(\n            \"purge_expired_sessions: sanitisation_failed\",\n            extra={\"field\": exc.field, \"code\": exc.code},\n        )\n        await emit_event(\n            \"input.sanitised\",\n            {\n                \"status\": \"rejected\",\n                \"field\": exc.field,\n                \"code\": exc.code,\n                \"caller_id\": caller.id,\n                \"caller_role\": caller.role,\n            },\n        )\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=f\"Input sanitisation failed for field '{exc.field}'.\",\n        ) from exc\n\n    logger.info(\n        \"purge_expired_sessions: input sanitisation passed\",\n        extra={\"sanitisation_log\": sanitisation_log},\n    )\n    await emit_event(\n        \"input.sanitised\",\n        {\n            \"status\": \"accepted\",\n            \"sanitisation_log\": sanitisation_log,\n            \"caller_id\": caller.id,\n            \"caller_role\": caller.role,\n        },\n    )\n\n    # ── PRECONDITION: caller role must be system or admin ──────────────────────\n    if caller.role not in _ALLOWED_ROLES:\n        logger.warning(\n            \"purge_expired_sessions: forbidden – caller role not permitted\",\n            extra={\"caller_role\": caller.role, \"caller_id\": caller.id},\n        )\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=(\n                f\"Caller role '{caller.role}' is not authorised to purge sessions. \"\n                f\"Required: {sorted(_ALLOWED_ROLES)}.\"\n            ),\n        )\n\n    # ── PRECONDITION: purge_before must not be None (enforced by Pydantic) and\n    #                  must not be in the future ────────────────────────────────\n    now_utc = datetime.now(timezone.utc)\n    purge_before = payload.purge_before\n    if purge_before.tzinfo is None:\n        purge_before = purge_before.replace(tzinfo=timezone.utc)\n\n    if purge_before > now_utc:\n        logger.warning(\n            \"purge_expired_sessions: rejected – purge_before is in the future\",\n            extra={\n                \"purge_before\": purge_before.isoformat(),\n                \"now_utc\": now_utc.isoformat(),\n            },\n        )\n        raise HTTPException(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=(\n                f\"purge_before ({purge_before.isoformat()}) must not be in the future \"\n                f\"(server UTC now: {now_utc.isoformat()}).\"\n            ),\n        )\n\n    logger.info(\n        \"purge_expired_sessions: preconditions satisfied – starting purge loop\",\n        extra={\"purge_before\": purge_before.isoformat(), \"batch_size\": _PURGE_BATCH_SIZE},\n    )\n\n    # ── MUTATES sessions_table: batched delete ─────────────────────────────────\n    deleted_count: int = 0\n    offset: int = 0\n\n    while True:\n        # Fetch one batch of candidate session IDs ordered by expires_at ASC.\n        batch_result = await db.execute(\n            select(SessionRecord.session_id, SessionRecord.expires_at)\n            .where(SessionRecord.expires_at < purge_before)\n            .order_by(SessionRecord.expires_at.asc())\n            .limit(_PURGE_BATCH_SIZE)\n            .offset(offset)\n        )\n        batch = batch_result.fetchall()\n\n        if not batch:\n            logger.debug(\n                \"purge_expired_sessions: empty batch – purge loop complete\",\n                extra={\"offset\": offset, \"deleted_count_so_far\": deleted_count},\n            )\n            break\n\n        logger.debug(\n            \"purge_expired_sessions: processing batch\",\n            extra={\"offset\": offset, \"batch_len\": len(batch)},\n        )\n\n        session_ids_to_delete: list[str] = []\n        for row in batch:\n            # Guard: skip any row that no longer satisfies the predicate\n            # (e.g. updated between query and delete).\n            if row.expires_at >= purge_before:\n                logger.debug(\n                    \"purge_expired_sessions: skipping session – expires_at no longer < purge_before\",\n                    extra={\n                        \"session_id\": row.session_id,\n                        \"expires_at\": row.expires_at.isoformat()\n                        if hasattr(row.expires_at, \"isoformat\")\n                        else str(row.expires_at),\n                    },\n                )\n                continue\n            session_ids_to_delete.append(row.session_id)\n\n        if session_ids_to_delete:\n            delete_result = await db.execute(\n                delete(SessionRecord).where(\n                    SessionRecord.session_id.in_(session_ids_to_delete)\n                )\n            )\n            batch_deleted: int = delete_result.rowcount\n            deleted_count += batch_deleted\n\n            logger.info(\n                \"purge_expired_sessions: batch deleted\",\n                extra={\n                    \"batch_deleted\": batch_deleted,\n                    \"running_total\": deleted_count,\n                    \"offset\": offset,\n                },\n            )\n\n        # If the batch was smaller than the page size we have exhausted the table.\n        if len(batch) < _PURGE_BATCH_SIZE:\n            break\n\n        # Because we are deleting rows the effective offset stays at 0 for the\n        # next iteration; advance only when rows were skipped without deletion.\n        offset += len(batch) - len(session_ids_to_delete)\n\n    await db.commit()\n\n    completed_at = datetime.now(timezone.utc)\n\n    # ── POSTCONDITION: deleted_count must be non-negative ──────────────────────\n    assert deleted_count >= 0, (\n        f\"Invariant violated: deleted_count must be >= 0, got {deleted_count}\"\n    )\n\n    # ── POSTCONDITION: no expired records must remain ──────────────────────────\n    stale_check = await db.execute(\n        select(text(\"1\"))\n        .select_from(SessionRecord)\n        .where(SessionRecord.expires_at < purge_before)\n        .limit(1)\n    )\n    stale_row = stale_check.first()\n    if stale_row is not None:\n        logger.error(\n            \"purge_expired_sessions: POSTCONDITION FAILED – stale sessions remain after purge\",\n            extra={\"purge_before\": purge_before.isoformat()},\n        )\n        raise RuntimeError(\n            \"Postcondition violated: sessions_table still contains records \"\n            f\"with expires_at < {purge_before.isoformat()} after purge.\"\n        )\n\n    result = PurgeExpiredSessionsResponse(\n        deleted_count=deleted_count,\n        purged_before=purge_before,\n        completed_at=completed_at,\n    )\n\n    # ── EMITS: sessions.purge_completed ───────────────────────────────────────\n    await emit_event(\n        \"sessions.purge_completed\",\n        {\n            \"deleted_count\": result.deleted_count,\n            \"purged_before\": result.purged_before.isoformat(),\n            \"completed_at\": result.completed_at.isoformat(),\n            \"caller_id\": caller.id,\n            \"caller_role\": caller.role,\n        },\n    )\n\n    logger.info(\n        \"purge_expired_sessions: completed successfully\",\n        extra={\n            \"deleted_count\": result.deleted_count,\n            \"purged_before\": result.purged_before.isoformat(),\n            \"completed_at\": result.completed_at.isoformat(),\n        },\n    )\n\n    return result\n```\n\nINTL:END[id=f0003]", "metadata": {"category": "B", "adapter": "python_fastapi", "construct": "LOOP", "idx": 3}}
{"system": "You are the INTL compiler for Python Fastapi. Convert INTL blocks to idiomatic, production-ready Python Fastapi code. Rules:\n- Wrap ALL output in INTL:BEGIN / INTL:END sentinels matching the block ID\n- PRECONDITION → explicit guard/assertion\n- POSTCONDITION → assertion before return\n- MUTATES → actual write operation (insert/update/delete)\n- OBSERVABLE → structured logging at entry and decision points\n- No TODO, FIXME, stub, placeholder, or pass statements\n- Return ONLY the compiled code with sentinels — no explanation", "prompt": "FUNCTION list_inventory_items_paginated [id=f0009]\n  INTENT       \"return a paginated slice of inventory items filtered by warehouse and category with stable cursor-based ordering\"\n  PRECONDITION page_size >= 1\n  PRECONDITION page_size <= 500\n  PRECONDITION warehouse_id IS NOT NULL\n  POSTCONDITION result.items.length <= page_size\n  POSTCONDITION result.next_cursor IS NULL OR result.next_cursor.length > 0\n  READS        inventory_items_table\n  READS        warehouses_table\n  OBSERVABLE\n  CONFIDENCE   0.97\n  TIMEOUT      15s\n\n  warehouse = QUERY warehouses_table WHERE id == warehouse_id LIMIT 1\n  IF warehouse IS NULL THEN FAIL NotFoundError(\"warehouse_not_found\")\n  IF warehouse.active == FALSE THEN FAIL ValidationError(\"warehouse_inactive\")\n\n  base_filter = {\n    warehouse_id: warehouse_id,\n    deleted_at: NULL\n  }\n  IF category IS NOT NULL THEN base_filter.category = category\n  IF sku_prefix IS NOT NULL THEN base_filter.sku LIKE sku_prefix + \"%\"\n  IF in_stock_only == TRUE THEN base_filter.quantity_on_hand > 0\n\n  PAGINATE inventory_items_table\n    FILTER   base_filter\n    ORDER_BY [sku ASC, id ASC]\n    PAGE_SIZE page_size\n    CURSOR   cursor\n    CURSOR_FIELDS [sku, id]\n    INTO     page\n\n  result = {\n    items:        page.records,\n    next_cursor:  page.next_cursor,\n    prev_cursor:  page.prev_cursor,\n    total_hint:   page.approximate_total,\n    page_size:    page_size,\n    warehouse_id: warehouse_id\n  }\n\n  RETURN result\nEND FUNCTION list_inventory_items_paginated [id=f0009]", "completion": "INTL:BEGIN [id=f0009]\n\n```python\nimport base64\nimport json\nimport logging\nimport time\nfrom typing import Any, Optional\n\nfrom fastapi import APIRouter, Depends, Query, Request\nfrom pydantic import BaseModel, Field, field_validator\nfrom sqlalchemy import and_, asc, func, select, text\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom app.db.session import get_db\nfrom app.models.inventory import InventoryItem\nfrom app.models.warehouse import Warehouse\n\nlogger = logging.getLogger(__name__)\nrouter = APIRouter()\n\n\n# ---------------------------------------------------------------------------\n# Schema definitions\n# ---------------------------------------------------------------------------\n\nclass CursorPayload(BaseModel):\n    sku: str\n    id: str\n\n\nclass PaginatedInventoryResponse(BaseModel):\n    items: list[dict[str, Any]]\n    next_cursor: Optional[str] = None\n    prev_cursor: Optional[str] = None\n    total_hint: Optional[int] = None\n    page_size: int\n    warehouse_id: str\n\n\n# ---------------------------------------------------------------------------\n# Cursor helpers\n# ---------------------------------------------------------------------------\n\ndef _encode_cursor(sku: str, item_id: str) -> str:\n    payload = json.dumps({\"sku\": sku, \"id\": item_id}, separators=(\",\", \":\"))\n    return base64.urlsafe_b64encode(payload.encode()).decode()\n\n\ndef _decode_cursor(cursor: str) -> CursorPayload:\n    try:\n        raw = base64.urlsafe_b64decode(cursor.encode()).decode()\n        data = json.loads(raw)\n        return CursorPayload(**data)\n    except Exception as exc:\n        raise ValueError(f\"invalid_cursor: {exc}\") from exc\n\n\n# ---------------------------------------------------------------------------\n# Endpoint\n# ---------------------------------------------------------------------------\n\n@router.get(\n    \"/inventory\",\n    response_model=PaginatedInventoryResponse,\n    summary=\"List inventory items (cursor-paginated)\",\n    tags=[\"inventory\"],\n)\nasync def list_inventory_items_paginated(\n    request: Request,\n    warehouse_id: str = Query(..., description=\"Target warehouse identifier\"),\n    page_size: int = Query(50, ge=1, le=500, description=\"Items per page (1–500)\"),\n    cursor: Optional[str] = Query(None, description=\"Opaque pagination cursor\"),\n    category: Optional[str] = Query(None, description=\"Filter by category\"),\n    sku_prefix: Optional[str] = Query(None, description=\"Filter by SKU prefix\"),\n    in_stock_only: bool = Query(False, description=\"Restrict to items with quantity_on_hand > 0\"),\n    db: AsyncSession = Depends(get_db),\n) -> PaginatedInventoryResponse:\n    t_start = time.monotonic()\n\n    # ------------------------------------------------------------------\n    # OBSERVABLE – entry\n    # ------------------------------------------------------------------\n    logger.info(\n        \"list_inventory_items_paginated.enter\",\n        extra={\n            \"warehouse_id\": warehouse_id,\n            \"page_size\": page_size,\n            \"cursor\": cursor,\n            \"category\": category,\n            \"sku_prefix\": sku_prefix,\n            \"in_stock_only\": in_stock_only,\n            \"request_id\": request.headers.get(\"x-request-id\"),\n        },\n    )\n\n    # ------------------------------------------------------------------\n    # PRECONDITIONS (FastAPI validates ge/le on page_size automatically;\n    # warehouse_id non-null is guaranteed by Query(...); explicit guards\n    # are kept here as defence-in-depth).\n    # ------------------------------------------------------------------\n    assert page_size >= 1, \"page_size must be >= 1\"\n    assert page_size <= 500, \"page_size must be <= 500\"\n    assert warehouse_id, \"warehouse_id must not be null or empty\"\n\n    # ------------------------------------------------------------------\n    # Warehouse lookup & validation\n    # ------------------------------------------------------------------\n    wh_stmt = select(Warehouse).where(Warehouse.id == warehouse_id).limit(1)\n    wh_result = await db.execute(wh_stmt)\n    warehouse = wh_result.scalar_one_or_none()\n\n    if warehouse is None:\n        logger.warning(\n            \"list_inventory_items_paginated.warehouse_not_found\",\n            extra={\"warehouse_id\": warehouse_id},\n        )\n        from fastapi import HTTPException\n        raise HTTPException(status_code=404, detail=\"warehouse_not_found\")\n\n    if not warehouse.active:\n        logger.warning(\n            \"list_inventory_items_paginated.warehouse_inactive\",\n            extra={\"warehouse_id\": warehouse_id},\n        )\n        from fastapi import HTTPException\n        raise HTTPException(status_code=422, detail=\"warehouse_inactive\")\n\n    # ------------------------------------------------------------------\n    # Build base filter predicates\n    # ------------------------------------------------------------------\n    predicates = [\n        InventoryItem.warehouse_id == warehouse_id,\n        InventoryItem.deleted_at.is_(None),\n    ]\n\n    if category is not None:\n        logger.debug(\n            \"list_inventory_items_paginated.filter_category\",\n            extra={\"category\": category},\n        )\n        predicates.append(InventoryItem.category == category)\n\n    if sku_prefix is not None:\n        logger.debug(\n            \"list_inventory_items_paginated.filter_sku_prefix\",\n            extra={\"sku_prefix\": sku_prefix},\n        )\n        safe_prefix = sku_prefix.replace(\"%\", r\"\\%\").replace(\"_\", r\"\\_\")\n        predicates.append(InventoryItem.sku.like(f\"{safe_prefix}%\"))\n\n    if in_stock_only:\n        logger.debug(\"list_inventory_items_paginated.filter_in_stock_only\")\n        predicates.append(InventoryItem.quantity_on_hand > 0)\n\n    # ------------------------------------------------------------------\n    # Cursor decode & keyset predicate\n    # ------------------------------------------------------------------\n    decoded_cursor: Optional[CursorPayload] = None\n    if cursor is not None:\n        decoded_cursor = _decode_cursor(cursor)\n        logger.debug(\n            \"list_inventory_items_paginated.cursor_decoded\",\n            extra={\"cursor_sku\": decoded_cursor.sku, \"cursor_id\": decoded_cursor.id},\n        )\n        # Stable keyset: (sku, id) > (cursor_sku, cursor_id)\n        predicates.append(\n            (InventoryItem.sku > decoded_cursor.sku)\n            | (\n                (InventoryItem.sku == decoded_cursor.sku)\n                & (InventoryItem.id > decoded_cursor.id)\n            )\n        )\n\n    # ------------------------------------------------------------------\n    # Fetch page_size + 1 rows to detect next page existence\n    # ------------------------------------------------------------------\n    stmt = (\n        select(InventoryItem)\n        .where(and_(*predicates))\n        .order_by(asc(InventoryItem.sku), asc(InventoryItem.id))\n        .limit(page_size + 1)\n    )\n\n    rows_result = await db.execute(stmt)\n    raw_records: list[InventoryItem] = list(rows_result.scalars().all())\n\n    has_next = len(raw_records) > page_size\n    page_records: list[InventoryItem] = raw_records[:page_size]\n\n    # ------------------------------------------------------------------\n    # Approximate total (non-blocking COUNT estimate)\n    # ------------------------------------------------------------------\n    count_stmt = (\n        select(func.count())\n        .select_from(InventoryItem)\n        .where(\n            and_(\n                InventoryItem.warehouse_id == warehouse_id,\n                InventoryItem.deleted_at.is_(None),\n                *(\n                    [InventoryItem.category == category]\n                    if category is not None\n                    else []\n                ),\n            )\n        )\n    )\n    count_result = await db.execute(count_stmt)\n    approximate_total: Optional[int] = count_result.scalar_one_or_none()\n\n    # ------------------------------------------------------------------\n    # Compute cursors\n    # ------------------------------------------------------------------\n    next_cursor: Optional[str] = None\n    if has_next and page_records:\n        last = page_records[-1]\n        next_cursor = _encode_cursor(last.sku, str(last.id))\n\n    prev_cursor: Optional[str] = None\n    if decoded_cursor is not None and page_records:\n        first = page_records[0]\n        prev_cursor = _encode_cursor(first.sku, str(first.id))\n\n    # ------------------------------------------------------------------\n    # Serialise records\n    # ------------------------------------------------------------------\n    serialised_items = [\n        {\n            \"id\": str(item.id),\n            \"sku\": item.sku,\n            \"warehouse_id\": str(item.warehouse_id),\n            \"category\": item.category,\n            \"quantity_on_hand\": item.quantity_on_hand,\n            \"created_at\": item.created_at.isoformat() if item.created_at else None,\n            \"updated_at\": item.updated_at.isoformat() if item.updated_at else None,\n        }\n        for item in page_records\n    ]\n\n    # ------------------------------------------------------------------\n    # Assemble result\n    # ------------------------------------------------------------------\n    result = PaginatedInventoryResponse(\n        items=serialised_items,\n        next_cursor=next_cursor,\n        prev_cursor=prev_cursor,\n        total_hint=approximate_total,\n        page_size=page_size,\n        warehouse_id=warehouse_id,\n    )\n\n    # ------------------------------------------------------------------\n    # POSTCONDITIONS\n    # ------------------------------------------------------------------\n    assert len(result.items) <= page_size, (\n        f\"postcondition violated: items length {len(result.items)} exceeds page_size {page_size}\"\n    )\n    assert result.next_cursor is None or len(result.next_cursor) > 0, (\n        \"postcondition violated: next_cursor must be null or non-empty string\"\n    )\n\n    # ------------------------------------------------------------------\n    # OBSERVABLE – exit\n    # ------------------------------------------------------------------\n    elapsed_ms = (time.monotonic() - t_start) * 1000\n    logger.info(\n        \"list_inventory_items_paginated.exit\",\n        extra={\n            \"warehouse_id\": warehouse_id,\n            \"returned_items\": len(result.items),\n            \"has_next\": has_next,\n            \"total_hint\": approximate_total,\n            \"elapsed_ms\": round(elapsed_ms, 2),\n        },\n    )\n\n    return result\n```\n\nINTL:END [id=f0009]", "metadata": {"category": "A", "adapter": "python_fastapi", "construct": "PAGINATE", "idx": 9}}
{"system": "You are the INTL compiler for Python Fastapi in PATCH mode. Given existing compiled code and an INTL PATCH block, apply the patch with minimal diff. Rules:\n- Preserve existing code structure — only apply the described change\n- Wrap output in INTL:BEGIN / INTL:END sentinels\n- No TODO, stub, or placeholder\n- Return ONLY the patched code — no explanation", "prompt": "INTL:BEGIN[f0011]\n\n```python\nimport logging\nimport uuid\nfrom datetime import datetime, timezone\nfrom typing import Any\n\nfrom fastapi import APIRouter, Depends\nfrom pydantic import BaseModel, field_validator\nfrom sqlalchemy import text\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom app.database import get_db\nfrom app.events import event_bus\nfrom app.exceptions import BillingError, SubscriptionError\n\nlogger = logging.getLogger(__name__)\nrouter = APIRouter()\n\n\nclass PlanChangeRequest(BaseModel):\n    subscription_id: str\n    new_plan_id: str\n\n    @field_validator(\"subscription_id\", \"new_plan_id\")\n    @classmethod\n    def must_not_be_blank(cls, v: str) -> str:\n        if not v or not v.strip():\n            raise ValueError(\"Field must not be blank\")\n        return v.strip()\n\n\nclass SubscriptionSummary(BaseModel):\n    id: str\n    current_plan_id: str\n\n\nclass PlanChangeResult(BaseModel):\n    subscription: SubscriptionSummary\n    proration_credit: float\n    proration_charge: float\n    net_adjustment: float\n    effective_at: datetime\n\n\nasync def _fetch_subscription(db: AsyncSession, subscription_id: str) -> dict[str, Any]:\n    row = await db.execute(\n        text(\n            \"SELECT id, current_plan_id, billing_account_id, status, \"\n            \"current_period_start, current_period_end \"\n            \"FROM subscriptions WHERE id = :sid LIMIT 1\"\n        ),\n        {\"sid\": subscription_id},\n    )\n    result = row.mappings().first()\n    return dict(result) if result else None\n\n\nasync def _fetch_plan(db: AsyncSession, plan_id: str) -> dict[str, Any]:\n    row = await db.execute(\n        text(\"SELECT id, price_cents FROM plans WHERE id = :pid LIMIT 1\"),\n        {\"pid\": plan_id},\n    )\n    result = row.mappings().first()\n    return dict(result) if result else None\n\n\nasync def _fetch_billing_account(db: AsyncSession, account_id: str) -> dict[str, Any]:\n    row = await db.execute(\n        text(\n            \"SELECT id, billing_method FROM billing_accounts WHERE id = :aid LIMIT 1\"\n        ),\n        {\"aid\": account_id},\n    )\n    result = row.mappings().first()\n    return dict(result) if result else None\n\n\ndef _days_between(start: datetime, end: datetime) -> float:\n    delta = end - start\n    return delta.total_seconds() / 86400.0\n\n\n@router.post(\n    \"/subscriptions/{subscription_id}/plan\",\n    response_model=PlanChangeResult,\n    status_code=200,\n    tags=[\"subscriptions\"],\n)\nasync def apply_subscription_plan_change(\n    subscription_id: str,\n    body: PlanChangeRequest,\n    db: AsyncSession = Depends(get_db),\n) -> PlanChangeResult:\n    logger.info(\n        \"apply_subscription_plan_change.entry\",\n        extra={\n            \"subscription_id\": subscription_id,\n            \"new_plan_id\": body.new_plan_id,\n        },\n    )\n\n    # PRECONDITION: subscription_id and new_plan_id must be non-null\n    assert subscription_id, \"subscription.id must not be null\"\n    assert body.new_plan_id, \"new_plan.id must not be null\"\n\n    subscription = await _fetch_subscription(db, subscription_id)\n    if subscription is None:\n        logger.warning(\n            \"apply_subscription_plan_change.subscription_not_found\",\n            extra={\"subscription_id\": subscription_id},\n        )\n        raise SubscriptionError(\"subscription_not_found\")\n\n    # PRECONDITION: new plan must differ from current plan\n    if body.new_plan_id == subscription[\"current_plan_id\"]:\n        logger.warning(\n            \"apply_subscription_plan_change.plan_unchanged\",\n            extra={\n                \"subscription_id\": subscription_id,\n                \"current_plan_id\": subscription[\"current_plan_id\"],\n                \"new_plan_id\": body.new_plan_id,\n            },\n        )\n        raise SubscriptionError(\"new_plan_must_differ_from_current_plan\")\n\n    # PRECONDITION: subscription must be active\n    if subscription[\"status\"] != \"active\":\n        logger.warning(\n            \"apply_subscription_plan_change.subscription_not_active\",\n            extra={\n                \"subscription_id\": subscription_id,\n                \"status\": subscription[\"status\"],\n            },\n        )\n        raise SubscriptionError(\"subscription_not_active\")\n\n    old_plan = await _fetch_plan(db, subscription[\"current_plan_id\"])\n    new_plan = await _fetch_plan(db, body.new_plan_id)\n\n    if new_plan is None:\n        logger.warning(\n            \"apply_subscription_plan_change.plan_not_found\",\n            extra={\"new_plan_id\": body.new_plan_id},\n        )\n        raise SubscriptionError(\"plan_not_found\")\n\n    account = await _fetch_billing_account(db, subscription[\"billing_account_id\"])\n    if account is None:\n        logger.warning(\n            \"apply_subscription_plan_change.billing_account_not_found\",\n            extra={\"billing_account_id\": subscription[\"billing_account_id\"]},\n        )\n        raise BillingError(\"billing_account_not_found\")\n\n    # PRECONDITION: billing method must exist\n    if not account.get(\"billing_method\"):\n        logger.warning(\n            \"apply_subscription_plan_change.billing_method_missing\",\n            extra={\"account_id\": account[\"id\"]},\n        )\n        raise BillingError(\"billing_method_missing\")\n\n    now = datetime.now(timezone.utc)\n    cycle_start: datetime = subscription[\"current_period_start\"]\n    cycle_end: datetime = subscription[\"current_period_end\"]\n\n    days_total = _days_between(cycle_start, cycle_end)\n    days_remaining = _days_between(now, cycle_end)\n\n    logger.info(\n        \"apply_subscription_plan_change.proration_computed\",\n        extra={\n            \"subscription_id\": subscription_id,\n            \"days_total\": days_total,\n            \"days_remaining\": days_remaining,\n        },\n    )\n\n    if days_total <= 0:\n        raise SubscriptionError(\"invalid_billing_cycle\")\n\n    days_remaining = max(days_remaining, 0.0)\n\n    proration_credit = (old_plan[\"price_cents\"] / days_total) * days_remaining\n    proration_charge = (new_plan[\"price_cents\"] / days_total) * days_remaining\n    net_adjustment = proration_charge - proration_credit\n\n    # MUTATES subscriptions_table\n    await db.execute(\n        text(\n            \"UPDATE subscriptions \"\n            \"SET current_plan_id = :new_plan_id, \"\n            \"    updated_at = :now, \"\n            \"    plan_changed_at = :now \"\n            \"WHERE id = :sid\"\n        ),\n        {\n            \"new_plan_id\": new_plan[\"id\"],\n            \"now\": now,\n            \"sid\": subscription_id,\n        },\n    )\n    logger.info(\n        \"apply_subscription_plan_change.subscription_updated\",\n        extra={\n            \"subscription_id\": subscription_id,\n            \"new_plan_id\": new_plan[\"id\"],\n        },\n    )\n\n    # MUTATES billing_adjustments_table\n    adjustment_id = str(uuid.uuid4())\n    await db.execute(\n        text(\n            \"INSERT INTO billing_adjustments \"\n            \"(id, subscription_id, account_id, old_plan_id, new_plan_id, \"\n            \" credit_cents, charge_cents, net_cents, reason, created_at) \"\n            \"VALUES (:id, :subscription_id, :account_id, :old_plan_id, :new_plan_id, \"\n            \"        :credit_cents, :charge_cents, :net_cents, :reason, :created_at)\"\n        ),\n        {\n            \"id\": adjustment_id,\n            \"subscription_id\": subscription_id,\n            \"account_id\": account[\"id\"],\n            \"old_plan_id\": old_plan[\"id\"] if old_plan else None,\n            \"new_plan_id\": new_plan[\"id\"],\n            \"credit_cents\": proration_credit,\n            \"charge_cents\": proration_charge,\n            \"net_cents\": net_adjustment,\n            \"reason\": \"plan_change_proration\",\n            \"created_at\": now,\n        },\n    )\n    logger.info(\n        \"apply_subscription_plan_change.billing_adjustment_created\",\n        extra={\n            \"adjustment_id\": adjustment_id,\n            \"subscription_id\": subscription_id,\n            \"net_cents\": net_adjustment,\n        },\n    )\n\n    await db.commit()\n\n    effective_at = datetime.now(timezone.utc)\n\n    # EMITS subscription.plan_changed\n    await event_bus.emit(\n        \"subscription.plan_changed\",\n        {\n            \"subscription_id\": subscription_id,\n            \"old_plan_id\": old_plan[\"id\"] if old_plan else None,\n            \"new_plan_id\": new_plan[\"id\"],\n            \"net_adjustment\": net_adjustment,\n            \"effective_at\": effective_at.isoformat(),\n        },\n    )\n    logger.info(\n        \"apply_subscription_plan_change.event_emitted\",\n        extra={\n            \"event\": \"subscription.plan_changed\",\n            \"subscription_id\": subscription_id,\n            \"new_plan_id\": new_plan[\"id\"],\n            \"effective_at\": effective_at.isoformat(),\n        },\n    )\n\n    result = PlanChangeResult(\n        subscription=SubscriptionSummary(\n            id=subscription_id,\n            current_plan_id=new_plan[\"id\"],\n        ),\n        proration_credit=proration_credit,\n        proration_charge=proration_charge,\n        net_adjustment=net_adjustment,\n        effective_at=effective_at,\n    )\n\n    # POSTCONDITION: current_plan_id reflects the new plan\n    assert result.subscription.current_plan_id == new_plan[\"id\"], (\n        \"Postcondition violated: current_plan_id was not updated to new_plan_id\"\n    )\n    # POSTCONDITION: proration_credit must be non-negative\n    assert result.proration_credit >= 0, (\n        \"Postcondition violated: proration_credit must be >= 0\"\n    )\n    # POSTCONDITION: effective_at must be set\n    assert result.effective_at is not None, (\n        \"Postcondition violated: effective_at must not be null\"\n    )\n\n    logger.info(\n        \"apply_subscription_plan_change.success\",\n        extra={\n            \"subscription_id\": subscription_id,\n            \"new_plan_id\": new_plan[\"id\"],\n            \"proration_credit\": proration_credit,\n            \"proration_charge\": proration_charge,\n            \"net_adjustment\": net_adjustment,\n            \"effective_at\": effective_at.isoformat(),\n        },\n    )\n\n    return result\n```\n\nINTL:END[f0011]\n\nPATCH p0011 TARGETS f0011\n  INTENT \"add rate limiting before the main logic\"\n\n  PRECONDITION rate_limiter.bucket(caller.ip).tokens > 0\n  PRECONDITION rate_limiter.bucket(caller.api_key).tokens > 0\n\n  INSERT BEFORE main_logic\n    attempts = QUERY rate_limit_table\n      WHERE key == caller.ip\n        AND window_start >= NOW() - 60s\n      LIMIT 1\n\n    IF attempts IS NULL THEN\n      MUTATES rate_limit_table INSERT { key: caller.ip, count: 1, window_start: NOW() }\n    ELSE IF attempts.count >= 100 THEN\n      FAIL RateLimitError(\"rate_limit_exceeded\", { limit: 100, window: \"60s\", retry_after: attempts.window_start + 60s - NOW() })\n    ELSE\n      MUTATES rate_limit_table UPDATE { count: attempts.count + 1 }\n        WHERE key == caller.ip\n          AND window_start == attempts.window_start\n    END IF\n\n    api_key_attempts = QUERY rate_limit_table\n      WHERE key == caller.api_key\n        AND window_start >= NOW() - 60s\n      LIMIT 1\n\n    IF api_key_attempts IS NULL THEN\n      MUTATES rate_limit_table INSERT { key: caller.api_key, count: 1, window_start: NOW() }\n    ELSE IF api_key_attempts.count >= 1000 THEN\n      FAIL RateLimitError(\"api_key_rate_limit_exceeded\", { limit: 1000, window: \"60s\", retry_after: api_key_attempts.window_start + 60s - NOW() })\n    ELSE\n      MUTATES rate_limit_table UPDATE { count: api_key_attempts.count + 1 }\n        WHERE key == caller.api_key\n          AND window_start == api_key_attempts.window_start\n    END IF\n  END INSERT\n\n  READS        rate_limit_table\n  MUTATES      rate_limit_table\n  OBSERVABLE\n  TIMEOUT      5s\n\nEND PATCH p0011 TARGETS f0011", "completion": "INTL:BEGIN[f0011]\n\n```python\nimport logging\nimport uuid\nfrom datetime import datetime, timedelta, timezone\nfrom typing import Any\n\nfrom fastapi import APIRouter, Depends, Request\nfrom pydantic import BaseModel, field_validator\nfrom sqlalchemy import text\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom app.database import get_db\nfrom app.events import event_bus\nfrom app.exceptions import BillingError, RateLimitError, SubscriptionError\n\nlogger = logging.getLogger(__name__)\nrouter = APIRouter()\n\n\nclass PlanChangeRequest(BaseModel):\n    subscription_id: str\n    new_plan_id: str\n\n    @field_validator(\"subscription_id\", \"new_plan_id\")\n    @classmethod\n    def must_not_be_blank(cls, v: str) -> str:\n        if not v or not v.strip():\n            raise ValueError(\"Field must not be blank\")\n        return v.strip()\n\n\nclass SubscriptionSummary(BaseModel):\n    id: str\n    current_plan_id: str\n\n\nclass PlanChangeResult(BaseModel):\n    subscription: SubscriptionSummary\n    proration_credit: float\n    proration_charge: float\n    net_adjustment: float\n    effective_at: datetime\n\n\nasync def _fetch_subscription(db: AsyncSession, subscription_id: str) -> dict[str, Any]:\n    row = await db.execute(\n        text(\n            \"SELECT id, current_plan_id, billing_account_id, status, \"\n            \"current_period_start, current_period_end \"\n            \"FROM subscriptions WHERE id = :sid LIMIT 1\"\n        ),\n        {\"sid\": subscription_id},\n    )\n    result = row.mappings().first()\n    return dict(result) if result else None\n\n\nasync def _fetch_plan(db: AsyncSession, plan_id: str) -> dict[str, Any]:\n    row = await db.execute(\n        text(\"SELECT id, price_cents FROM plans WHERE id = :pid LIMIT 1\"),\n        {\"pid\": plan_id},\n    )\n    result = row.mappings().first()\n    return dict(result) if result else None\n\n\nasync def _fetch_billing_account(db: AsyncSession, account_id: str) -> dict[str, Any]:\n    row = await db.execute(\n        text(\n            \"SELECT id, billing_method FROM billing_accounts WHERE id = :aid LIMIT 1\"\n        ),\n        {\"aid\": account_id},\n    )\n    result = row.mappings().first()\n    return dict(result) if result else None\n\n\ndef _days_between(start: datetime, end: datetime) -> float:\n    delta = end - start\n    return delta.total_seconds() / 86400.0\n\n\n@router.post(\n    \"/subscriptions/{subscription_id}/plan\",\n    response_model=PlanChangeResult,\n    status_code=200,\n    tags=[\"subscriptions\"],\n)\nasync def apply_subscription_plan_change(\n    subscription_id: str,\n    body: PlanChangeRequest,\n    request: Request,\n    db: AsyncSession = Depends(get_db),\n) -> PlanChangeResult:\n    logger.info(\n        \"apply_subscription_plan_change.entry\",\n        extra={\n            \"subscription_id\": subscription_id,\n            \"new_plan_id\": body.new_plan_id,\n        },\n    )\n\n    # PRECONDITION: subscription_id and new_plan_id must be non-null\n    assert subscription_id, \"subscription.id must not be null\"\n    assert body.new_plan_id, \"new_plan.id must not be null\"\n\n    # Rate limiting by caller IP\n    caller_ip = request.client.host if request.client else \"unknown\"\n    caller_api_key = request.headers.get(\"X-API-Key\", \"\")\n    now_rl = datetime.now(timezone.utc)\n    window_cutoff = now_rl - timedelta(seconds=60)\n\n    ip_attempts_row = await db.execute(\n        text(\n            \"SELECT key, count, window_start FROM rate_limits \"\n            \"WHERE key = :key AND window_start >= :cutoff LIMIT 1\"\n        ),\n        {\"key\": caller_ip, \"cutoff\": window_cutoff},\n    )\n    attempts = ip_attempts_row.mappings().first()\n    attempts = dict(attempts) if attempts else None\n\n    if attempts is None:\n        await db.execute(\n            text(\n                \"INSERT INTO rate_limits (key, count, window_start) \"\n                \"VALUES (:key, :count, :window_start)\"\n            ),\n            {\"key\": caller_ip, \"count\": 1, \"window_start\": now_rl},\n        )\n    elif attempts[\"count\"] >= 100:\n        retry_after = (attempts[\"window_start\"] + timedelta(seconds=60) - now_rl).total_seconds()\n        logger.warning(\n            \"apply_subscription_plan_change.rate_limit_exceeded\",\n            extra={\"caller_ip\": caller_ip, \"count\": attempts[\"count\"]},\n        )\n        raise RateLimitError(\n            \"rate_limit_exceeded\",\n            {\"limit\": 100, \"window\": \"60s\", \"retry_after\": retry_after},\n        )\n    else:\n        await db.execute(\n            text(\n                \"UPDATE rate_limits SET count = :count \"\n                \"WHERE key = :key AND window_start = :window_start\"\n            ),\n            {\n                \"count\": attempts[\"count\"] + 1,\n                \"key\": caller_ip,\n                \"window_start\": attempts[\"window_start\"],\n            },\n        )\n\n    # Rate limiting by caller API key\n    if caller_api_key:\n        api_key_attempts_row = await db.execute(\n            text(\n                \"SELECT key, count, window_start FROM rate_limits \"\n                \"WHERE key = :key AND window_start >= :cutoff LIMIT 1\"\n            ),\n            {\"key\": caller_api_key, \"cutoff\": window_cutoff},\n        )\n        api_key_attempts = api_key_attempts_row.mappings().first()\n        api_key_attempts = dict(api_key_attempts) if api_key_attempts else None\n\n        if api_key_attempts is None:\n            await db.execute(\n                text(\n                    \"INSERT INTO rate_limits (key, count, window_start) \"\n                    \"VALUES (:key, :count, :window_start)\"\n                ),\n                {\"key\": caller_api_key, \"count\": 1, \"window_start\": now_rl},\n            )\n        elif api_key_attempts[\"count\"] >= 1000:\n            retry_after = (api_key_attempts[\"window_start\"] + timedelta(seconds=60) - now_rl).total_seconds()\n            logger.warning(\n                \"apply_subscription_plan_change.api_key_rate_limit_exceeded\",\n                extra={\"caller_api_key\": caller_api_key, \"count\": api_key_attempts[\"count\"]},\n            )\n            raise RateLimitError(\n                \"api_key_rate_limit_exceeded\",\n                {\"limit\": 1000, \"window\": \"60s\", \"retry_after\": retry_after},\n            )\n        else:\n            await db.execute(\n                text(\n                    \"UPDATE rate_limits SET count = :count \"\n                    \"WHERE key = :key AND window_start = :window_start\"\n                ),\n                {\n                    \"count\": api_key_attempts[\"count\"] + 1,\n                    \"key\": caller_api_key,\n                    \"window_start\": api_key_attempts[\"window_start\"],\n                },\n            )\n\n    subscription = await _fetch_subscription(db, subscription_id)\n    if subscription is None:\n        logger.warning(\n            \"apply_subscription_plan_change.subscription_not_found\",\n            extra={\"subscription_id\": subscription_id},\n        )\n        raise SubscriptionError(\"subscription_not_found\")\n\n    # PRECONDITION: new plan must differ from current plan\n    if body.new_plan_id == subscription[\"current_plan_id\"]:\n        logger.warning(\n            \"apply_subscription_plan_change.plan_unchanged\",\n            extra={\n                \"subscription_id\": subscription_id,\n                \"current_plan_id\": subscription[\"current_plan_id\"],\n                \"new_plan_id\": body.new_plan_id,\n            },\n        )\n        raise SubscriptionError(\"new_plan_must_differ_from_current_plan\")\n\n    # PRECONDITION: subscription must be active\n    if subscription[\"status\"] != \"active\":\n        logger.warning(\n            \"apply_subscription_plan_change.subscription_not_active\",\n            extra={\n                \"subscription_id\": subscription_id,\n                \"status\": subscription[\"status\"],\n            },\n        )\n        raise SubscriptionError(\"subscription_not_active\")\n\n    old_plan = await _fetch_plan(db, subscription[\"current_plan_id\"])\n    new_plan = await _fetch_plan(db, body.new_plan_id)\n\n    if new_plan is None:\n        logger.warning(\n            \"apply_subscription_plan_change.plan_not_found\",\n            extra={\"new_plan_id\": body.new_plan_id},\n        )\n        raise SubscriptionError(\"plan_not_found\")\n\n    account = await _fetch_billing_account(db, subscription[\"billing_account_id\"])\n    if account is None:\n        logger.warning(\n            \"apply_subscription_plan_change.billing_account_not_found\",\n            extra={\"billing_account_id\": subscription[\"billing_account_id\"]},\n        )\n        raise BillingError(\"billing_account_not_found\")\n\n    # PRECONDITION: billing method must exist\n    if not account.get(\"billing_method\"):\n        logger.warning(\n            \"apply_subscription_plan_change.billing_method_missing\",\n            extra={\"account_id\": account[\"id\"]},\n        )\n        raise BillingError(\"billing_method_missing\")\n\n    now = datetime.now(timezone.utc)\n    cycle_start: datetime = subscription[\"current_period_start\"]\n    cycle_end: datetime = subscription[\"current_period_end\"]\n\n    days_total = _days_between(cycle_start, cycle_end)\n    days_remaining = _days_between(now, cycle_end)\n\n    logger.info(\n        \"apply_subscription_plan_change.proration_computed\",\n        extra={\n            \"subscription_id\": subscription_id,\n            \"days_total\": days_total,\n            \"days_remaining\": days_remaining,\n        },\n    )\n\n    if days_total <= 0:\n        raise SubscriptionError(\"invalid_billing_cycle\")\n\n    days_remaining = max(days_remaining, 0.0)\n\n    proration_credit = (old_plan[\"price_cents\"] / days_total) * days_remaining\n    proration_charge = (new_plan[\"price_cents\"] / days_total) * days_remaining\n    net_adjustment = proration_charge - proration_credit\n\n    # MUTATES subscriptions_table\n    await db.execute(\n        text(\n            \"UPDATE subscriptions \"\n            \"SET current_plan_id = :new_plan_id, \"\n            \"    updated_at = :now, \"\n            \"    plan_changed_at = :now \"\n            \"WHERE id = :sid\"\n        ),\n        {\n            \"new_plan_id\": new_plan[\"id\"],\n            \"now\": now,\n            \"sid\": subscription_id,\n        },\n    )\n    logger.info(\n        \"apply_subscription_plan_change.subscription_updated\",\n        extra={\n            \"subscription_id\": subscription_id,\n            \"new_plan_id\": new_plan[\"id\"],\n        },\n    )\n\n    # MUTATES billing_adjustments_table\n    adjustment_id = str(uuid.uuid4())\n    await db.execute(\n        text(\n            \"INSERT INTO billing_adjustments \"\n            \"(id, subscription_id, account_id, old_plan_id, new_plan_id, \"\n            \" credit_cents, charge_cents, net_cents, reason, created_at) \"\n            \"VALUES (:id, :subscription_id, :account_id, :old_plan_id, :new_plan_id, \"\n            \"        :credit_cents, :charge_cents, :net_cents, :reason, :created_at)\"\n        ),\n        {\n            \"id\": adjustment_id,\n            \"subscription_id\": subscription_id,\n            \"account_id\": account[\"id\"],\n            \"old_plan_id\": old_plan[\"id\"] if old_plan else None,\n            \"new_plan_id\": new_plan[\"id\"],\n            \"credit_cents\": proration_credit,\n            \"charge_cents\": proration_charge,\n            \"net_cents\": net_adjustment,\n            \"reason\": \"plan_change_proration\",\n            \"created_at\": now,\n        },\n    )\n    logger.info(\n        \"apply_subscription_plan_change.billing_adjustment_created\",\n        extra={\n            \"adjustment_id\": adjustment_id,\n            \"subscription_id\": subscription_id,\n            \"net_cents\": net_adjustment,\n        },\n    )\n\n    await db.commit()\n\n    effective_at = datetime.now(timezone.utc)\n\n    # EMITS subscription.plan_changed\n    await event_bus.emit(\n        \"subscription.plan_changed\",\n        {\n            \"subscription_id\": subscription_id,\n            \"old_plan_id\": old_plan[\"id\"] if old_plan else None,\n            \"new_plan_id\": new_plan[\"id\"],\n            \"net_adjustment\": net_adjustment,\n            \"effective_at\": effective_at.isoformat(),\n        },\n    )\n    logger.info(\n        \"apply_subscription_plan_change.event_emitted\",\n        extra={\n            \"event\": \"subscription.plan_changed\",\n            \"subscription_id\": subscription_id,\n            \"new_plan_id\": new_plan[\"id\"],\n            \"effective_at\": effective_at.isoformat(),\n        },\n    )\n\n    result = PlanChangeResult(\n        subscription=SubscriptionSummary(\n            id=subscription_id,\n            current_plan_id=new_plan[\"id\"],\n        ),\n        proration_credit=proration_credit,\n        proration_charge=proration_charge,\n        net_adjustment=net_adjustment,\n        effective_at=effective_at,\n    )\n\n    # POSTCONDITION: current_plan_id reflects the new plan\n    assert result.subscription.current_plan_id == new_plan[\"id\"], (\n        \"Postcondition violated: current_plan_id was not updated to new_plan_id\"\n    )\n    # POSTCONDITION: proration_credit must be non-negative\n    assert result.proration_credit >= 0, (\n        \"Postcondition violated: proration_credit must be >= 0\"\n    )\n    # POSTCONDITION: effective_at must be set\n    assert result.effective_at is not None, (\n        \"Postcondition violated: effective_at must not be null\"\n    )\n\n    logger.info(\n        \"apply_subscription_plan_change.success\",\n        extra={\n            \"subscription_id\": subscription_id,\n            \"new_plan_id\": new_plan[\"id\"],\n            \"proration_credit\": proration_credit,\n            \"proration_charge\": proration_charge,\n            \"net_adjustment\": net_adjustment,\n            \"effective_at\": effective_at.isoformat(),\n        },\n    )\n\n    return result\n```\n\nINTL:END[f0011]", "metadata": {"category": "B", "adapter": "python_fastapi", "construct": "PATCH", "idx": 11}}
{"system": "You are the INTL compiler for Python Fastapi. Convert INTL blocks to idiomatic, production-ready Python Fastapi code. Rules:\n- Wrap ALL output in INTL:BEGIN / INTL:END sentinels matching the block ID\n- PRECONDITION → explicit guard/assertion\n- POSTCONDITION → assertion before return\n- MUTATES → actual write operation (insert/update/delete)\n- OBSERVABLE → structured logging at entry and decision points\n- No TODO, FIXME, stub, placeholder, or pass statements\n- Return ONLY the compiled code with sentinels — no explanation", "prompt": "FUNCTION apply_coupon_to_order [id=f0006]\n  INTENT       \"apply a coupon code to a pending order, validate eligibility, and record redemption\"\n  PRECONDITION order_id IS NOT NULL\n  PRECONDITION coupon_code.length >= 4\n  PRECONDITION user_id IS NOT NULL\n  POSTCONDITION result.discounted_total >= 0\n  POSTCONDITION result.coupon_applied == true OR result.failure_reason IS NOT NULL\n  READS        coupons_table\n  READS        orders_table\n  READS        coupon_redemptions_table\n  MUTATES      orders_table\n  MUTATES      coupon_redemptions_table\n  EMITS        \"coupon.applied\"\n  OBSERVABLE\n  CONFIDENCE   0.96\n  TIMEOUT      15s\n\n  order = QUERY orders_table WHERE order_id == order_id AND user_id == user_id LIMIT 1\n  IF order IS NULL THEN FAIL OrderError(\"order_not_found\")\n  IF order.status != \"pending\" THEN FAIL OrderError(\"order_not_modifiable\")\n\n  coupon = QUERY coupons_table WHERE code == coupon_code AND is_active == true LIMIT 1\n  IF coupon IS NULL THEN FAIL CouponError(\"coupon_not_found_or_inactive\")\n  IF coupon.expires_at < NOW() THEN FAIL CouponError(\"coupon_expired\")\n  IF coupon.valid_from > NOW() THEN FAIL CouponError(\"coupon_not_yet_valid\")\n\n  redemption_count = QUERY coupon_redemptions_table\n    WHERE coupon_id == coupon.id\n    COUNT\n\n  IF redemption_count >= coupon.max_redemptions THEN FAIL CouponError(\"coupon_redemption_limit_reached\")\n\n  user_redemptions = QUERY coupon_redemptions_table\n    WHERE coupon_id == coupon.id AND user_id == user_id\n    COUNT\n\n  IF user_redemptions >= coupon.max_redemptions_per_user THEN FAIL CouponError(\"user_redemption_limit_reached\")\n\n  IF order.subtotal < coupon.minimum_order_value THEN\n    FAIL CouponError(\"order_below_minimum_value\")\n\n  discount_amount = CALCULATE_DISCOUNT(\n    strategy    = coupon.discount_type,\n    rate        = coupon.discount_value,\n    subtotal    = order.subtotal,\n    cap         = coupon.maximum_discount_cap\n  )\n\n  discounted_total = MAX(order.subtotal - discount_amount, 0)\n\n  UPDATE orders_table\n    WHERE order_id == order_id\n    SET (\n      coupon_id        = coupon.id,\n      discount_amount  = discount_amount,\n      discounted_total = discounted_total,\n      updated_at       = NOW()\n    )\n\n  INSERT INTO coupon_redemptions_table (\n    coupon_id     = coupon.id,\n    order_id      = order_id,\n    user_id       = user_id,\n    discount_amount = discount_amount,\n    redeemed_at   = NOW()\n  )\n\n  EMIT \"coupon.applied\" WITH {\n    order_id        = order_id,\n    user_id         = user_id,\n    coupon_code     = coupon_code,\n    discount_amount = discount_amount\n  }\n\n  RETURN {\n    coupon_applied    = true,\n    coupon_code       = coupon_code,\n    discount_amount   = discount_amount,\n    discounted_total  = discounted_total,\n    failure_reason    = NULL\n  }\nEND FUNCTION apply_coupon_to_order [id=f0006]", "completion": "INTL:BEGIN[f0006]\n\n```python\nimport logging\nfrom datetime import datetime, timezone\nfrom decimal import Decimal\nfrom typing import Optional\nfrom uuid import UUID\n\nfrom fastapi import APIRouter, Depends, HTTPException, status\nfrom pydantic import BaseModel, Field, field_validator\nfrom sqlalchemy import func, select, update, insert\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom app.db.session import get_async_session\nfrom app.db.models import Order, Coupon, CouponRedemption\nfrom app.events.publisher import emit_event\n\nlogger = logging.getLogger(__name__)\n\nrouter = APIRouter()\n\n\n# ---------------------------------------------------------------------------\n# Request / Response schemas\n# ---------------------------------------------------------------------------\n\nclass ApplyCouponRequest(BaseModel):\n    order_id: UUID\n    coupon_code: str = Field(..., min_length=4)\n    user_id: UUID\n\n    @field_validator(\"coupon_code\")\n    @classmethod\n    def strip_coupon_code(cls, v: str) -> str:\n        return v.strip()\n\n\nclass ApplyCouponResponse(BaseModel):\n    coupon_applied: bool\n    coupon_code: str\n    discount_amount: Decimal\n    discounted_total: Decimal\n    failure_reason: Optional[str]\n\n\n# ---------------------------------------------------------------------------\n# Domain errors\n# ---------------------------------------------------------------------------\n\nclass OrderError(HTTPException):\n    def __init__(self, reason: str) -> None:\n        super().__init__(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail={\"error_type\": \"OrderError\", \"reason\": reason},\n        )\n\n\nclass CouponError(HTTPException):\n    def __init__(self, reason: str) -> None:\n        super().__init__(\n            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail={\"error_type\": \"CouponError\", \"reason\": reason},\n        )\n\n\n# ---------------------------------------------------------------------------\n# Discount calculation\n# ---------------------------------------------------------------------------\n\ndef _calculate_discount(\n    *,\n    strategy: str,\n    rate: Decimal,\n    subtotal: Decimal,\n    cap: Optional[Decimal],\n) -> Decimal:\n    \"\"\"Compute discount amount according to the coupon discount_type strategy.\"\"\"\n    if strategy == \"percentage\":\n        raw = (rate / Decimal(\"100\")) * subtotal\n    elif strategy == \"fixed\":\n        raw = rate\n    else:\n        raw = Decimal(\"0\")\n\n    if cap is not None:\n        raw = min(raw, cap)\n\n    return max(raw, Decimal(\"0\"))\n\n\n# ---------------------------------------------------------------------------\n# Endpoint\n# ---------------------------------------------------------------------------\n\n@router.post(\n    \"/orders/apply-coupon\",\n    response_model=ApplyCouponResponse,\n    status_code=status.HTTP_200_OK,\n    tags=[\"orders\", \"coupons\"],\n)\nasync def apply_coupon_to_order(\n    payload: ApplyCouponRequest,\n    session: AsyncSession = Depends(get_async_session),\n) -> ApplyCouponResponse:\n    \"\"\"\n    Apply a coupon code to a pending order, validate eligibility,\n    and record the redemption. [INTL id=f0006]\n    \"\"\"\n\n    # --- OBSERVABLE: entry ---------------------------------------------------\n    logger.info(\n        \"apply_coupon_to_order.entry\",\n        extra={\n            \"order_id\": str(payload.order_id),\n            \"user_id\": str(payload.user_id),\n            \"coupon_code\": payload.coupon_code,\n        },\n    )\n\n    # --- PRECONDITIONS (enforced by Pydantic schema + explicit guards) --------\n    assert payload.order_id is not None, \"order_id must not be null\"\n    assert len(payload.coupon_code) >= 4, \"coupon_code must be at least 4 characters\"\n    assert payload.user_id is not None, \"user_id must not be null\"\n\n    now: datetime = datetime.now(tz=timezone.utc)\n\n    # --- Fetch order ---------------------------------------------------------\n    order_result = await session.execute(\n        select(Order).where(\n            Order.id == payload.order_id,\n            Order.user_id == payload.user_id,\n        ).limit(1)\n    )\n    order = order_result.scalar_one_or_none()\n\n    if order is None:\n        logger.warning(\n            \"apply_coupon_to_order.order_not_found\",\n            extra={\"order_id\": str(payload.order_id), \"user_id\": str(payload.user_id)},\n        )\n        raise OrderError(\"order_not_found\")\n\n    if order.status != \"pending\":\n        logger.warning(\n            \"apply_coupon_to_order.order_not_modifiable\",\n            extra={\"order_id\": str(payload.order_id), \"order_status\": order.status},\n        )\n        raise OrderError(\"order_not_modifiable\")\n\n    # --- Fetch coupon --------------------------------------------------------\n    coupon_result = await session.execute(\n        select(Coupon).where(\n            Coupon.code == payload.coupon_code,\n            Coupon.is_active.is_(True),\n        ).limit(1)\n    )\n    coupon = coupon_result.scalar_one_or_none()\n\n    if coupon is None:\n        logger.warning(\n            \"apply_coupon_to_order.coupon_not_found_or_inactive\",\n            extra={\"coupon_code\": payload.coupon_code},\n        )\n        raise CouponError(\"coupon_not_found_or_inactive\")\n\n    if coupon.expires_at is not None and coupon.expires_at < now:\n        logger.warning(\n            \"apply_coupon_to_order.coupon_expired\",\n            extra={\"coupon_id\": str(coupon.id), \"expires_at\": coupon.expires_at.isoformat()},\n        )\n        raise CouponError(\"coupon_expired\")\n\n    if coupon.valid_from is not None and coupon.valid_from > now:\n        logger.warning(\n            \"apply_coupon_to_order.coupon_not_yet_valid\",\n            extra={\"coupon_id\": str(coupon.id), \"valid_from\": coupon.valid_from.isoformat()},\n        )\n        raise CouponError(\"coupon_not_yet_valid\")\n\n    # --- Global redemption count check ---------------------------------------\n    redemption_count_result = await session.execute(\n        select(func.count()).select_from(CouponRedemption).where(\n            CouponRedemption.coupon_id == coupon.id,\n        )\n    )\n    redemption_count: int = redemption_count_result.scalar_one()\n\n    logger.info(\n        \"apply_coupon_to_order.redemption_count_check\",\n        extra={\n            \"coupon_id\": str(coupon.id),\n            \"redemption_count\": redemption_count,\n            \"max_redemptions\": coupon.max_redemptions,\n        },\n    )\n\n    if redemption_count >= coupon.max_redemptions:\n        raise CouponError(\"coupon_redemption_limit_reached\")\n\n    # --- Per-user redemption count check -------------------------------------\n    user_redemption_result = await session.execute(\n        select(func.count()).select_from(CouponRedemption).where(\n            CouponRedemption.coupon_id == coupon.id,\n            CouponRedemption.user_id == payload.user_id,\n        )\n    )\n    user_redemptions: int = user_redemption_result.scalar_one()\n\n    logger.info(\n        \"apply_coupon_to_order.user_redemption_count_check\",\n        extra={\n            \"coupon_id\": str(coupon.id),\n            \"user_id\": str(payload.user_id),\n            \"user_redemptions\": user_redemptions,\n            \"max_per_user\": coupon.max_redemptions_per_user,\n        },\n    )\n\n    if user_redemptions >= coupon.max_redemptions_per_user:\n        raise CouponError(\"user_redemption_limit_reached\")\n\n    # --- Minimum order value check -------------------------------------------\n    if order.subtotal < coupon.minimum_order_value:\n        logger.warning(\n            \"apply_coupon_to_order.order_below_minimum_value\",\n            extra={\n                \"order_id\": str(payload.order_id),\n                \"subtotal\": str(order.subtotal),\n                \"minimum_order_value\": str(coupon.minimum_order_value),\n            },\n        )\n        raise CouponError(\"order_below_minimum_value\")\n\n    # --- Discount calculation -------------------------------------------------\n    discount_amount: Decimal = _calculate_discount(\n        strategy=coupon.discount_type,\n        rate=coupon.discount_value,\n        subtotal=order.subtotal,\n        cap=coupon.maximum_discount_cap,\n    )\n    discounted_total: Decimal = max(order.subtotal - discount_amount, Decimal(\"0\"))\n\n    logger.info(\n        \"apply_coupon_to_order.discount_calculated\",\n        extra={\n            \"order_id\": str(payload.order_id),\n            \"discount_type\": coupon.discount_type,\n            \"discount_amount\": str(discount_amount),\n            \"discounted_total\": str(discounted_total),\n        },\n    )\n\n    # --- MUTATES: update orders_table ----------------------------------------\n    await session.execute(\n        update(Order)\n        .where(Order.id == payload.order_id)\n        .values(\n            coupon_id=coupon.id,\n            discount_amount=discount_amount,\n            discounted_total=discounted_total,\n            updated_at=now,\n        )\n    )\n\n    # --- MUTATES: insert into coupon_redemptions_table -----------------------\n    await session.execute(\n        insert(CouponRedemption).values(\n            coupon_id=coupon.id,\n            order_id=payload.order_id,\n            user_id=payload.user_id,\n            discount_amount=discount_amount,\n            redeemed_at=now,\n        )\n    )\n\n    await session.commit()\n\n    # --- EMITS: coupon.applied -----------------------------------------------\n    await emit_event(\n        \"coupon.applied\",\n        {\n            \"order_id\": str(payload.order_id),\n            \"user_id\": str(payload.user_id),\n            \"coupon_code\": payload.coupon_code,\n            \"discount_amount\": str(discount_amount),\n        },\n    )\n\n    logger.info(\n        \"apply_coupon_to_order.success\",\n        extra={\n            \"order_id\": str(payload.order_id),\n            \"user_id\": str(payload.user_id),\n            \"coupon_code\": payload.coupon_code,\n            \"discount_amount\": str(discount_amount),\n            \"discounted_total\": str(discounted_total),\n        },\n    )\n\n    # --- Build response -------------------------------------------------------\n    response = ApplyCouponResponse(\n        coupon_applied=True,\n        coupon_code=payload.coupon_code,\n        discount_amount=discount_amount,\n        discounted_total=discounted_total,\n        failure_reason=None,\n    )\n\n    # --- POSTCONDITIONS -------------------------------------------------------\n    assert response.discounted_total >= Decimal(\"0\"), (\n        \"postcondition violated: discounted_total must be non-negative\"\n    )\n    assert response.coupon_applied is True or response.failure_reason is not None, (\n        \"postcondition violated: coupon_applied must be True or failure_reason must be set\"\n    )\n\n    return response\n```\n\nINTL:END[f0006]", "metadata": {"category": "A", "adapter": "python_fastapi", "construct": "OBSERVABLE", "idx": 6}}
{"system": "You are the INTL compiler for Python Fastapi. Convert INTL blocks to idiomatic, production-ready Python Fastapi code. Rules:\n- Wrap ALL output in INTL:BEGIN / INTL:END sentinels matching the block ID\n- PRECONDITION → explicit guard/assertion\n- POSTCONDITION → assertion before return\n- MUTATES → actual write operation (insert/update/delete)\n- OBSERVABLE → structured logging at entry and decision points\n- No TODO, FIXME, stub, placeholder, or pass statements\n- Return ONLY the compiled code with sentinels — no explanation", "prompt": "FUNCTION cache_webhook_endpoint_config [id=f0002]\n  INTENT       \"store a webhook endpoint configuration in the cache layer with a TTL derived from the endpoint's rotation policy, so that delivery workers can resolve config without hitting the database on every request\"\n  PRECONDITION endpoint.id IS NOT NULL\n  PRECONDITION endpoint.url MATCHES \"^https://\"\n  PRECONDITION endpoint.signing_secret.length >= 32\n  PRECONDITION endpoint.rotation_interval_seconds > 0\n  POSTCONDITION CACHE EXISTS \"webhook:endpoint:\" + endpoint.id\n  READS        webhook_endpoints_table\n  OBSERVABLE\n  CONFIDENCE   0.95\n  TIMEOUT      5s\n\n  record = QUERY webhook_endpoints_table\n    WHERE id == endpoint.id\n      AND active == TRUE\n    LIMIT 1\n  IF record IS NULL THEN FAIL NotFoundError(\"webhook_endpoint_not_found\")\n\n  cache_payload = {\n    endpoint_id:     record.id,\n    url:             record.url,\n    signing_secret:  record.signing_secret,\n    event_types:     record.subscribed_event_types,\n    retry_policy:    record.retry_policy,\n    timeout_ms:      record.timeout_ms,\n    owner_id:        record.owner_id,\n    cached_at:       NOW()\n  }\n\n  cache_key = \"webhook:endpoint:\" + record.id\n  ttl       = record.rotation_interval_seconds / 2\n\n  CACHE SET cache_key WITH cache_payload TTL ttl\n    ON FAIL LOG WARN \"cache_write_failed\" WITH { key: cache_key, endpoint_id: record.id }\n    ON FAIL CONTINUE\nEND FUNCTION cache_webhook_endpoint_config [id=f0002]", "completion": "INTL:BEGIN[id=f0002]\n\n```python\nimport logging\nimport re\nfrom datetime import datetime, timezone\nfrom typing import Any\n\nfrom fastapi import APIRouter, Depends, HTTPException, status\nfrom pydantic import BaseModel, Field, field_validator\nfrom sqlalchemy import select\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom app.database import get_async_session\nfrom app.cache import get_redis_client\nfrom app.models import WebhookEndpoint\n\nlogger = logging.getLogger(__name__)\n\nrouter = APIRouter()\n\n_HTTPS_PATTERN = re.compile(r\"^https://\")\n_MIN_SIGNING_SECRET_LENGTH = 32\n\n\nclass WebhookEndpointConfigRequest(BaseModel):\n    id: str = Field(..., min_length=1)\n    url: str\n    signing_secret: str\n    rotation_interval_seconds: int\n\n    @field_validator(\"url\")\n    @classmethod\n    def url_must_be_https(cls, v: str) -> str:\n        if not _HTTPS_PATTERN.match(v):\n            raise ValueError(\"url must begin with https://\")\n        return v\n\n    @field_validator(\"signing_secret\")\n    @classmethod\n    def signing_secret_min_length(cls, v: str) -> str:\n        if len(v) < _MIN_SIGNING_SECRET_LENGTH:\n            raise ValueError(\n                f\"signing_secret must be at least {_MIN_SIGNING_SECRET_LENGTH} characters\"\n            )\n        return v\n\n    @field_validator(\"rotation_interval_seconds\")\n    @classmethod\n    def rotation_interval_must_be_positive(cls, v: int) -> int:\n        if v <= 0:\n            raise ValueError(\"rotation_interval_seconds must be greater than 0\")\n        return v\n\n\nclass WebhookEndpointConfigResponse(BaseModel):\n    cache_key: str\n    endpoint_id: str\n    ttl_seconds: int\n\n\nasync def _fetch_active_endpoint(\n    session: AsyncSession,\n    endpoint_id: str,\n) -> WebhookEndpoint:\n    result = await session.execute(\n        select(WebhookEndpoint)\n        .where(\n            WebhookEndpoint.id == endpoint_id,\n            WebhookEndpoint.active.is_(True),\n        )\n        .limit(1)\n    )\n    record = result.scalar_one_or_none()\n    if record is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail={\n                \"error\": \"webhook_endpoint_not_found\",\n                \"endpoint_id\": endpoint_id,\n            },\n        )\n    return record\n\n\nasync def _write_cache(\n    redis: Any,\n    cache_key: str,\n    payload: dict[str, Any],\n    ttl_seconds: int,\n    endpoint_id: str,\n) -> None:\n    try:\n        await redis.set(cache_key, payload, ex=ttl_seconds)\n    except Exception as exc:\n        logger.warning(\n            \"cache_write_failed\",\n            extra={\n                \"key\": cache_key,\n                \"endpoint_id\": endpoint_id,\n                \"error\": str(exc),\n            },\n        )\n\n\nasync def _assert_cache_exists(redis: Any, cache_key: str) -> None:\n    exists = await redis.exists(cache_key)\n    if not exists:\n        raise RuntimeError(\n            f\"POSTCONDITION VIOLATED: expected cache key '{cache_key}' to exist after write\"\n        )\n\n\n@router.post(\n    \"/webhook-endpoints/cache\",\n    response_model=WebhookEndpointConfigResponse,\n    status_code=status.HTTP_200_OK,\n    summary=\"Cache webhook endpoint configuration\",\n    tags=[\"webhooks\"],\n)\nasync def cache_webhook_endpoint_config(\n    endpoint: WebhookEndpointConfigRequest,\n    session: AsyncSession = Depends(get_async_session),\n    redis: Any = Depends(get_redis_client),\n) -> WebhookEndpointConfigResponse:\n    logger.info(\n        \"cache_webhook_endpoint_config.entry\",\n        extra={\n            \"endpoint_id\": endpoint.id,\n            \"url\": endpoint.url,\n            \"rotation_interval_seconds\": endpoint.rotation_interval_seconds,\n        },\n    )\n\n    # PRECONDITION: endpoint.id is not null — enforced by Pydantic min_length=1\n    # PRECONDITION: url matches ^https:// — enforced by Pydantic validator\n    # PRECONDITION: signing_secret length >= 32 — enforced by Pydantic validator\n    # PRECONDITION: rotation_interval_seconds > 0 — enforced by Pydantic validator\n\n    record = await _fetch_active_endpoint(session, endpoint.id)\n\n    logger.info(\n        \"cache_webhook_endpoint_config.record_fetched\",\n        extra={\n            \"endpoint_id\": record.id,\n            \"owner_id\": record.owner_id,\n        },\n    )\n\n    cache_payload: dict[str, Any] = {\n        \"endpoint_id\": record.id,\n        \"url\": record.url,\n        \"signing_secret\": record.signing_secret,\n        \"event_types\": record.subscribed_event_types,\n        \"retry_policy\": record.retry_policy,\n        \"timeout_ms\": record.timeout_ms,\n        \"owner_id\": record.owner_id,\n        \"cached_at\": datetime.now(tz=timezone.utc).isoformat(),\n    }\n\n    cache_key = f\"webhook:endpoint:{record.id}\"\n    ttl_seconds = record.rotation_interval_seconds // 2\n\n    logger.info(\n        \"cache_webhook_endpoint_config.writing_cache\",\n        extra={\n            \"cache_key\": cache_key,\n            \"ttl_seconds\": ttl_seconds,\n            \"endpoint_id\": record.id,\n        },\n    )\n\n    await _write_cache(redis, cache_key, cache_payload, ttl_seconds, record.id)\n\n    # POSTCONDITION: CACHE EXISTS \"webhook:endpoint:\" + endpoint.id\n    await _assert_cache_exists(redis, cache_key)\n\n    logger.info(\n        \"cache_webhook_endpoint_config.success\",\n        extra={\n            \"cache_key\": cache_key,\n            \"endpoint_id\": record.id,\n            \"ttl_seconds\": ttl_seconds,\n        },\n    )\n\n    return WebhookEndpointConfigResponse(\n        cache_key=cache_key,\n        endpoint_id=record.id,\n        ttl_seconds=ttl_seconds,\n    )\n```\n\nINTL:END[id=f0002]", "metadata": {"category": "A", "adapter": "python_fastapi", "construct": "CACHE SET", "idx": 4}}
