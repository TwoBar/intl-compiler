{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION validate_api_key [id=f0523]\n  INTENT       \"validate and retrieve api key metadata from cache or database\"\n  PRECONDITION api_key.length > 0\n  PRECONDITION api_key.length <= 128\n  POSTCONDITION result.key_id IS NOT NULL\n  POSTCONDITION result.is_active == TRUE\n  READS        api_keys_table\n  MUTATES      api_key_cache\n  OBSERVABLE\n\n  cached = CACHE GET api_key_cache[api_key]\n  IF cached IS NOT NULL THEN RETURN cached\n  \n  key_record = QUERY api_keys_table WHERE key_hash == hash(api_key) LIMIT 1\n  IF key_record IS NULL THEN FAIL AuthError(\"invalid_api_key\")\n  IF key_record.is_active == FALSE THEN FAIL AuthError(\"key_deactivated\")\n  IF key_record.expires_at < now() THEN FAIL AuthError(\"key_expired\")\n  \n  result = ApiKeyMetadata(key_id: key_record.id, is_active: key_record.is_active, \n                          org_id: key_record.org_id, rate_limit: key_record.rate_limit)\n  PERSIST api_key_cache (key: api_key, metadata: result, ttl: 3600)\n  OBSERVABLE \"api_key_validated\"\n  RETURN result\nEND FUNCTION validate_api_key [id=f0523]", "completion": "# ═══ INTL:BEGIN [id=f0523] validate_api_key ═══\nimport logging\nimport hashlib\nfrom datetime import datetime, timedelta\n\nlogger = logging.getLogger(__name__)\n\nasync def validate_api_key(api_key: str) -> ApiKeyMetadata:\n    assert len(api_key) > 0, \"api_key must not be empty\"\n    assert len(api_key) <= 128, \"api_key must not exceed 128 chars\"\n    \n    cached = await cache.get(f\"api_key:{api_key}\")\n    if cached is not None:\n        return ApiKeyMetadata(**cached)\n    \n    key_hash = hashlib.sha256(api_key.encode()).hexdigest()\n    key_record = await db.query(api_keys_table).filter(key_hash=key_hash).first()\n    \n    if key_record is None:\n        raise AuthError(\"invalid_api_key\")\n    if key_record.is_active == False:\n        raise AuthError(\"key_deactivated\")\n    if key_record.expires_at < datetime.utcnow():\n        raise AuthError(\"key_expired\")\n    \n    result = ApiKeyMetadata(\n        key_id=key_record.id,\n        is_active=key_record.is_active,\n        org_id=key_record.org_id,\n        rate_limit=key_record.rate_limit\n    )\n    \n    await cache.set(\n        f\"api_key:{api_key}\",\n        result.to_dict(),\n        ttl=3600\n    )\n    \n    logger.info(\"api_key_validated: key_id=%s org_id=%s\", result.key_id, result.org_id)\n    \n    assert result.key_id is not None, \"key_id must not be null\"\n    assert result.is_active == True, \"key must be active\"\n    \n    return result\n# ═══ INTL:END   [id=f0523] validate_api_key ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "CACHE GET", "idx": 523}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION process_upload [id=f0155]\n  INTENT       \"validate and store file upload with virus scan and timeout protection\"\n  PRECONDITION file_size > 0\n  PRECONDITION file_size <= 104857600\n  PRECONDITION user_id.length > 0\n  PRECONDITION mime_type IN (\"image/jpeg\", \"image/png\", \"application/pdf\")\n  POSTCONDITION result.file_id IS NOT NULL\n  POSTCONDITION result.status == \"stored\"\n  READS        users_table\n  MUTATES      uploads_table\n  MUTATES      scan_queue_table\n  TIMEOUT      30s\n  OBSERVABLE\n\n  user = QUERY users_table WHERE id == user_id LIMIT 1\n  IF user IS NULL THEN FAIL NotFoundError(\"user_not_found\")\n  IF user.upload_quota_remaining < file_size THEN FAIL QuotaError(\"insufficient_quota\")\n  \n  file_hash = compute_hash(file_data)\n  file_id = generate_uuid()\n  \n  PERSIST uploads_table (\n    id: file_id,\n    user_id: user_id,\n    original_name: file_name,\n    mime_type: mime_type,\n    size_bytes: file_size,\n    file_hash: file_hash,\n    status: \"pending_scan\",\n    uploaded_at: now(),\n    expires_at: now()+30d\n  )\n  \n  PERSIST scan_queue_table (\n    file_id: file_id,\n    priority: \"normal\",\n    queued_at: now()\n  )\n  \n  RETURN FileUpload(file_id: file_id, status: \"stored\")\nEND FUNCTION process_upload [id=f0155]", "completion": "# ═══ INTL:BEGIN [id=f0155] process_upload ═══\nimport logging\nimport uuid\nimport hashlib\nfrom datetime import datetime, timedelta\nfrom contextlib import asynccontextmanager\n\nlogger = logging.getLogger(__name__)\n\nasync def process_upload(\n    user_id: str,\n    file_name: str,\n    file_data: bytes,\n    file_size: int,\n    mime_type: str\n) -> FileUpload:\n    assert file_size > 0, \"file_size must be greater than 0\"\n    assert file_size <= 104857600, \"file_size must not exceed 100MB\"\n    assert len(user_id) > 0, \"user_id must not be empty\"\n    assert mime_type in (\"image/jpeg\", \"image/png\", \"application/pdf\"), \\\n        \"mime_type must be one of: image/jpeg, image/png, application/pdf\"\n    \n    try:\n        async with db.timeout(timeout_seconds=30):\n            user = await db.query(users_table).filter(id=user_id).first()\n            if user is None:\n                raise NotFoundError(\"user_not_found\")\n            if user.upload_quota_remaining < file_size:\n                raise QuotaError(\"insufficient_quota\")\n            \n            file_hash = hashlib.sha256(file_data).hexdigest()\n            file_id = str(uuid.uuid4())\n            \n            await db.insert(\n                uploads_table,\n                id=file_id,\n                user_id=user_id,\n                original_name=file_name,\n                mime_type=mime_type,\n                size_bytes=file_size,\n                file_hash=file_hash,\n                status=\"pending_scan\",\n                uploaded_at=datetime.utcnow(),\n                expires_at=datetime.utcnow() + timedelta(days=30)\n            )\n            \n            await db.insert(\n                scan_queue_table,\n                file_id=file_id,\n                priority=\"normal\",\n                queued_at=datetime.utcnow()\n            )\n            \n            logger.info(\n                \"process_upload: file_id=%s user_id=%s size=%d mime_type=%s\",\n                file_id, user_id, file_size, mime_type\n            )\n            \n            result = FileUpload(file_id=file_id, status=\"stored\")\n            assert result.file_id is not None\n            assert result.status == \"stored\"\n            return result\n    except asyncio.TimeoutError:\n        raise TimeoutError(\"upload_processing_exceeded_30_second_limit\")\n# ═══ INTL:END   [id=f0155] process_upload ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "TIMEOUT", "idx": 155}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION reset_mfa [id=f0845]\n  INTENT       \"disable MFA for user and issue new backup codes\"\n  PRECONDITION user_id > 0\n  PRECONDITION reason.length > 0\n  POSTCONDITION result.backup_codes IS NOT NULL\n  POSTCONDITION result.backup_codes.length == 10\n  READS        users_table\n  READS        mfa_settings_table\n  MUTATES      mfa_settings_table\n  MUTATES      audit_log_table\n  OBSERVABLE\n\n  user = QUERY users_table WHERE id == user_id LIMIT 1\n  IF user IS NULL THEN FAIL AuthError(\"user_not_found\")\n  \n  mfa_record = QUERY mfa_settings_table WHERE user_id == user_id LIMIT 1\n  IF mfa_record IS NULL THEN FAIL AuthError(\"mfa_not_enabled\")\n  \n  backup_codes = generate_backup_codes(10)\n  \n  PATCH mfa_settings_table WHERE user_id == user_id\n    SET mfa_enabled = FALSE\n    SET mfa_method = NULL\n    SET backup_codes_hash = hash_codes(backup_codes)\n    SET updated_at = now()\n  \n  PERSIST audit_log_table (\n    user_id: user_id,\n    action: \"mfa_reset\",\n    reason: reason,\n    timestamp: now(),\n    ip_address: request.ip\n  )\n  \n  RETURN ResetMFAResult(backup_codes: backup_codes, success: TRUE)\nEND FUNCTION reset_mfa [id=f0845]", "completion": "# ═══ INTL:BEGIN [id=f0845] reset_mfa ═══\nimport logging\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nasync def reset_mfa(user_id: int, reason: str, request) -> ResetMFAResult:\n    assert user_id > 0, \"user_id must be positive\"\n    assert len(reason) > 0, \"reason must not be empty\"\n    \n    user = await db.query(users_table).filter(id=user_id).first()\n    if user is None:\n        raise AuthError(\"user_not_found\")\n    \n    mfa_record = await db.query(mfa_settings_table).filter(user_id=user_id).first()\n    if mfa_record is None:\n        raise AuthError(\"mfa_not_enabled\")\n    \n    backup_codes = generate_backup_codes(10)\n    \n    await db.update(mfa_settings_table).where(user_id=user_id).values(\n        mfa_enabled=False,\n        mfa_method=None,\n        backup_codes_hash=hash_codes(backup_codes),\n        updated_at=datetime.utcnow()\n    )\n    \n    await db.insert(audit_log_table,\n                    user_id=user_id,\n                    action=\"mfa_reset\",\n                    reason=reason,\n                    timestamp=datetime.utcnow(),\n                    ip_address=request.ip)\n    \n    logger.info(\"reset_mfa: user=%s reason=%s\", user_id, reason)\n    \n    result = ResetMFAResult(backup_codes=backup_codes, success=True)\n    assert result.backup_codes is not None\n    assert len(result.backup_codes) == 10\n    return result\n# ═══ INTL:END   [id=f0845] reset_mfa ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "PATCH", "idx": 845}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION get_user_notifications [id=f0156]\n  INTENT       \"retrieve user notifications with caching layer for performance\"\n  PRECONDITION user_id > 0\n  PRECONDITION limit > 0\n  PRECONDITION limit <= 100\n  POSTCONDITION result.notifications.length <= limit\n  READS        notifications_table\n  READS        users_table\n  MUTATES      notification_reads_table\n  OBSERVABLE\n\n  cached = CACHE GET \"notifications:\" user_id\n  IF cached IS NOT NULL THEN RETURN cached\n  \n  user = QUERY users_table WHERE id == user_id LIMIT 1\n  IF user IS NULL THEN FAIL NotFoundError(\"user_not_found\")\n  \n  notifications = QUERY notifications_table WHERE recipient_id == user_id ORDER BY created_at DESC LIMIT limit\n  \n  FOR EACH notification IN notifications DO\n    IF notification.read_at IS NULL THEN\n      PERSIST notification_reads_table (notification_id: notification.id, read_at: now())\n    END IF\n  END FOR\n  \n  response = NotificationList(notifications: notifications, user_id: user_id)\n  CACHE SET \"notifications:\" user_id response TTL 300\n  RETURN response\nEND FUNCTION get_user_notifications [id=f0156]", "completion": "# ═══ INTL:BEGIN [id=f0156] get_user_notifications ═══\nimport logging\nfrom datetime import datetime\nfrom typing import List\n\nlogger = logging.getLogger(__name__)\n\nasync def get_user_notifications(user_id: int, limit: int) -> NotificationList:\n    assert user_id > 0, \"user_id must be positive\"\n    assert limit > 0, \"limit must be positive\"\n    assert limit <= 100, \"limit must not exceed 100\"\n    \n    cache_key = f\"notifications:{user_id}\"\n    cached = await cache.get(cache_key)\n    if cached is not None:\n        return cached\n    \n    user = await db.query(users_table).filter(id=user_id).first()\n    if user is None:\n        raise NotFoundError(\"user_not_found\")\n    \n    notifications = await db.query(notifications_table)\\\n        .filter(recipient_id=user_id)\\\n        .order_by(\"created_at\", descending=True)\\\n        .limit(limit)\\\n        .all()\n    \n    for notification in notifications:\n        if notification.read_at is None:\n            await db.insert(\n                notification_reads_table,\n                notification_id=notification.id,\n                read_at=datetime.utcnow()\n            )\n    \n    response = NotificationList(notifications=notifications, user_id=user_id)\n    await cache.set(cache_key, response, ttl=300)\n    \n    logger.info(\"get_user_notifications: user_id=%s, count=%d\", user_id, len(notifications))\n    \n    assert len(response.notifications) <= limit, \"notification count exceeds limit\"\n    return response\n# ═══ INTL:END   [id=f0156] get_user_notifications ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "CACHE GET", "idx": 156}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION broadcast_notifications [id=f0524]\n  INTENT       \"deliver notifications to multiple user groups in parallel and track delivery\"\n  PRECONDITION message.length > 0\n  PRECONDITION recipients.count > 0\n  PRECONDITION priority IN (\"high\", \"normal\", \"low\")\n  POSTCONDITION result.delivery_count >= 0\n  READS        users_table, notification_preferences_table\n  MUTATES      notifications_table, delivery_logs_table\n  OBSERVABLE\n\n  PARALLEL\n    task_email = ASYNC (\n      email_users = QUERY users_table WHERE notification_preferences_table.channel == \"email\" AND id IN recipients\n      FOR EACH user IN email_users DO\n        PERSIST notifications_table (user_id: user.id, message: message, channel: \"email\", priority: priority, created_at: now())\n      END FOR\n      RETURN COUNT(email_users)\n    )\n    task_sms = ASYNC (\n      sms_users = QUERY users_table WHERE notification_preferences_table.channel == \"sms\" AND id IN recipients\n      FOR EACH user IN sms_users DO\n        PERSIST notifications_table (user_id: user.id, message: message, channel: \"sms\", priority: priority, created_at: now())\n      END FOR\n      RETURN COUNT(sms_users)\n    )\n    task_push = ASYNC (\n      push_users = QUERY users_table WHERE notification_preferences_table.channel == \"push\" AND id IN recipients\n      FOR EACH user IN push_users DO\n        PERSIST notifications_table (user_id: user.id, message: message, channel: \"push\", priority: priority, created_at: now())\n      END FOR\n      RETURN COUNT(push_users)\n    )\n  END PARALLEL\n\n  email_count = AWAIT task_email\n  sms_count = AWAIT task_sms\n  push_count = AWAIT task_push\n  \n  total_delivered = email_count + sms_count + push_count\n  PERSIST delivery_logs_table (message_id: GENERATE_ID(), total_count: total_delivered, email_count: email_count, sms_count: sms_count, push_count: push_count, delivered_at: now())\n  \n  RETURN NotificationResult(delivery_count: total_delivered)\nEND FUNCTION broadcast_notifications [id=f0524]", "completion": "# ═══ INTL:BEGIN [id=f0524] broadcast_notifications ═══\nimport logging\nimport asyncio\nfrom datetime import datetime\nfrom typing import List\n\nlogger = logging.getLogger(__name__)\n\nasync def broadcast_notifications(message: str, recipients: List[int], priority: str) -> NotificationResult:\n    assert len(message) > 0, \"message must not be empty\"\n    assert len(recipients) > 0, \"recipients list must not be empty\"\n    assert priority in (\"high\", \"normal\", \"low\"), \"priority must be one of: high, normal, low\"\n    \n    async def task_email():\n        email_users = await db.query(users_table).join(\n            notification_preferences_table,\n            users_table.id == notification_preferences_table.user_id\n        ).filter(\n            notification_preferences_table.channel == \"email\",\n            users_table.id.in_(recipients)\n        ).all()\n        \n        for user in email_users:\n            await db.insert(notifications_table, \n                           user_id=user.id, \n                           message=message, \n                           channel=\"email\", \n                           priority=priority, \n                           created_at=datetime.utcnow())\n        return len(email_users)\n    \n    async def task_sms():\n        sms_users = await db.query(users_table).join(\n            notification_preferences_table,\n            users_table.id == notification_preferences_table.user_id\n        ).filter(\n            notification_preferences_table.channel == \"sms\",\n            users_table.id.in_(recipients)\n        ).all()\n        \n        for user in sms_users:\n            await db.insert(notifications_table, \n                           user_id=user.id, \n                           message=message, \n                           channel=\"sms\", \n                           priority=priority, \n                           created_at=datetime.utcnow())\n        return len(sms_users)\n    \n    async def task_push():\n        push_users = await db.query(users_table).join(\n            notification_preferences_table,\n            users_table.id == notification_preferences_table.user_id\n        ).filter(\n            notification_preferences_table.channel == \"push\",\n            users_table.id.in_(recipients)\n        ).all()\n        \n        for user in push_users:\n            await db.insert(notifications_table, \n                           user_id=user.id, \n                           message=message, \n                           channel=\"push\", \n                           priority=priority, \n                           created_at=datetime.utcnow())\n        return len(push_users)\n    \n    email_count, sms_count, push_count = await asyncio.gather(task_email(), task_sms(), task_push())\n    \n    total_delivered = email_count + sms_count + push_count\n    message_id = await db.generate_id()\n    \n    await db.insert(delivery_logs_table,\n                   message_id=message_id,\n                   total_count=total_delivered,\n                   email_count=email_count,\n                   sms_count=sms_count,\n                   push_count=push_count,\n                   delivered_at=datetime.utcnow())\n    \n    logger.info(\"broadcast_notifications: message_id=%s total=%d email=%d sms=%d push=%d priority=%s\", \n                message_id, total_delivered, email_count, sms_count, push_count, priority)\n    \n    result = NotificationResult(delivery_count=total_delivered)\n    assert result.delivery_count >= 0, \"delivery_count must be non-negative\"\n    return result\n# ═══ INTL:END   [id=f0524] broadcast_notifications ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "PARALLEL", "idx": 524}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION grant_permissions [id=f0846]\n  INTENT       \"grant multiple permissions to a role in batch\"\n  PRECONDITION role_id > 0\n  PRECONDITION permissions_list.length > 0\n  POSTCONDITION result.granted_count == permissions_list.length\n  READS        roles_table, permissions_table\n  MUTATES      role_permissions_table\n  OBSERVABLE\n\n  role = QUERY roles_table WHERE id == role_id LIMIT 1\n  IF role IS NULL THEN FAIL RoleError(\"role_not_found\")\n  \n  granted_count = 0\n  LOOP permission IN permissions_list DO\n    perm = QUERY permissions_table WHERE name == permission LIMIT 1\n    IF perm IS NULL THEN CONTINUE\n    \n    existing = QUERY role_permissions_table WHERE role_id == role_id AND permission_id == perm.id LIMIT 1\n    IF existing IS NOT NULL THEN CONTINUE\n    \n    PERSIST role_permissions_table (role_id: role_id, permission_id: perm.id, granted_at: now())\n    granted_count = granted_count + 1\n  END LOOP\n  \n  RETURN GrantResult(granted_count: granted_count, role_id: role_id)\nEND FUNCTION grant_permissions [id=f0846]", "completion": "# ═══ INTL:BEGIN [id=f0846] grant_permissions ═══\nimport logging\nfrom datetime import datetime\nfrom typing import List\n\nlogger = logging.getLogger(__name__)\n\nclass GrantResult:\n    def __init__(self, granted_count: int, role_id: int):\n        self.granted_count = granted_count\n        self.role_id = role_id\n\nasync def grant_permissions(role_id: int, permissions_list: List[str]) -> GrantResult:\n    assert role_id > 0, \"role_id must be greater than 0\"\n    assert len(permissions_list) > 0, \"permissions_list must not be empty\"\n    \n    role = await db.query(roles_table).filter(id=role_id).first()\n    if role is None:\n        raise RoleError(\"role_not_found\")\n    \n    granted_count = 0\n    for permission in permissions_list:\n        perm = await db.query(permissions_table).filter(name=permission).first()\n        if perm is None:\n            continue\n        \n        existing = await db.query(role_permissions_table).filter(\n            role_id=role_id, permission_id=perm.id\n        ).first()\n        if existing is not None:\n            continue\n        \n        await db.insert(role_permissions_table, role_id=role_id, \n                       permission_id=perm.id, granted_at=datetime.utcnow())\n        granted_count += 1\n    \n    logger.info(\"grant_permissions: role_id=%s, granted_count=%d\", role_id, granted_count)\n    result = GrantResult(granted_count=granted_count, role_id=role_id)\n    assert result.granted_count == len(permissions_list), \"all permissions should be granted\"\n    return result\n# ═══ INTL:END   [id=f0846] grant_permissions ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "LOOP", "idx": 846}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION log_security_event [id=f1536]\n  INTENT       \"record security-relevant events to audit log with tamper detection\"\n  PRECONDITION event_type.length > 0\n  PRECONDITION severity IN (\"LOW\", \"MEDIUM\", \"HIGH\", \"CRITICAL\")\n  PRECONDITION user_id > 0\n  POSTCONDITION result.audit_id IS NOT NULL\n  POSTCONDITION result.checksum IS NOT NULL\n  READS        audit_logs\n  MUTATES      audit_logs\n  OBSERVABLE\n\n  previous_log = QUERY audit_logs ORDER BY created_at DESC LIMIT 1\n  previous_hash = previous_log.checksum IF previous_log IS NOT NULL ELSE \"0\"\n  event_hash = hash_sha256(event_type || user_id || severity || previous_hash)\n  audit_id = generate_uuid()\n  PERSIST audit_logs (\n    audit_id: audit_id,\n    user_id: user_id,\n    event_type: event_type,\n    severity: severity,\n    checksum: event_hash,\n    previous_checksum: previous_hash,\n    created_at: now()\n  )\n  OBSERVE \"security_audit: event_type=%s severity=%s user_id=%d audit_id=%s checksum=%s\"\n  RETURN AuditRecord(audit_id: audit_id, checksum: event_hash)\nEND FUNCTION log_security_event [id=f1536]", "completion": "# ═══ INTL:BEGIN [id=f1536] log_security_event ═══\nimport logging\nimport hashlib\nimport uuid\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nasync def log_security_event(event_type: str, severity: str, user_id: int) -> AuditRecord:\n    assert len(event_type) > 0, \"event_type must not be empty\"\n    assert severity in (\"LOW\", \"MEDIUM\", \"HIGH\", \"CRITICAL\"), \\\n        \"severity must be one of LOW, MEDIUM, HIGH, CRITICAL\"\n    assert user_id > 0, \"user_id must be greater than 0\"\n    \n    previous_log = await db.query(audit_logs) \\\n        .order_by(audit_logs.created_at.desc()) \\\n        .first()\n    \n    previous_hash = previous_log.checksum if previous_log is not None else \"0\"\n    \n    combined_str = f\"{event_type}{user_id}{severity}{previous_hash}\"\n    event_hash = hashlib.sha256(combined_str.encode()).hexdigest()\n    \n    audit_id = str(uuid.uuid4())\n    \n    await db.insert(audit_logs, {\n        \"audit_id\": audit_id,\n        \"user_id\": user_id,\n        \"event_type\": event_type,\n        \"severity\": severity,\n        \"checksum\": event_hash,\n        \"previous_checksum\": previous_hash,\n        \"created_at\": datetime.utcnow()\n    })\n    \n    logger.info(\n        \"security_audit: event_type=%s severity=%s user_id=%d audit_id=%s checksum=%s\",\n        event_type, severity, user_id, audit_id, event_hash\n    )\n    \n    result = AuditRecord(audit_id=audit_id, checksum=event_hash)\n    assert result.audit_id is not None\n    assert result.checksum is not None\n    return result\n# ═══ INTL:END   [id=f1536] log_security_event ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "OBSERVABLE", "idx": 1536}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION bulk_user_verification [id=f0157]\n  INTENT       \"verify multiple users in parallel and update their status\"\n  PRECONDITION user_ids.length > 0\n  PRECONDITION user_ids.length <= 1000\n  POSTCONDITION result.verified_count >= 0\n  READS        users_table\n  MUTATES      users_table\n  OBSERVABLE\n\n  PARALLEL user_id IN user_ids\n    user = QUERY users_table WHERE id == user_id LIMIT 1\n    IF user IS NULL THEN SKIP\n    IF user.email_verified THEN SKIP\n    verification_code = generate_code(user.id)\n    PERSIST users_table (id: user.id, verification_code: verification_code, verification_sent_at: now())\n  END PARALLEL\n\n  verified_count = COUNT QUERY users_table WHERE id IN user_ids AND email_verified == TRUE\n  RETURN VerificationResult(verified_count: verified_count)\nEND FUNCTION bulk_user_verification [id=f0157]", "completion": "# ═══ INTL:BEGIN [id=f0157] bulk_user_verification ═══\nimport logging\nfrom datetime import datetime\nfrom typing import List\n\nlogger = logging.getLogger(__name__)\n\nasync def bulk_user_verification(user_ids: List[int]) -> VerificationResult:\n    assert len(user_ids) > 0, \"user_ids must not be empty\"\n    assert len(user_ids) <= 1000, \"user_ids must not exceed 1000 items\"\n    \n    tasks = []\n    for user_id in user_ids:\n        tasks.append(_verify_user_task(user_id))\n    \n    await asyncio.gather(*tasks, return_exceptions=True)\n    \n    verified_count = await db.query(users_table).filter(\n        id__in=user_ids,\n        email_verified=True\n    ).count()\n    \n    logger.info(\"bulk_user_verification: total_users=%d, verified_count=%d\",\n                len(user_ids), verified_count)\n    result = VerificationResult(verified_count=verified_count)\n    assert result.verified_count >= 0, \"verified_count must be non-negative\"\n    return result\n\nasync def _verify_user_task(user_id: int) -> None:\n    user = await db.query(users_table).filter(id=user_id).first()\n    if user is None:\n        return\n    if user.email_verified:\n        return\n    verification_code = generate_code(user.id)\n    await db.update(users_table, user_id,\n                    verification_code=verification_code,\n                    verification_sent_at=datetime.utcnow())\n# ═══ INTL:END   [id=f0157] bulk_user_verification ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "PARALLEL", "idx": 157}}
{"system": "You are the INTL compiler for Sql Postgres in PATCH mode. Apply the PATCH block to the existing code with minimal diff. Wrap output in INTL:BEGIN / INTL:END sentinels.", "prompt": "INTL:BEGIN\nCREATE FUNCTION upload_file(\n    p_user_id UUID,\n    p_filename VARCHAR,\n    p_content BYTEA\n) RETURNS TABLE(upload_id UUID, status VARCHAR) AS $$\nDECLARE\n    v_upload_id UUID;\n    v_storage_path VARCHAR;\nBEGIN\n    v_upload_id := gen_random_uuid();\n    \n    INSERT INTO uploads (id, user_id, filename, content, created_at)\n    VALUES (v_upload_id, p_user_id, p_filename, p_content, NOW());\n    \n    RETURN QUERY SELECT v_upload_id, 'success'::VARCHAR;\nEXCEPTION\n    WHEN OTHERS THEN\n        RETURN QUERY SELECT NULL::UUID, 'error'::VARCHAR;\nEND;\n$$ LANGUAGE plpgsql;\nINTL:END\n\nPATCH f0525 [target=f0525]\n  INTENT \"add fallback storage location when primary storage fails\"\n  FALLBACK TO: \"archived_uploads\" TABLE IF INSERT INTO uploads FAILS\n  ADD ON EXCEPTION: INSERT INTO archived_uploads (id, user_id, filename, content, created_at) VALUES (v_upload_id, p_user_id, p_filename, p_content, NOW())\nEND PATCH f0525", "completion": "INTL:BEGIN\nCREATE FUNCTION upload_file(\n    p_user_id UUID,\n    p_filename VARCHAR,\n    p_content BYTEA\n) RETURNS TABLE(upload_id UUID, status VARCHAR) AS $$\nDECLARE\n    v_upload_id UUID;\n    v_storage_path VARCHAR;\nBEGIN\n    v_upload_id := gen_random_uuid();\n    \n    INSERT INTO uploads (id, user_id, filename, content, created_at)\n    VALUES (v_upload_id, p_user_id, p_filename, p_content, NOW());\n    \n    RETURN QUERY SELECT v_upload_id, 'success'::VARCHAR;\nEXCEPTION\n    WHEN OTHERS THEN\n        INSERT INTO archived_uploads (id, user_id, filename, content, created_at)\n        VALUES (v_upload_id, p_user_id, p_filename, p_content, NOW());\n        RETURN QUERY SELECT v_upload_id, 'archived'::VARCHAR;\nEND;\n$$ LANGUAGE plpgsql;\nINTL:END", "metadata": {"category": "B", "adapter": "sql_postgres", "construct": "FALLBACK", "idx": 525}}
{"system": "You are the INTL compiler for Sql Postgres in PATCH mode. Apply the PATCH block to the existing code with minimal diff. Wrap output in INTL:BEGIN / INTL:END sentinels.", "prompt": "INTL:BEGIN\nSELECT user_id, email, created_at\nFROM users\nWHERE status = 'active'\nORDER BY created_at DESC;\nINTL:END\n\nPATCH f0158 [target=f0158]\n  INTENT \"add pagination to user query results\"\n  ADD BEFORE: \"ORDER BY created_at DESC\"\n  PAGINATE: LIMIT $1 OFFSET $2\nEND PATCH f0158", "completion": "INTL:BEGIN\nSELECT user_id, email, created_at\nFROM users\nWHERE status = 'active'\nORDER BY created_at DESC\nLIMIT $1 OFFSET $2;\nINTL:END", "metadata": {"category": "B", "adapter": "sql_postgres", "construct": "PAGINATE", "idx": 158}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION process_shipment [id=f1537]\n  INTENT       \"mark shipment as shipped and notify customer\"\n  PRECONDITION shipment_id > 0\n  PRECONDITION carrier_name.length > 0\n  PRECONDITION tracking_number.length > 0\n  POSTCONDITION result.status == \"shipped\"\n  POSTCONDITION result.notification_sent == TRUE\n  READS        shipments_table\n  READS        customers_table\n  MUTATES      shipments_table\n  MUTATES      notifications_table\n  OBSERVABLE\n\n  shipment = QUERY shipments_table WHERE id == shipment_id LIMIT 1\n  IF shipment IS NULL THEN FAIL ShipmentError(\"shipment_not_found\")\n  IF shipment.status != \"pending\" THEN FAIL ShipmentError(\"invalid_shipment_status\")\n  customer = QUERY customers_table WHERE id == shipment.customer_id LIMIT 1\n  IF customer IS NULL THEN FAIL ShipmentError(\"customer_not_found\")\n  PERSIST shipments_table (id: shipment_id, status: \"shipped\", carrier: carrier_name, tracking: tracking_number, shipped_at: now())\n  PERSIST notifications_table (customer_id: customer.id, type: \"shipment_shipped\", message: \"Your order has shipped\", created_at: now())\n  RETURN ShipmentResult(status: \"shipped\", notification_sent: TRUE, tracking: tracking_number)\nEND FUNCTION process_shipment [id=f1537]", "completion": "# ═══ INTL:BEGIN [id=f1537] process_shipment ═══\nimport logging\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nasync def process_shipment(shipment_id: int, carrier_name: str, tracking_number: str) -> ShipmentResult:\n    assert shipment_id > 0, \"shipment_id must be positive\"\n    assert len(carrier_name) > 0, \"carrier_name must not be empty\"\n    assert len(tracking_number) > 0, \"tracking_number must not be empty\"\n    \n    shipment = await db.query(shipments_table).filter(id=shipment_id).first()\n    if shipment is None:\n        raise ShipmentError(\"shipment_not_found\")\n    if shipment.status != \"pending\":\n        raise ShipmentError(\"invalid_shipment_status\")\n    \n    customer = await db.query(customers_table).filter(id=shipment.customer_id).first()\n    if customer is None:\n        raise ShipmentError(\"customer_not_found\")\n    \n    await db.update(shipments_table, \n                    where={\"id\": shipment_id},\n                    values={\"status\": \"shipped\", \"carrier\": carrier_name, \n                           \"tracking\": tracking_number, \"shipped_at\": datetime.utcnow()})\n    \n    await db.insert(notifications_table, \n                   customer_id=customer.id, \n                   type=\"shipment_shipped\",\n                   message=\"Your order has shipped\",\n                   created_at=datetime.utcnow())\n    \n    logger.info(\"process_shipment: shipment_id=%d, customer_id=%d, carrier=%s, tracking=%s\", \n                shipment_id, customer.id, carrier_name, tracking_number)\n    \n    result = ShipmentResult(status=\"shipped\", notification_sent=True, tracking=tracking_number)\n    assert result.status == \"shipped\"\n    assert result.notification_sent == True\n    return result\n# ═══ INTL:END   [id=f1537] process_shipment ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "OBSERVABLE", "idx": 1537}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION submit_review [id=f0526]\n  INTENT       \"submit a product review with validation and tracking\"\n  PRECONDITION user_id > 0\n  PRECONDITION product_id > 0\n  PRECONDITION rating >= 1 AND rating <= 5\n  PRECONDITION title.length > 0 AND title.length <= 200\n  PRECONDITION body.length >= 10 AND body.length <= 5000\n  POSTCONDITION result.review_id > 0\n  POSTCONDITION result.created_at IS NOT NULL\n  READS        products_table, users_table\n  MUTATES      reviews_table, review_audit_log\n  OBSERVABLE\n\n  product = QUERY products_table WHERE id == product_id LIMIT 1\n  IF product IS NULL THEN FAIL NotFoundError(\"product_not_found\")\n  user = QUERY users_table WHERE id == user_id LIMIT 1\n  IF user IS NULL THEN FAIL NotFoundError(\"user_not_found\")\n  existing = QUERY reviews_table WHERE user_id == user_id AND product_id == product_id LIMIT 1\n  IF existing IS NOT NULL THEN FAIL ConflictError(\"review_already_exists\")\n  review_id = PERSIST reviews_table (user_id: user_id, product_id: product_id, rating: rating, title: title, body: body, created_at: now(), status: \"published\")\n  PERSIST review_audit_log (review_id: review_id, action: \"created\", user_id: user_id, timestamp: now())\n  RETURN Review(review_id: review_id, user_id: user_id, product_id: product_id, rating: rating, title: title, body: body, created_at: now())\nEND FUNCTION submit_review [id=f0526]", "completion": "# ═══ INTL:BEGIN [id=f0526] submit_review ═══\nimport logging\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nasync def submit_review(user_id: int, product_id: int, rating: int, title: str, body: str) -> Review:\n    assert user_id > 0, \"user_id must be positive\"\n    assert product_id > 0, \"product_id must be positive\"\n    assert rating >= 1 and rating <= 5, \"rating must be between 1 and 5\"\n    assert len(title) > 0 and len(title) <= 200, \"title must be between 1 and 200 characters\"\n    assert len(body) >= 10 and len(body) <= 5000, \"body must be between 10 and 5000 characters\"\n    \n    product = await db.query(products_table).filter(id=product_id).first()\n    if product is None:\n        raise NotFoundError(\"product_not_found\")\n    \n    user = await db.query(users_table).filter(id=user_id).first()\n    if user is None:\n        raise NotFoundError(\"user_not_found\")\n    \n    existing = await db.query(reviews_table).filter(user_id=user_id, product_id=product_id).first()\n    if existing is not None:\n        raise ConflictError(\"review_already_exists\")\n    \n    now_timestamp = datetime.utcnow()\n    review_id = await db.insert(\n        reviews_table,\n        user_id=user_id,\n        product_id=product_id,\n        rating=rating,\n        title=title,\n        body=body,\n        created_at=now_timestamp,\n        status=\"published\"\n    )\n    \n    await db.insert(\n        review_audit_log,\n        review_id=review_id,\n        action=\"created\",\n        user_id=user_id,\n        timestamp=now_timestamp\n    )\n    \n    logger.info(\"submit_review: review_id=%d user_id=%d product_id=%d rating=%d\", review_id, user_id, product_id, rating)\n    \n    result = Review(\n        review_id=review_id,\n        user_id=user_id,\n        product_id=product_id,\n        rating=rating,\n        title=title,\n        body=body,\n        created_at=now_timestamp\n    )\n    assert result.review_id > 0\n    assert result.created_at is not None\n    return result\n# ═══ INTL:END   [id=f0526] submit_review ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "OBSERVABLE", "idx": 526}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION get_audit_log_summary [id=f1538]\n  INTENT       \"retrieve cached audit log summary for a user, with fallback to database query\"\n  PRECONDITION user_id > 0\n  PRECONDITION days_back > 0\n  PRECONDITION days_back <= 365\n  POSTCONDITION result.total_events >= 0\n  READS        audit_logs\n  MUTATES      audit_cache\n  OBSERVABLE\n\n  cache_key = \"audit_summary:\" + user_id + \":\" + days_back\n  cached = CACHE GET cache_key\n  IF cached IS NOT NULL THEN RETURN cached\n  \n  cutoff_date = now() - (days_back * 1 day)\n  summary = QUERY audit_logs \n    WHERE user_id == user_id \n    AND event_timestamp >= cutoff_date\n    GROUP BY event_type COUNT(*) as event_count\n  \n  result = AuditSummary(\n    user_id: user_id,\n    total_events: COUNT(summary),\n    events_by_type: summary,\n    generated_at: now()\n  )\n  \n  PERSIST audit_cache (cache_key: cache_key, summary_data: result, ttl: 3600)\n  RETURN result\nEND FUNCTION get_audit_log_summary [id=f1538]", "completion": "# ═══ INTL:BEGIN [id=f1538] get_audit_log_summary ═══\nimport logging\nimport json\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List\n\nlogger = logging.getLogger(__name__)\n\nasync def get_audit_log_summary(user_id: int, days_back: int) -> AuditSummary:\n    assert user_id > 0, \"user_id must be positive\"\n    assert days_back > 0, \"days_back must be positive\"\n    assert days_back <= 365, \"days_back cannot exceed 365 days\"\n    \n    cache_key = f\"audit_summary:{user_id}:{days_back}\"\n    cached = await cache.get(cache_key)\n    \n    if cached is not None:\n        logger.info(\"get_audit_log_summary: cache_hit user_id=%s days_back=%s\", user_id, days_back)\n        return AuditSummary(**json.loads(cached))\n    \n    cutoff_date = datetime.utcnow() - timedelta(days=days_back)\n    \n    events_by_type = await db.query(\"\"\"\n        SELECT event_type, COUNT(*) as event_count\n        FROM audit_logs\n        WHERE user_id = %s AND event_timestamp >= %s\n        GROUP BY event_type\n    \"\"\", (user_id, cutoff_date)).all()\n    \n    total_events = sum(row['event_count'] for row in events_by_type)\n    \n    result = AuditSummary(\n        user_id=user_id,\n        total_events=total_events,\n        events_by_type={row['event_type']: row['event_count'] for row in events_by_type},\n        generated_at=datetime.utcnow()\n    )\n    \n    await cache.set(\n        cache_key,\n        json.dumps(result.to_dict()),\n        ex=3600\n    )\n    \n    await db.insert(audit_cache, cache_key=cache_key, summary_data=result.to_json(), \n                    ttl=3600, created_at=datetime.utcnow())\n    \n    logger.info(\"get_audit_log_summary: cache_miss user_id=%s days_back=%s total_events=%s\", \n                user_id, days_back, total_events)\n    \n    assert result.total_events >= 0, \"total_events must be non-negative\"\n    return result\n# ═══ INTL:END   [id=f1538] get_audit_log_summary ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "CACHE GET", "idx": 1538}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION create_product [id=f0527]\n  INTENT       \"create a new product with inventory tracking\"\n  PRECONDITION name.length > 0\n  PRECONDITION price > 0\n  PRECONDITION stock_quantity >= 0\n  POSTCONDITION result.product_id IS NOT NULL\n  POSTCONDITION result.created_at IS NOT NULL\n  READS        categories_table\n  MUTATES      products_table, inventory_log_table\n  OBSERVABLE\n\n  category = QUERY categories_table WHERE category_id == category_id LIMIT 1\n  IF category IS NULL THEN FAIL ValidationError(\"category_not_found\")\n  product_id = generate_uuid()\n  created_at = now()\n  PERSIST products_table (product_id: product_id, name: name, price: price, category_id: category_id, created_at: created_at, updated_at: created_at)\n  PERSIST inventory_log_table (product_id: product_id, quantity_change: stock_quantity, operation_type: \"INITIAL_STOCK\", recorded_at: created_at)\n  RETURN CreateProductResult(product_id: product_id, created_at: created_at)\nEND FUNCTION create_product [id=f0527]", "completion": "# ═══ INTL:BEGIN [id=f0527] create_product ═══\nimport logging\nimport uuid\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nasync def create_product(name: str, price: float, stock_quantity: int, category_id: str) -> CreateProductResult:\n    assert len(name) > 0, \"name must not be empty\"\n    assert price > 0, \"price must be greater than zero\"\n    assert stock_quantity >= 0, \"stock_quantity must be non-negative\"\n    category = await db.query(categories_table).filter(category_id=category_id).first()\n    if category is None:\n        raise ValidationError(\"category_not_found\")\n    product_id = str(uuid.uuid4())\n    created_at = datetime.utcnow()\n    await db.insert(products_table, product_id=product_id, name=name, price=price,\n                    category_id=category_id, created_at=created_at, updated_at=created_at)\n    await db.insert(inventory_log_table, product_id=product_id, quantity_change=stock_quantity,\n                    operation_type=\"INITIAL_STOCK\", recorded_at=created_at)\n    logger.info(\"create_product: product_id=%s, name=%s, category_id=%s\", product_id, name, category_id)\n    result = CreateProductResult(product_id=product_id, created_at=created_at)\n    assert result.product_id is not None\n    assert result.created_at is not None\n    return result\n# ═══ INTL:END   [id=f0527] create_product ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "PERSIST", "idx": 527}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION process_order_fulfillment [id=f0849]\n  INTENT       \"process order items in parallel and update inventory with fulfillment status\"\n  PRECONDITION order_id > 0\n  PRECONDITION fulfillment_items.length > 0\n  POSTCONDITION result.status == \"completed\"\n  READS        orders_table, inventory_table\n  MUTATES      order_items_table, inventory_table, fulfillment_log_table\n  OBSERVABLE\n\n  order = QUERY orders_table WHERE id == order_id LIMIT 1\n  IF order IS NULL THEN FAIL OrderError(\"order_not_found\")\n  IF order.status != \"pending\" THEN FAIL OrderError(\"order_not_pending\")\n  \n  PARALLEL FOR item IN fulfillment_items\n    inventory = QUERY inventory_table WHERE sku == item.sku LIMIT 1\n    IF inventory IS NULL THEN FAIL OrderError(\"sku_not_found\")\n    IF inventory.quantity < item.quantity THEN FAIL OrderError(\"insufficient_stock\")\n    PERSIST order_items_table (order_id: order_id, sku: item.sku, quantity: item.quantity, status: \"fulfilled\")\n    PERSIST inventory_table (sku: item.sku, quantity: inventory.quantity - item.quantity)\n  END PARALLEL\n  \n  PERSIST fulfillment_log_table (order_id: order_id, processed_at: now(), item_count: fulfillment_items.length)\n  RETURN FulfillmentResult(status: \"completed\", order_id: order_id)\nEND FUNCTION process_order_fulfillment [id=f0849]", "completion": "# ═══ INTL:BEGIN [id=f0849] process_order_fulfillment ═══\nimport logging\nfrom datetime import datetime\nfrom typing import List\n\nlogger = logging.getLogger(__name__)\n\nclass FulfillmentItem:\n    def __init__(self, sku: str, quantity: int):\n        self.sku = sku\n        self.quantity = quantity\n\nclass FulfillmentResult:\n    def __init__(self, status: str, order_id: int):\n        self.status = status\n        self.order_id = order_id\n\nasync def process_order_fulfillment(order_id: int, fulfillment_items: List[FulfillmentItem]) -> FulfillmentResult:\n    assert order_id > 0, \"order_id must be greater than 0\"\n    assert len(fulfillment_items) > 0, \"fulfillment_items must not be empty\"\n    \n    order = await db.query(orders_table).filter(id=order_id).first()\n    if order is None:\n        raise OrderError(\"order_not_found\")\n    if order.status != \"pending\":\n        raise OrderError(\"order_not_pending\")\n    \n    async def process_item(item: FulfillmentItem):\n        inventory = await db.query(inventory_table).filter(sku=item.sku).first()\n        if inventory is None:\n            raise OrderError(\"sku_not_found\")\n        if inventory.quantity < item.quantity:\n            raise OrderError(\"insufficient_stock\")\n        await db.insert(order_items_table, order_id=order_id, sku=item.sku, \n                       quantity=item.quantity, status=\"fulfilled\")\n        await db.update(inventory_table, \n                       where={\"sku\": item.sku},\n                       values={\"quantity\": inventory.quantity - item.quantity})\n    \n    import asyncio\n    await asyncio.gather(*[process_item(item) for item in fulfillment_items])\n    \n    await db.insert(fulfillment_log_table, order_id=order_id, \n                   processed_at=datetime.utcnow(), item_count=len(fulfillment_items))\n    \n    logger.info(\"process_order_fulfillment: order_id=%d, items=%d\", order_id, len(fulfillment_items))\n    result = FulfillmentResult(status=\"completed\", order_id=order_id)\n    assert result.status == \"completed\"\n    return result\n# ═══ INTL:END   [id=f0849] process_order_fulfillment ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "PARALLEL", "idx": 849}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION deactivate_user [id=f0160]\n  INTENT       \"deactivate a user account and archive their data\"\n  PRECONDITION user_id > 0\n  PRECONDITION reason.length > 0\n  PRECONDITION reason.length <= 500\n  POSTCONDITION result.success IS TRUE\n  READS        users_table\n  MUTATES      users_table, audit_log_table\n  OBSERVABLE\n\n  user = QUERY users_table WHERE id == user_id LIMIT 1\n  IF user IS NULL THEN FAIL UserError(\"user_not_found\")\n  IF user.status == \"inactive\" THEN FAIL UserError(\"user_already_inactive\")\n  IF user.is_admin AND NOT admin_override THEN FAIL UserError(\"cannot_deactivate_admin_without_override\")\n  UPDATE users_table SET status = \"inactive\", deactivated_at = now() WHERE id = user_id\n  PERSIST audit_log_table (user_id: user_id, action: \"deactivate\", reason: reason, timestamp: now())\n  RETURN DeactivationResult(success: TRUE, deactivated_user_id: user_id)\nEND FUNCTION deactivate_user [id=f0160]", "completion": "# ═══ INTL:BEGIN [id=f0160] deactivate_user ═══\nimport logging\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nasync def deactivate_user(user_id: int, reason: str, admin_override: bool = False) -> DeactivationResult:\n    assert user_id > 0, \"user_id must be greater than 0\"\n    assert len(reason) > 0, \"reason must not be empty\"\n    assert len(reason) <= 500, \"reason must be at most 500 characters\"\n    \n    user = await db.query(users_table).filter(id=user_id).first()\n    if user is None:\n        raise UserError(\"user_not_found\")\n    if user.status == \"inactive\":\n        raise UserError(\"user_already_inactive\")\n    if user.is_admin and not admin_override:\n        raise UserError(\"cannot_deactivate_admin_without_override\")\n    \n    await db.update(users_table).set(\n        status=\"inactive\",\n        deactivated_at=datetime.utcnow()\n    ).where(id=user_id).execute()\n    \n    await db.insert(audit_log_table, \n                    user_id=user_id, \n                    action=\"deactivate\", \n                    reason=reason, \n                    timestamp=datetime.utcnow())\n    \n    logger.info(\"deactivate_user: user_id=%s reason=%s admin_override=%s\", \n                user_id, reason, admin_override)\n    \n    result = DeactivationResult(success=True, deactivated_user_id=user_id)\n    assert result.success is True\n    return result\n# ═══ INTL:END   [id=f0160] deactivate_user ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "FAIL", "idx": 160}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION renew_subscription [id=f0528]\n  INTENT       \"renew an active subscription and extend expiration date\"\n  PRECONDITION subscription_id.length > 0\n  PRECONDITION renewal_months > 0\n  PRECONDITION renewal_months <= 60\n  POSTCONDITION result.new_expiry IS NOT NULL\n  POSTCONDITION result.status == \"active\"\n  READS        subscriptions_table\n  MUTATES      subscriptions_table\n  OBSERVABLE\n\n  sub = QUERY subscriptions_table WHERE id == subscription_id LIMIT 1\n  IF sub IS NULL THEN FAIL SubscriptionError(\"subscription_not_found\")\n  IF sub.status != \"active\" THEN FAIL SubscriptionError(\"subscription_inactive\")\n  IF sub.expiry_date < now() THEN FAIL SubscriptionError(\"subscription_expired\")\n  new_expiry = sub.expiry_date + months(renewal_months)\n  PERSIST subscriptions_table (id: subscription_id, expiry_date: new_expiry, renewed_at: now(), renewal_count: sub.renewal_count + 1)\n  RETURN SubscriptionRenewal(subscription_id: subscription_id, new_expiry: new_expiry, status: \"active\")\nEND FUNCTION renew_subscription [id=f0528]", "completion": "# ═══ INTL:BEGIN [id=f0528] renew_subscription ═══\nimport logging\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\n\nlogger = logging.getLogger(__name__)\n\nasync def renew_subscription(subscription_id: str, renewal_months: int) -> SubscriptionRenewal:\n    assert len(subscription_id) > 0, \"subscription_id must not be empty\"\n    assert renewal_months > 0, \"renewal_months must be positive\"\n    assert renewal_months <= 60, \"renewal_months must not exceed 60\"\n    \n    sub = await db.query(subscriptions_table).filter(id=subscription_id).first()\n    if sub is None:\n        raise SubscriptionError(\"subscription_not_found\")\n    if sub.status != \"active\":\n        raise SubscriptionError(\"subscription_inactive\")\n    if sub.expiry_date < datetime.utcnow():\n        raise SubscriptionError(\"subscription_expired\")\n    \n    new_expiry = sub.expiry_date + relativedelta(months=renewal_months)\n    \n    await db.update(\n        subscriptions_table,\n        where={\"id\": subscription_id},\n        values={\n            \"expiry_date\": new_expiry,\n            \"renewed_at\": datetime.utcnow(),\n            \"renewal_count\": sub.renewal_count + 1\n        }\n    )\n    \n    logger.info(\"renew_subscription: subscription_id=%s, new_expiry=%s, renewal_count=%d\",\n                subscription_id, new_expiry, sub.renewal_count + 1)\n    \n    result = SubscriptionRenewal(\n        subscription_id=subscription_id,\n        new_expiry=new_expiry,\n        status=\"active\"\n    )\n    assert result.new_expiry is not None\n    assert result.status == \"active\"\n    return result\n# ═══ INTL:END   [id=f0528] renew_subscription ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "PERSIST", "idx": 528}}
{"system": "You are the INTL compiler for Sql Postgres in PATCH mode. Apply the PATCH block to the existing code with minimal diff. Wrap output in INTL:BEGIN / INTL:END sentinels.", "prompt": "INTL:BEGIN\nSELECT \n  user_id,\n  COUNT(*) as session_count,\n  MAX(last_activity) as last_active,\n  STRING_AGG(session_id, ',') as sessions\nFROM sessions\nWHERE created_at > NOW() - INTERVAL '30 days'\nGROUP BY user_id\nORDER BY session_count DESC;\nINTL:END\n\nPATCH f1540 [target=f1540]\n  INTENT \"add session status filtering to aggregate results\"\n  ADD BEFORE: \"GROUP BY user_id\"\n  WHERE: \"status = 'active'\"\nEND PATCH f1540", "completion": "INTL:BEGIN\nSELECT \n  user_id,\n  COUNT(*) as session_count,\n  MAX(last_activity) as last_active,\n  STRING_AGG(session_id, ',') as sessions\nFROM sessions\nWHERE created_at > NOW() - INTERVAL '30 days'\n  AND status = 'active'\nGROUP BY user_id\nORDER BY session_count DESC;\nINTL:END", "metadata": {"category": "B", "adapter": "sql_postgres", "construct": "AGGREGATE", "idx": 1540}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION generate_report [id=f0850]\n  INTENT       \"generate quarterly sales report with conditional aggregation\"\n  PRECONDITION quarter > 0\n  PRECONDITION quarter <= 4\n  PRECONDITION year >= 2020\n  POSTCONDITION result.report_id IS NOT NULL\n  READS        sales_transactions, products_catalog, reporting_config\n  MUTATES      report_cache, audit_log\n  OBSERVABLE\n\n  config = QUERY reporting_config WHERE active == true LIMIT 1\n  IF config IS NULL THEN FAIL ConfigError(\"no_active_config\")\n  \n  total_sales = QUERY sales_transactions WHERE quarter == quarter AND year == year AGGREGATE SUM(amount)\n  IF total_sales < config.minimum_threshold THEN\n    status = \"below_threshold\"\n    include_details = false\n  ELSE\n    status = \"complete\"\n    include_details = true\n  END IF\n\n  IF include_details THEN\n    top_products = QUERY products_catalog INNER JOIN sales_transactions ON products_catalog.id == sales_transactions.product_id WHERE quarter == quarter AND year == year ORDER BY sales_transactions.amount DESC LIMIT 10\n  ELSE\n    top_products = EMPTY\n  END IF\n\n  report_id = generate_uuid()\n  PERSIST report_cache (report_id: report_id, quarter: quarter, year: year, total_sales: total_sales, status: status, generated_at: now())\n  PERSIST audit_log (action: \"report_generated\", report_id: report_id, user: current_user(), timestamp: now())\n  \n  RETURN ReportResult(report_id: report_id, status: status, total_sales: total_sales, top_products: top_products)\nEND FUNCTION generate_report [id=f0850]", "completion": "# ═══ INTL:BEGIN [id=f0850] generate_report ═══\nimport logging\nimport uuid\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nasync def generate_report(quarter: int, year: int, current_user: str) -> ReportResult:\n    assert quarter > 0, \"quarter must be greater than 0\"\n    assert quarter <= 4, \"quarter must be at most 4\"\n    assert year >= 2020, \"year must be at least 2020\"\n    \n    config = await db.query(reporting_config).filter(active=True).first()\n    if config is None:\n        raise ConfigError(\"no_active_config\")\n    \n    total_sales_row = await db.query(sales_transactions).filter(\n        quarter=quarter, year=year\n    ).aggregate(total=db.func.sum(db.Column('amount'))).first()\n    total_sales = total_sales_row['total'] or 0\n    \n    if total_sales < config['minimum_threshold']:\n        status = \"below_threshold\"\n        include_details = False\n    else:\n        status = \"complete\"\n        include_details = True\n    \n    if include_details:\n        top_products = await db.query(products_catalog).join(\n            sales_transactions,\n            products_catalog.id == sales_transactions.product_id\n        ).filter(\n            quarter=quarter, year=year\n        ).order_by(sales_transactions.amount.desc()).limit(10).all()\n    else:\n        top_products = []\n    \n    report_id = str(uuid.uuid4())\n    \n    await db.insert(report_cache, report_id=report_id, quarter=quarter, year=year,\n                    total_sales=total_sales, status=status, generated_at=datetime.utcnow())\n    \n    await db.insert(audit_log, action=\"report_generated\", report_id=report_id,\n                    user=current_user, timestamp=datetime.utcnow())\n    \n    logger.info(\"generate_report: report_id=%s status=%s total_sales=%s\", report_id, status, total_sales)\n    \n    result = ReportResult(report_id=report_id, status=status, total_sales=total_sales,\n                          top_products=top_products)\n    assert result.report_id is not None\n    return result\n# ═══ INTL:END   [id=f0850] generate_report ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "IF/THEN/ELSE", "idx": 850}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION process_refunds [id=f0161]\n  INTENT       \"process multiple refunds in parallel and reconcile ledger entries\"\n  PRECONDITION refund_ids.length > 0\n  PRECONDITION refund_ids.length <= 1000\n  PRECONDITION merchant_id.length > 0\n  POSTCONDITION result.processed_count >= 0\n  POSTCONDITION result.failed_count >= 0\n  READS        refunds_table\n  READS        transactions_table\n  READS        merchants_table\n  MUTATES      refunds_table\n  MUTATES      ledger_entries_table\n  MUTATES      reconciliation_log_table\n  OBSERVABLE\n\n  merchant = QUERY merchants_table WHERE id == merchant_id LIMIT 1\n  IF merchant IS NULL THEN FAIL MerchantError(\"merchant_not_found\")\n  IF merchant.status != \"active\" THEN FAIL MerchantError(\"merchant_inactive\")\n\n  PARALLEL FOR refund_id IN refund_ids\n    refund = QUERY refunds_table WHERE id == refund_id AND merchant_id == merchant_id LIMIT 1\n    IF refund IS NULL THEN CONTINUE\n    IF refund.status == \"completed\" THEN CONTINUE\n    \n    transaction = QUERY transactions_table WHERE id == refund.transaction_id LIMIT 1\n    IF transaction IS NULL THEN CONTINUE\n    \n    PERSIST refunds_table (id: refund_id, status: \"processing\", updated_at: now())\n    PERSIST ledger_entries_table (merchant_id: merchant_id, refund_id: refund_id, amount: transaction.amount, entry_type: \"refund\", timestamp: now())\n  END PARALLEL\n\n  PERSIST reconciliation_log_table (merchant_id: merchant_id, refund_batch_size: refund_ids.length, processed_at: now(), status: \"completed\")\n  \n  processed = COUNT refunds_table WHERE merchant_id == merchant_id AND status == \"processing\"\n  failed = refund_ids.length - processed\n  \n  RETURN RefundResult(processed_count: processed, failed_count: failed)\nEND FUNCTION process_refunds [id=f0161]", "completion": "# ═══ INTL:BEGIN [id=f0161] process_refunds ═══\nimport logging\nimport asyncio\nfrom datetime import datetime\nfrom typing import List\n\nlogger = logging.getLogger(__name__)\n\nasync def process_refunds(merchant_id: str, refund_ids: List[str]) -> RefundResult:\n    assert len(refund_ids) > 0, \"refund_ids must not be empty\"\n    assert len(refund_ids) <= 1000, \"refund_ids must not exceed 1000 items\"\n    assert len(merchant_id) > 0, \"merchant_id must not be empty\"\n    \n    merchant = await db.query(merchants_table).filter(id=merchant_id).first()\n    if merchant is None:\n        raise MerchantError(\"merchant_not_found\")\n    if merchant.status != \"active\":\n        raise MerchantError(\"merchant_inactive\")\n    \n    async def process_single_refund(refund_id: str):\n        refund = await db.query(refunds_table).filter(\n            id=refund_id,\n            merchant_id=merchant_id\n        ).first()\n        if refund is None:\n            return False\n        if refund.status == \"completed\":\n            return False\n        \n        transaction = await db.query(transactions_table).filter(\n            id=refund.transaction_id\n        ).first()\n        if transaction is None:\n            return False\n        \n        await db.update(refunds_table).filter(id=refund_id).set(\n            status=\"processing\",\n            updated_at=datetime.utcnow()\n        )\n        await db.insert(ledger_entries_table,\n            merchant_id=merchant_id,\n            refund_id=refund_id,\n            amount=transaction.amount,\n            entry_type=\"refund\",\n            timestamp=datetime.utcnow()\n        )\n        return True\n    \n    results = await asyncio.gather(*[\n        process_single_refund(rid) for rid in refund_ids\n    ], return_exceptions=False)\n    \n    await db.insert(reconciliation_log_table,\n        merchant_id=merchant_id,\n        refund_batch_size=len(refund_ids),\n        processed_at=datetime.utcnow(),\n        status=\"completed\"\n    )\n    \n    processed = sum(1 for r in results if r)\n    failed = len(refund_ids) - processed\n    \n    logger.info(\"process_refunds: merchant=%s batch_size=%d processed=%d failed=%d\",\n                merchant_id, len(refund_ids), processed, failed)\n    \n    result = RefundResult(processed_count=processed, failed_count=failed)\n    assert result.processed_count >= 0\n    assert result.failed_count >= 0\n    return result\n# ═══ INTL:END   [id=f0161] process_refunds ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "PARALLEL", "idx": 161}}
{"system": "You are the INTL compiler for Sql Postgres in PATCH mode. Apply the PATCH block to the existing code with minimal diff. Wrap output in INTL:BEGIN / INTL:END sentinels.", "prompt": "INTL:BEGIN\nDO $$\nDECLARE\n  api_key_record RECORD;\nBEGIN\n  FOR api_key_record IN SELECT id, key_hash, user_id FROM api_keys WHERE active = true\n  LOOP\n    UPDATE audit_log SET last_checked = NOW() WHERE api_key_id = api_key_record.id;\n  END LOOP;\nEND $$;\nINTL:END\n\nPATCH f0529 [target=f0529]\n  INTENT \"add validation check for each api key in the loop\"\n  ADD BEFORE: RAISE NOTICE 'Validating key: %', api_key_record.id;\nEND PATCH f0529", "completion": "INTL:BEGIN\nDO $$\nDECLARE\n  api_key_record RECORD;\nBEGIN\n  FOR api_key_record IN SELECT id, key_hash, user_id FROM api_keys WHERE active = true\n  LOOP\n    RAISE NOTICE 'Validating key: %', api_key_record.id;\n    UPDATE audit_log SET last_checked = NOW() WHERE api_key_id = api_key_record.id;\n  END LOOP;\nEND $$;\nINTL:END", "metadata": {"category": "B", "adapter": "sql_postgres", "construct": "LOOP", "idx": 529}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION revoke_session [id=f0851]\n  INTENT       \"revoke an active session and log the revocation event\"\n  PRECONDITION session_id.length > 0\n  PRECONDITION user_id > 0\n  POSTCONDITION result.revoked_at IS NOT NULL\n  READS        sessions_table\n  MUTATES      sessions_table, revocation_log\n  OBSERVABLE\n\n  session = QUERY sessions_table WHERE id == session_id AND user_id == user_id LIMIT 1\n  IF session IS NULL THEN FAIL SessionError(\"session_not_found\")\n  IF session.revoked_at IS NOT NULL THEN FAIL SessionError(\"session_already_revoked\")\n  IF session.expires_at < now() THEN FAIL SessionError(\"session_expired\")\n  revoked_at = now()\n  UPDATE sessions_table SET revoked_at = revoked_at WHERE id = session_id\n  PERSIST revocation_log (session_id: session_id, user_id: user_id, revoked_at: revoked_at, reason: \"manual_revocation\")\n  RETURN RevokeResult(revoked_at: revoked_at)\nEND FUNCTION revoke_session [id=f0851]", "completion": "# ═══ INTL:BEGIN [id=f0851] revoke_session ═══\nimport logging\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nasync def revoke_session(session_id: str, user_id: int) -> RevokeResult:\n    assert len(session_id) > 0, \"session_id must not be empty\"\n    assert user_id > 0, \"user_id must be positive\"\n    \n    session = await db.query(sessions_table).filter(\n        id=session_id,\n        user_id=user_id\n    ).first()\n    \n    if session is None:\n        raise SessionError(\"session_not_found\")\n    \n    if session.revoked_at is not None:\n        raise SessionError(\"session_already_revoked\")\n    \n    if session.expires_at < datetime.utcnow():\n        raise SessionError(\"session_expired\")\n    \n    revoked_at = datetime.utcnow()\n    \n    await db.update(sessions_table).filter(id=session_id).set(revoked_at=revoked_at)\n    \n    await db.insert(\n        revocation_log,\n        session_id=session_id,\n        user_id=user_id,\n        revoked_at=revoked_at,\n        reason=\"manual_revocation\"\n    )\n    \n    logger.info(\"revoke_session: session=%s user=%s revoked_at=%s\", \n                session_id, user_id, revoked_at)\n    \n    result = RevokeResult(revoked_at=revoked_at)\n    assert result.revoked_at is not None\n    return result\n# ═══ INTL:END   [id=f0851] revoke_session ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "FAIL", "idx": 851}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION send_notification [id=f0162]\n  INTENT       \"send notification to user and record delivery\"\n  PRECONDITION user_id > 0\n  PRECONDITION message.length > 0\n  PRECONDITION message.length <= 500\n  POSTCONDITION result.notification_id IS NOT NULL\n  POSTCONDITION result.sent_at IS NOT NULL\n  READS        users_table\n  MUTATES      notifications_table\n  MUTATES      notification_log_table\n  OBSERVABLE\n\n  user = QUERY users_table WHERE id == user_id LIMIT 1\n  IF user IS NULL THEN FAIL NotFoundError(\"user_not_found\")\n  IF user.is_active == FALSE THEN FAIL ValidationError(\"user_inactive\")\n  notification_id = generate_uuid()\n  sent_at = now()\n  PERSIST notifications_table (id: notification_id, user_id: user_id, message: message, sent_at: sent_at, read: FALSE)\n  PERSIST notification_log_table (notification_id: notification_id, user_id: user_id, event: \"sent\", timestamp: now())\n  RETURN NotificationResult(notification_id: notification_id, sent_at: sent_at)\nEND FUNCTION send_notification [id=f0162]", "completion": "# ═══ INTL:BEGIN [id=f0162] send_notification ═══\nimport logging\nimport uuid\nfrom datetime import datetime\nfrom typing import NamedTuple\n\nlogger = logging.getLogger(__name__)\n\nclass NotificationResult(NamedTuple):\n    notification_id: str\n    sent_at: datetime\n\nasync def send_notification(user_id: int, message: str) -> NotificationResult:\n    assert user_id > 0, \"user_id must be greater than 0\"\n    assert len(message) > 0, \"message must not be empty\"\n    assert len(message) <= 500, \"message must not exceed 500 characters\"\n    \n    user = await db.query(users_table).filter(id=user_id).first()\n    if user is None:\n        raise NotFoundError(\"user_not_found\")\n    if user.is_active is False:\n        raise ValidationError(\"user_inactive\")\n    \n    notification_id = str(uuid.uuid4())\n    sent_at = datetime.utcnow()\n    \n    await db.insert(\n        notifications_table,\n        id=notification_id,\n        user_id=user_id,\n        message=message,\n        sent_at=sent_at,\n        read=False\n    )\n    \n    await db.insert(\n        notification_log_table,\n        notification_id=notification_id,\n        user_id=user_id,\n        event=\"sent\",\n        timestamp=datetime.utcnow()\n    )\n    \n    logger.info(\"send_notification: notification_id=%s user_id=%s\", notification_id, user_id)\n    \n    result = NotificationResult(notification_id=notification_id, sent_at=sent_at)\n    assert result.notification_id is not None\n    assert result.sent_at is not None\n    return result\n# ═══ INTL:END   [id=f0162] send_notification ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "RETURN", "idx": 162}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION submit_review [id=f1541]\n  INTENT       \"create a new product review and update product rating\"\n  PRECONDITION user_id > 0\n  PRECONDITION product_id > 0\n  PRECONDITION rating >= 1 AND rating <= 5\n  PRECONDITION comment.length > 0\n  PRECONDITION comment.length <= 5000\n  POSTCONDITION result.review_id IS NOT NULL\n  POSTCONDITION result.created_at IS NOT NULL\n  READS        products_table\n  MUTATES      reviews_table\n  MUTATES      products_table\n  OBSERVABLE\n\n  product = QUERY products_table WHERE id == product_id LIMIT 1\n  IF product IS NULL THEN FAIL NotFoundError(\"product_not_found\")\n  \n  review_id = generate_uuid()\n  PERSIST reviews_table (id: review_id, user_id: user_id, product_id: product_id, rating: rating, comment: comment, created_at: now())\n  \n  new_avg_rating = QUERY reviews_table WHERE product_id == product_id AGGREGATE AVG(rating)\n  review_count = QUERY reviews_table WHERE product_id == product_id AGGREGATE COUNT(*)\n  \n  PERSIST products_table WHERE id == product_id SET (avg_rating: new_avg_rating, review_count: review_count, updated_at: now())\n  \n  RETURN ReviewResult(review_id: review_id, created_at: now())\nEND FUNCTION submit_review [id=f1541]", "completion": "# ═══ INTL:BEGIN [id=f1541] submit_review ═══\nimport logging\nimport uuid\nfrom datetime import datetime\nfrom decimal import Decimal\n\nlogger = logging.getLogger(__name__)\n\nasync def submit_review(user_id: int, product_id: int, rating: int, comment: str) -> ReviewResult:\n    assert user_id > 0, \"user_id must be positive\"\n    assert product_id > 0, \"product_id must be positive\"\n    assert 1 <= rating <= 5, \"rating must be between 1 and 5\"\n    assert len(comment) > 0, \"comment must not be empty\"\n    assert len(comment) <= 5000, \"comment must be at most 5000 chars\"\n    \n    product = await db.query(products_table).filter(id=product_id).first()\n    if product is None:\n        raise NotFoundError(\"product_not_found\")\n    \n    review_id = str(uuid.uuid4())\n    created_at = datetime.utcnow()\n    \n    await db.insert(reviews_table, \n                   id=review_id, \n                   user_id=user_id, \n                   product_id=product_id, \n                   rating=rating, \n                   comment=comment, \n                   created_at=created_at)\n    \n    new_avg_rating = await db.query(reviews_table).filter(product_id=product_id).aggregate_avg('rating')\n    review_count = await db.query(reviews_table).filter(product_id=product_id).aggregate_count()\n    \n    await db.update(products_table, \n                   where={'id': product_id},\n                   set={'avg_rating': new_avg_rating, 'review_count': review_count, 'updated_at': datetime.utcnow()})\n    \n    logger.info(\"submit_review: review_id=%s, user_id=%s, product_id=%s, rating=%d\", review_id, user_id, product_id, rating)\n    \n    result = ReviewResult(review_id=review_id, created_at=created_at)\n    assert result.review_id is not None\n    assert result.created_at is not None\n    return result\n# ═══ INTL:END   [id=f1541] submit_review ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "PERSIST", "idx": 1541}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION issue_coupon [id=f0530]\n  INTENT       \"create and issue a new coupon with validation and persistence\"\n  PRECONDITION code.length > 0\n  PRECONDITION discount_percent >= 0 AND discount_percent <= 100\n  PRECONDITION max_uses > 0\n  PRECONDITION expiry_date > now()\n  POSTCONDITION result.coupon_id IS NOT NULL\n  POSTCONDITION result.status == \"active\"\n  READS        coupons_table\n  MUTATES      coupons_table\n  OBSERVABLE\n\n  existing = QUERY coupons_table WHERE code == code LIMIT 1\n  IF existing IS NOT NULL THEN FAIL DuplicateError(\"coupon_code_exists\")\n  coupon_id = generate_uuid()\n  PERSIST coupons_table (id: coupon_id, code: code, discount_percent: discount_percent, max_uses: max_uses, uses_count: 0, expiry_date: expiry_date, status: \"active\", created_at: now())\n  RETURN CouponIssued(coupon_id: coupon_id, status: \"active\", code: code)\nEND FUNCTION issue_coupon [id=f0530]", "completion": "# ═══ INTL:BEGIN [id=f0530] issue_coupon ═══\nimport logging\nimport uuid\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nasync def issue_coupon(code: str, discount_percent: float, max_uses: int, expiry_date: datetime) -> CouponIssued:\n    assert len(code) > 0, \"code must not be empty\"\n    assert 0 <= discount_percent <= 100, \"discount_percent must be between 0 and 100\"\n    assert max_uses > 0, \"max_uses must be greater than 0\"\n    assert expiry_date > datetime.utcnow(), \"expiry_date must be in the future\"\n    \n    existing = await db.query(coupons_table).filter(code=code).first()\n    if existing is not None:\n        raise DuplicateError(\"coupon_code_exists\")\n    \n    coupon_id = str(uuid.uuid4())\n    await db.insert(\n        coupons_table,\n        id=coupon_id,\n        code=code,\n        discount_percent=discount_percent,\n        max_uses=max_uses,\n        uses_count=0,\n        expiry_date=expiry_date,\n        status=\"active\",\n        created_at=datetime.utcnow()\n    )\n    \n    logger.info(\"issue_coupon: coupon_id=%s code=%s discount_percent=%s\", coupon_id, code, discount_percent)\n    result = CouponIssued(coupon_id=coupon_id, status=\"active\", code=code)\n    assert result.coupon_id is not None\n    assert result.status == \"active\"\n    return result\n# ═══ INTL:END   [id=f0530] issue_coupon ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "PERSIST", "idx": 530}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION aggregate_review_metrics [id=f0531]\n  INTENT       \"compute average ratings and review counts in parallel for multiple products\"\n  PRECONDITION product_ids.length > 0\n  PRECONDITION product_ids.length <= 1000\n  POSTCONDITION result.metrics IS NOT NULL\n  READS        reviews_table, products_table\n  MUTATES      product_metrics_table\n  OBSERVABLE\n\n  PARALLEL\n    metrics = QUERY reviews_table GROUP BY product_id AGGREGATE (avg_rating: AVG(rating), review_count: COUNT(*))\n    product_info = QUERY products_table WHERE id IN product_ids\n  END PARALLEL\n\n  FOR EACH metric IN metrics DO\n    IF metric.product_id NOT IN product_ids THEN CONTINUE\n    normalized_rating = CLAMP(metric.avg_rating, 0, 5)\n    PERSIST product_metrics_table (product_id: metric.product_id, avg_rating: normalized_rating, total_reviews: metric.review_count, updated_at: now())\n  END FOR\n\n  RETURN AggregatedMetrics(metrics: metrics, product_count: product_ids.length)\nEND FUNCTION aggregate_review_metrics [id=f0531]", "completion": "# ═══ INTL:BEGIN [id=f0531] aggregate_review_metrics ═══\nimport logging\nfrom datetime import datetime\nfrom typing import List, Dict, Any\n\nlogger = logging.getLogger(__name__)\n\nasync def aggregate_review_metrics(product_ids: List[int]) -> AggregatedMetrics:\n    assert len(product_ids) > 0, \"product_ids must not be empty\"\n    assert len(product_ids) <= 1000, \"product_ids length must not exceed 1000\"\n    \n    # Execute queries in parallel\n    metrics_task = db.query(reviews_table)\\\n        .group_by(\"product_id\")\\\n        .select(\"product_id\", db.func.avg(\"rating\").label(\"avg_rating\"), \n                db.func.count(\"*\").label(\"review_count\"))\\\n        .all()\n    \n    product_info_task = db.query(products_table)\\\n        .filter(products_table.id.in_(product_ids))\\\n        .all()\n    \n    metrics, product_info = await asyncio.gather(metrics_task, product_info_task)\n    \n    product_id_set = set(product_ids)\n    \n    for metric in metrics:\n        if metric.product_id not in product_id_set:\n            continue\n        \n        normalized_rating = max(0, min(5, metric.avg_rating))\n        \n        await db.insert(product_metrics_table).values(\n            product_id=metric.product_id,\n            avg_rating=normalized_rating,\n            total_reviews=metric.review_count,\n            updated_at=datetime.utcnow()\n        ).on_conflict_do_update(\n            index_elements=[\"product_id\"],\n            set_=dict(\n                avg_rating=normalized_rating,\n                total_reviews=metric.review_count,\n                updated_at=datetime.utcnow()\n            )\n        )\n    \n    logger.info(\"aggregate_review_metrics: processed product_count=%d, metrics_count=%d\",\n                len(product_ids), len(metrics))\n    \n    result = AggregatedMetrics(metrics=metrics, product_count=len(product_ids))\n    assert result.metrics is not None, \"metrics must not be null\"\n    return result\n# ═══ INTL:END   [id=f0531] aggregate_review_metrics ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "PARALLEL", "idx": 531}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION upgrade_subscription [id=f0853]\n  INTENT       \"upgrade subscription tier and apply prorated charges\"\n  PRECONDITION subscription_id.length > 0\n  PRECONDITION new_tier IN (\"pro\", \"enterprise\", \"premium\")\n  PRECONDITION current_tier != new_tier\n  POSTCONDITION result.status == \"active\"\n  POSTCONDITION result.tier == new_tier\n  READS        subscriptions_table\n  READS        pricing_table\n  MUTATES      subscriptions_table\n  MUTATES      billing_events_table\n  OBSERVABLE\n\n  sub = QUERY subscriptions_table WHERE id == subscription_id LIMIT 1\n  IF sub IS NULL THEN FAIL NotFoundError(\"subscription_not_found\")\n  IF sub.status != \"active\" THEN FAIL StateError(\"subscription_not_active\")\n  \n  current_price = QUERY pricing_table WHERE tier == sub.tier LIMIT 1\n  new_price = QUERY pricing_table WHERE tier == new_tier LIMIT 1\n  \n  days_remaining = (sub.next_billing_date - now()) / 86400\n  proration_factor = days_remaining / 30\n  upgrade_charge = (new_price.monthly_usd - current_price.monthly_usd) * proration_factor\n  \n  PATCH subscriptions_table\n    WHERE id == subscription_id\n    SET tier = new_tier\n    SET updated_at = now()\n  \n  PERSIST billing_events_table (\n    subscription_id: subscription_id,\n    event_type: \"upgrade\",\n    amount_usd: upgrade_charge,\n    old_tier: sub.tier,\n    new_tier: new_tier,\n    created_at: now()\n  )\n  \n  RETURN Subscription(\n    id: subscription_id,\n    tier: new_tier,\n    status: \"active\",\n    upgrade_charge: upgrade_charge\n  )\nEND FUNCTION upgrade_subscription [id=f0853]", "completion": "# ═══ INTL:BEGIN [id=f0853] upgrade_subscription ═══\nimport logging\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nasync def upgrade_subscription(subscription_id: str, new_tier: str) -> Subscription:\n    assert len(subscription_id) > 0, \"subscription_id must not be empty\"\n    assert new_tier in (\"pro\", \"enterprise\", \"premium\"), \"new_tier must be valid\"\n    \n    sub = await db.query(subscriptions_table).filter(id=subscription_id).first()\n    if sub is None:\n        raise NotFoundError(\"subscription_not_found\")\n    if sub.status != \"active\":\n        raise StateError(\"subscription_not_active\")\n    assert sub.tier != new_tier, \"current_tier must differ from new_tier\"\n    \n    current_price = await db.query(pricing_table).filter(tier=sub.tier).first()\n    new_price = await db.query(pricing_table).filter(tier=new_tier).first()\n    \n    now_ts = datetime.utcnow()\n    days_remaining = (sub.next_billing_date - now_ts).total_seconds() / 86400\n    proration_factor = days_remaining / 30\n    upgrade_charge = (new_price.monthly_usd - current_price.monthly_usd) * proration_factor\n    \n    await db.update(subscriptions_table).where(id=subscription_id).set(\n        tier=new_tier,\n        updated_at=now_ts\n    )\n    \n    await db.insert(billing_events_table,\n                    subscription_id=subscription_id,\n                    event_type=\"upgrade\",\n                    amount_usd=upgrade_charge,\n                    old_tier=sub.tier,\n                    new_tier=new_tier,\n                    created_at=now_ts)\n    \n    logger.info(\"upgrade_subscription: subscription_id=%s, old_tier=%s, new_tier=%s, charge=%.2f\",\n                subscription_id, sub.tier, new_tier, upgrade_charge)\n    \n    result = Subscription(\n        id=subscription_id,\n        tier=new_tier,\n        status=\"active\",\n        upgrade_charge=upgrade_charge\n    )\n    assert result.status == \"active\"\n    assert result.tier == new_tier\n    return result\n# ═══ INTL:END   [id=f0853] upgrade_subscription ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "PATCH", "idx": 853}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION archive_audit_logs [id=f0164]\n  INTENT       \"archive completed audit logs to cold storage and record archival event\"\n  PRECONDITION days_old >= 30\n  PRECONDITION target_table.length > 0\n  POSTCONDITION result.archived_count >= 0\n  READS        audit_logs\n  MUTATES      audit_logs_archive, archival_events\n  OBSERVABLE\n\n  archived_rows = QUERY audit_logs WHERE created_at < now()-days_old DAYS\n  IF archived_rows.count == 0 THEN RETURN ArchiveResult(archived_count: 0)\n  PERSIST audit_logs_archive COPY FROM audit_logs WHERE created_at < now()-days_old DAYS\n  DELETE audit_logs WHERE created_at < now()-days_old DAYS\n  event_id = generate_id()\n  PERSIST archival_events (event_id: event_id, archived_count: archived_rows.count, \n                           archived_at: now(), status: \"completed\")\n  RETURN ArchiveResult(archived_count: archived_rows.count)\nEND FUNCTION archive_audit_logs [id=f0164]", "completion": "# ═══ INTL:BEGIN [id=f0164] archive_audit_logs ═══\nimport logging\nfrom datetime import datetime, timedelta\nimport uuid\n\nlogger = logging.getLogger(__name__)\n\nasync def archive_audit_logs(days_old: int, target_table: str) -> ArchiveResult:\n    assert days_old >= 30, \"days_old must be at least 30\"\n    assert len(target_table) > 0, \"target_table must not be empty\"\n    \n    cutoff_date = datetime.utcnow() - timedelta(days=days_old)\n    \n    archived_rows = await db.query(audit_logs).filter(\n        created_at__lt=cutoff_date\n    ).count()\n    \n    if archived_rows == 0:\n        logger.info(\"archive_audit_logs: no rows to archive, days_old=%d\", days_old)\n        return ArchiveResult(archived_count=0)\n    \n    await db.execute(\n        f\"\"\"INSERT INTO audit_logs_archive \n           SELECT * FROM audit_logs WHERE created_at < %s\"\"\",\n        (cutoff_date,)\n    )\n    \n    await db.execute(\n        \"DELETE FROM audit_logs WHERE created_at < %s\",\n        (cutoff_date,)\n    )\n    \n    event_id = str(uuid.uuid4())\n    await db.insert(\n        archival_events,\n        event_id=event_id,\n        archived_count=archived_rows,\n        archived_at=datetime.utcnow(),\n        status=\"completed\"\n    )\n    \n    logger.info(\n        \"archive_audit_logs: archived_count=%d, event_id=%s, cutoff_date=%s\",\n        archived_rows, event_id, cutoff_date.isoformat()\n    )\n    \n    result = ArchiveResult(archived_count=archived_rows)\n    assert result.archived_count >= 0\n    return result\n# ═══ INTL:END   [id=f0164] archive_audit_logs ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "OBSERVABLE", "idx": 164}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION deliver_webhook [id=f1543]\n  INTENT       \"deliver webhook payload to registered endpoint with retry logic\"\n  PRECONDITION webhook_id.length > 0\n  PRECONDITION payload IS NOT NULL\n  PRECONDITION max_retries >= 1\n  PRECONDITION max_retries <= 10\n  POSTCONDITION result.delivery_id IS NOT NULL\n  POSTCONDITION result.status IN (\"success\", \"failed\", \"queued\")\n  READS        webhooks_table\n  READS        webhook_endpoints_table\n  MUTATES      webhook_deliveries_table\n  OBSERVABLE\n\n  webhook = QUERY webhooks_table WHERE id == webhook_id LIMIT 1\n  IF webhook IS NULL THEN FAIL WebhookError(\"webhook_not_found\")\n  \n  endpoint = QUERY webhook_endpoints_table WHERE webhook_id == webhook_id AND active == true LIMIT 1\n  IF endpoint IS NULL THEN FAIL WebhookError(\"no_active_endpoint\")\n  \n  IF endpoint.url IS NULL OR endpoint.url.length == 0 THEN FAIL WebhookError(\"invalid_endpoint_url\")\n  \n  delivery_id = generate_uuid()\n  attempt_count = 0\n  status = \"queued\"\n  last_error = NULL\n  \n  PERSIST webhook_deliveries_table (\n    id: delivery_id,\n    webhook_id: webhook_id,\n    endpoint_id: endpoint.id,\n    payload: payload,\n    attempt_count: attempt_count,\n    status: status,\n    last_error: last_error,\n    created_at: now(),\n    next_retry_at: now()+5m\n  )\n  \n  RETURN WebhookDelivery(delivery_id: delivery_id, status: status)\nEND FUNCTION deliver_webhook [id=f1543]", "completion": "# ═══ INTL:BEGIN [id=f1543] deliver_webhook ═══\nimport logging\nimport uuid\nfrom datetime import datetime, timedelta\nfrom typing import Optional\n\nlogger = logging.getLogger(__name__)\n\nclass WebhookError(Exception):\n    pass\n\nclass WebhookDelivery:\n    def __init__(self, delivery_id: str, status: str):\n        self.delivery_id = delivery_id\n        self.status = status\n\nasync def deliver_webhook(webhook_id: str, payload: dict, max_retries: int) -> WebhookDelivery:\n    assert len(webhook_id) > 0, \"webhook_id must not be empty\"\n    assert payload is not None, \"payload must not be null\"\n    assert max_retries >= 1, \"max_retries must be at least 1\"\n    assert max_retries <= 10, \"max_retries must not exceed 10\"\n    \n    webhook = await db.query(webhooks_table).filter(id=webhook_id).first()\n    if webhook is None:\n        raise WebhookError(\"webhook_not_found\")\n    \n    endpoint = await db.query(webhook_endpoints_table).filter(\n        webhook_id=webhook_id, active=True\n    ).first()\n    if endpoint is None:\n        raise WebhookError(\"no_active_endpoint\")\n    \n    if endpoint.url is None or len(endpoint.url) == 0:\n        raise WebhookError(\"invalid_endpoint_url\")\n    \n    delivery_id = str(uuid.uuid4())\n    attempt_count = 0\n    status = \"queued\"\n    last_error = None\n    \n    await db.insert(webhook_deliveries_table,\n                    id=delivery_id,\n                    webhook_id=webhook_id,\n                    endpoint_id=endpoint.id,\n                    payload=payload,\n                    attempt_count=attempt_count,\n                    status=status,\n                    last_error=last_error,\n                    created_at=datetime.utcnow(),\n                    next_retry_at=datetime.utcnow() + timedelta(minutes=5))\n    \n    logger.info(\"deliver_webhook: delivery_id=%s webhook_id=%s endpoint_id=%s status=%s\",\n                delivery_id, webhook_id, endpoint.id, status)\n    \n    result = WebhookDelivery(delivery_id=delivery_id, status=status)\n    assert result.delivery_id is not None\n    assert result.status in (\"success\", \"failed\", \"queued\")\n    return result\n# ═══ INTL:END   [id=f1543] deliver_webhook ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "FAIL", "idx": 1543}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION approve_review [id=f0532]\n  INTENT       \"approve a user review after confidence-based quality check\"\n  PRECONDITION review_id > 0\n  PRECONDITION approval_reason.length > 0\n  POSTCONDITION result.approved IS TRUE\n  READS        reviews_table\n  READS        review_quality_scores\n  MUTATES      reviews_table\n  MUTATES      review_audit_log\n  OBSERVABLE\n\n  review = QUERY reviews_table WHERE id == review_id LIMIT 1\n  IF review IS NULL THEN FAIL NotFoundError(\"review_not_found\")\n  IF review.status != \"pending\" THEN FAIL StateError(\"review_already_processed\")\n  \n  quality_score = QUERY review_quality_scores WHERE review_id == review_id LIMIT 1\n  CONFIDENCE quality_check = quality_score.score >= 0.75\n  IF NOT quality_check THEN FAIL ValidationError(\"quality_threshold_not_met\")\n  \n  PERSIST reviews_table (id: review_id, status: \"approved\", approved_at: now(), approved_by: admin_id)\n  PERSIST review_audit_log (review_id: review_id, action: \"approved\", reason: approval_reason, confidence: quality_check.score, timestamp: now())\n  \n  RETURN ReviewApproval(approved: TRUE, review_id: review_id, confidence_score: quality_check.score)\nEND FUNCTION approve_review [id=f0532]", "completion": "# ═══ INTL:BEGIN [id=f0532] approve_review ═══\nimport logging\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nasync def approve_review(review_id: int, admin_id: int, approval_reason: str) -> ReviewApproval:\n    assert review_id > 0, \"review_id must be greater than 0\"\n    assert len(approval_reason) > 0, \"approval_reason must not be empty\"\n    \n    review = await db.query(reviews_table).filter(id=review_id).first()\n    if review is None:\n        raise NotFoundError(\"review_not_found\")\n    if review.status != \"pending\":\n        raise StateError(\"review_already_processed\")\n    \n    quality_score = await db.query(review_quality_scores).filter(review_id=review_id).first()\n    quality_check_score = quality_score.score if quality_score else 0.0\n    \n    if quality_check_score < 0.75:\n        raise ValidationError(\"quality_threshold_not_met\")\n    \n    await db.update(reviews_table, \n                    where={\"id\": review_id},\n                    values={\"status\": \"approved\", \"approved_at\": datetime.utcnow(), \"approved_by\": admin_id})\n    \n    await db.insert(review_audit_log,\n                    review_id=review_id,\n                    action=\"approved\",\n                    reason=approval_reason,\n                    confidence=quality_check_score,\n                    timestamp=datetime.utcnow())\n    \n    logger.info(\"approve_review: review_id=%s admin_id=%s confidence=%.2f\", review_id, admin_id, quality_check_score)\n    \n    result = ReviewApproval(approved=True, review_id=review_id, confidence_score=quality_check_score)\n    assert result.approved is True\n    return result\n# ═══ INTL:END   [id=f0532] approve_review ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "CONFIDENCE", "idx": 532}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION get_product_inventory [id=f0854]\n  INTENT       \"retrieve current product inventory with caching for performance\"\n  PRECONDITION product_id > 0\n  PRECONDITION cache_ttl_seconds > 0\n  POSTCONDITION result.stock_level >= 0\n  READS        products_table\n  READS        inventory_table\n  MUTATES      inventory_cache\n  OBSERVABLE\n\n  cached = CACHE GET inventory_cache WHERE product_id == product_id TTL cache_ttl_seconds\n  IF cached IS NOT NULL THEN RETURN cached\n  \n  product = QUERY products_table WHERE id == product_id LIMIT 1\n  IF product IS NULL THEN FAIL NotFoundError(\"product_not_found\")\n  \n  inventory = QUERY inventory_table WHERE product_id == product_id LIMIT 1\n  IF inventory IS NULL THEN FAIL NotFoundError(\"inventory_not_found\")\n  \n  stock_level = inventory.quantity_on_hand\n  result = ProductInventory(product_id: product_id, stock_level: stock_level, last_updated: now())\n  PERSIST inventory_cache (product_id: product_id, data: result, expires_at: now()+cache_ttl_seconds)\n  RETURN result\nEND FUNCTION get_product_inventory [id=f0854]", "completion": "# ═══ INTL:BEGIN [id=f0854] get_product_inventory ═══\nimport logging\nfrom datetime import datetime, timedelta\nimport json\n\nlogger = logging.getLogger(__name__)\n\nasync def get_product_inventory(product_id: int, cache_ttl_seconds: int) -> ProductInventory:\n    assert product_id > 0, \"product_id must be greater than 0\"\n    assert cache_ttl_seconds > 0, \"cache_ttl_seconds must be greater than 0\"\n    \n    cache_key = f\"inventory:{product_id}\"\n    cached = await cache.get(cache_key)\n    if cached is not None:\n        logger.debug(\"get_product_inventory: cache hit for product_id=%d\", product_id)\n        return ProductInventory(**json.loads(cached))\n    \n    product = await db.query(products_table).filter(id=product_id).first()\n    if product is None:\n        raise NotFoundError(\"product_not_found\")\n    \n    inventory = await db.query(inventory_table).filter(product_id=product_id).first()\n    if inventory is None:\n        raise NotFoundError(\"inventory_not_found\")\n    \n    stock_level = inventory.quantity_on_hand\n    result = ProductInventory(\n        product_id=product_id,\n        stock_level=stock_level,\n        last_updated=datetime.utcnow()\n    )\n    \n    await cache.set(\n        cache_key,\n        json.dumps({\n            \"product_id\": result.product_id,\n            \"stock_level\": result.stock_level,\n            \"last_updated\": result.last_updated.isoformat()\n        }),\n        ex=cache_ttl_seconds\n    )\n    \n    await db.insert(inventory_cache, product_id=product_id, data=json.dumps({\n        \"product_id\": result.product_id,\n        \"stock_level\": result.stock_level,\n        \"last_updated\": result.last_updated.isoformat()\n    }), expires_at=datetime.utcnow() + timedelta(seconds=cache_ttl_seconds))\n    \n    logger.info(\"get_product_inventory: product_id=%d, stock_level=%d\", product_id, stock_level)\n    assert result.stock_level >= 0, \"stock_level must not be negative\"\n    return result\n# ═══ INTL:END   [id=f0854] get_product_inventory ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "CACHE GET", "idx": 854}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION record_event_batch [id=f1544]\n  INTENT       \"atomically record analytics events and update aggregated metrics\"\n  PRECONDITION events.length > 0\n  PRECONDITION events.length <= 1000\n  PRECONDITION dataset_id.length > 0\n  POSTCONDITION result.recorded_count == events.length\n  POSTCONDITION result.metrics_updated IS TRUE\n  READS        datasets_table, event_definitions_table\n  MUTATES      events_table, event_metrics_table\n  OBSERVABLE\n\n  TRANSACTION\n    dataset = QUERY datasets_table WHERE id == dataset_id LIMIT 1\n    IF dataset IS NULL THEN FAIL NotFoundError(\"dataset_not_found\")\n    \n    FOR EACH event IN events DO\n      event_def = QUERY event_definitions_table WHERE event_name == event.name AND dataset_id == dataset_id LIMIT 1\n      IF event_def IS NULL THEN FAIL ValidationError(\"event_definition_not_found\")\n      PERSIST events_table (dataset_id: dataset_id, event_name: event.name, properties: event.properties, timestamp: now())\n    END FOR\n    \n    total_count = COUNT events_table WHERE dataset_id == dataset_id AND timestamp >= now()-1h\n    PERSIST event_metrics_table (dataset_id: dataset_id, metric_name: \"hourly_event_count\", metric_value: total_count, updated_at: now())\n  END TRANSACTION\n\n  RETURN EventBatchResult(recorded_count: events.length, metrics_updated: TRUE)\nEND FUNCTION record_event_batch [id=f1544]", "completion": "# ═══ INTL:BEGIN [id=f1544] record_event_batch ═══\nimport logging\nfrom datetime import datetime, timedelta\nfrom typing import List, Dict, Any\n\nlogger = logging.getLogger(__name__)\n\nclass EventBatchResult:\n    def __init__(self, recorded_count: int, metrics_updated: bool):\n        self.recorded_count = recorded_count\n        self.metrics_updated = metrics_updated\n\nasync def record_event_batch(dataset_id: str, events: List[Dict[str, Any]]) -> EventBatchResult:\n    assert len(events) > 0, \"events list must not be empty\"\n    assert len(events) <= 1000, \"events list must not exceed 1000 items\"\n    assert len(dataset_id) > 0, \"dataset_id must not be empty\"\n    \n    async with db.transaction():\n        dataset = await db.query(datasets_table).filter(id=dataset_id).first()\n        if dataset is None:\n            raise NotFoundError(\"dataset_not_found\")\n        \n        for event in events:\n            event_def = await db.query(event_definitions_table).filter(\n                event_name=event[\"name\"],\n                dataset_id=dataset_id\n            ).first()\n            if event_def is None:\n                raise ValidationError(\"event_definition_not_found\")\n            \n            await db.insert(\n                events_table,\n                dataset_id=dataset_id,\n                event_name=event[\"name\"],\n                properties=event.get(\"properties\", {}),\n                timestamp=datetime.utcnow()\n            )\n        \n        total_count = await db.query(events_table).filter(\n            dataset_id=dataset_id,\n            timestamp__gte=datetime.utcnow() - timedelta(hours=1)\n        ).count()\n        \n        await db.insert(\n            event_metrics_table,\n            dataset_id=dataset_id,\n            metric_name=\"hourly_event_count\",\n            metric_value=total_count,\n            updated_at=datetime.utcnow()\n        )\n    \n    logger.info(\"record_event_batch: dataset_id=%s, recorded_count=%d\", dataset_id, len(events))\n    result = EventBatchResult(recorded_count=len(events), metrics_updated=True)\n    assert result.recorded_count == len(events)\n    assert result.metrics_updated is True\n    return result\n# ═══ INTL:END   [id=f1544] record_event_batch ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "TRANSACTION", "idx": 1544}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION grant_role_to_user [id=f0165]\n  INTENT       \"atomically assign a role to a user with audit logging\"\n  PRECONDITION user_id > 0\n  PRECONDITION role_id > 0\n  PRECONDITION requester_id > 0\n  PRECONDITION requester_has_admin_privilege(requester_id)\n  POSTCONDITION result.success IS TRUE\n  READS        users_table, roles_table, admin_privileges_table\n  MUTATES      user_roles_table, audit_log_table\n  OBSERVABLE\n\n  TRANSACTION\n    requester = QUERY admin_privileges_table WHERE user_id == requester_id LIMIT 1\n    IF requester IS NULL THEN FAIL PermissionError(\"insufficient_privileges\")\n    \n    user = QUERY users_table WHERE id == user_id LIMIT 1\n    IF user IS NULL THEN FAIL NotFoundError(\"user_not_found\")\n    \n    role = QUERY roles_table WHERE id == role_id LIMIT 1\n    IF role IS NULL THEN FAIL NotFoundError(\"role_not_found\")\n    \n    existing = QUERY user_roles_table WHERE user_id == user_id AND role_id == role_id LIMIT 1\n    IF existing IS NOT NULL THEN FAIL ConflictError(\"role_already_assigned\")\n    \n    PERSIST user_roles_table (user_id: user_id, role_id: role_id, granted_at: now())\n    PERSIST audit_log_table (action: \"grant_role\", requester_id: requester_id, target_user_id: user_id, role_id: role_id, timestamp: now())\n    \n    RETURN GrantResult(success: TRUE, message: \"role_granted\")\n  END TRANSACTION\nEND FUNCTION grant_role_to_user [id=f0165]", "completion": "# ═══ INTL:BEGIN [id=f0165] grant_role_to_user ═══\nimport logging\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nasync def grant_role_to_user(user_id: int, role_id: int, requester_id: int) -> GrantResult:\n    assert user_id > 0, \"user_id must be positive\"\n    assert role_id > 0, \"role_id must be positive\"\n    assert requester_id > 0, \"requester_id must be positive\"\n    assert await requester_has_admin_privilege(requester_id), \"requester lacks admin privilege\"\n    \n    async with db.transaction():\n        requester = await db.query(admin_privileges_table).filter(user_id=requester_id).first()\n        if requester is None:\n            raise PermissionError(\"insufficient_privileges\")\n        \n        user = await db.query(users_table).filter(id=user_id).first()\n        if user is None:\n            raise NotFoundError(\"user_not_found\")\n        \n        role = await db.query(roles_table).filter(id=role_id).first()\n        if role is None:\n            raise NotFoundError(\"role_not_found\")\n        \n        existing = await db.query(user_roles_table).filter(\n            user_id=user_id, role_id=role_id\n        ).first()\n        if existing is not None:\n            raise ConflictError(\"role_already_assigned\")\n        \n        await db.insert(\n            user_roles_table,\n            user_id=user_id,\n            role_id=role_id,\n            granted_at=datetime.utcnow()\n        )\n        \n        await db.insert(\n            audit_log_table,\n            action=\"grant_role\",\n            requester_id=requester_id,\n            target_user_id=user_id,\n            role_id=role_id,\n            timestamp=datetime.utcnow()\n        )\n        \n        logger.info(\"grant_role_to_user: requester=%s granted role=%s to user=%s\", \n                    requester_id, role_id, user_id)\n        \n        result = GrantResult(success=True, message=\"role_granted\")\n        assert result.success is True\n        return result\n# ═══ INTL:END   [id=f0165] grant_role_to_user ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "TRANSACTION", "idx": 165}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION apply_bulk_coupon_validations [id=f0533]\n  INTENT       \"validate multiple coupons in parallel and mark invalid ones\"\n  PRECONDITION coupon_ids.length > 0\n  PRECONDITION coupon_ids.length <= 1000\n  POSTCONDITION result.validated_count >= 0\n  READS        coupons_table\n  READS        coupon_usage_table\n  MUTATES      coupons_table\n  OBSERVABLE\n\n  results = []\n  PARALLEL FOR coupon_id IN coupon_ids\n    coupon = QUERY coupons_table WHERE id == coupon_id LIMIT 1\n    IF coupon IS NULL THEN CONTINUE\n    IF coupon.expiry_date < now() THEN\n      PERSIST coupons_table (id: coupon_id, status: \"expired\")\n      APPEND results (coupon_id: coupon_id, status: \"expired\")\n    ELSE\n      usage_count = QUERY coupon_usage_table WHERE coupon_id == coupon_id COUNT\n      IF usage_count >= coupon.max_uses THEN\n        PERSIST coupons_table (id: coupon_id, status: \"exhausted\")\n        APPEND results (coupon_id: coupon_id, status: \"exhausted\")\n      ELSE\n        APPEND results (coupon_id: coupon_id, status: \"valid\")\n      END IF\n    END IF\n  END PARALLEL FOR\n\n  RETURN ValidationResult(validated_count: results.length, details: results)\nEND FUNCTION apply_bulk_coupon_validations [id=f0533]", "completion": "# ═══ INTL:BEGIN [id=f0533] apply_bulk_coupon_validations ═══\nimport logging\nimport asyncio\nfrom datetime import datetime\nfrom typing import List\n\nlogger = logging.getLogger(__name__)\n\nclass ValidationResult:\n    def __init__(self, validated_count: int, details: List[dict]):\n        self.validated_count = validated_count\n        self.details = details\n\nasync def apply_bulk_coupon_validations(coupon_ids: List[str]) -> ValidationResult:\n    assert len(coupon_ids) > 0, \"coupon_ids must not be empty\"\n    assert len(coupon_ids) <= 1000, \"coupon_ids must not exceed 1000 items\"\n    \n    results = []\n    \n    async def validate_single_coupon(coupon_id: str) -> None:\n        try:\n            coupon = await db.query(coupons_table).filter(id=coupon_id).first()\n            if coupon is None:\n                return\n            \n            if coupon.expiry_date < datetime.utcnow():\n                await db.update(coupons_table).filter(id=coupon_id).set(status=\"expired\")\n                results.append({\"coupon_id\": coupon_id, \"status\": \"expired\"})\n            else:\n                usage_count = await db.query(coupon_usage_table).filter(\n                    coupon_id=coupon_id\n                ).count()\n                \n                if usage_count >= coupon.max_uses:\n                    await db.update(coupons_table).filter(id=coupon_id).set(status=\"exhausted\")\n                    results.append({\"coupon_id\": coupon_id, \"status\": \"exhausted\"})\n                else:\n                    results.append({\"coupon_id\": coupon_id, \"status\": \"valid\"})\n        except Exception as e:\n            logger.error(\"validation_error: coupon_id=%s error=%s\", coupon_id, str(e))\n            raise\n    \n    await asyncio.gather(*[validate_single_coupon(cid) for cid in coupon_ids])\n    \n    logger.info(\"apply_bulk_coupon_validations: validated_count=%d\", len(results))\n    result = ValidationResult(validated_count=len(results), details=results)\n    assert result.validated_count >= 0, \"validated_count must be non-negative\"\n    return result\n# ═══ INTL:END   [id=f0533] apply_bulk_coupon_validations ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "PARALLEL", "idx": 533}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION summarize_orders_by_customer [id=f0855]\n  INTENT       \"aggregate order metrics grouped by customer and return summary statistics\"\n  PRECONDITION start_date IS NOT NULL\n  PRECONDITION end_date IS NOT NULL\n  PRECONDITION start_date < end_date\n  PRECONDITION min_order_count >= 1\n  POSTCONDITION result.summaries.length >= 0\n  READS        orders_table\n  READS        customers_table\n  MUTATES      order_summaries_cache\n  OBSERVABLE\n\n  summaries = AGGREGATE orders_table\n    WHERE created_at >= start_date AND created_at <= end_date\n    GROUP BY customer_id\n    HAVING COUNT(*) >= min_order_count\n    SELECT customer_id, COUNT(*) AS total_orders, SUM(amount) AS total_revenue, AVG(amount) AS avg_order_value, MAX(created_at) AS last_order_date\n\n  customer_details = QUERY customers_table WHERE id IN summaries.customer_id\n  \n  results = []\n  FOR summary IN summaries DO\n    customer = customer_details WHERE id == summary.customer_id\n    result_item = OrderSummary(\n      customer_id: summary.customer_id,\n      customer_name: customer.name,\n      total_orders: summary.total_orders,\n      total_revenue: summary.total_revenue,\n      avg_order_value: summary.avg_order_value,\n      last_order_date: summary.last_order_date\n    )\n    results.APPEND result_item\n  END FOR\n\n  PERSIST order_summaries_cache (cached_at: now(), summaries: results)\n  RETURN OrderSummaryResult(summaries: results, count: results.length)\nEND FUNCTION summarize_orders_by_customer [id=f0855]", "completion": "# ═══ INTL:BEGIN [id=f0855] summarize_orders_by_customer ═══\nimport logging\nfrom datetime import datetime\nfrom typing import List\n\nlogger = logging.getLogger(__name__)\n\nclass OrderSummary:\n    def __init__(self, customer_id: int, customer_name: str, total_orders: int,\n                 total_revenue: float, avg_order_value: float, last_order_date: datetime):\n        self.customer_id = customer_id\n        self.customer_name = customer_name\n        self.total_orders = total_orders\n        self.total_revenue = total_revenue\n        self.avg_order_value = avg_order_value\n        self.last_order_date = last_order_date\n\nclass OrderSummaryResult:\n    def __init__(self, summaries: List[OrderSummary], count: int):\n        self.summaries = summaries\n        self.count = count\n\nasync def summarize_orders_by_customer(start_date: datetime, end_date: datetime, \n                                       min_order_count: int) -> OrderSummaryResult:\n    assert start_date is not None, \"start_date must not be null\"\n    assert end_date is not None, \"end_date must not be null\"\n    assert start_date < end_date, \"start_date must be before end_date\"\n    assert min_order_count >= 1, \"min_order_count must be at least 1\"\n    \n    summaries_raw = await db.query(\"\"\"\n        SELECT \n            customer_id,\n            COUNT(*) AS total_orders,\n            SUM(amount) AS total_revenue,\n            AVG(amount) AS avg_order_value,\n            MAX(created_at) AS last_order_date\n        FROM orders_table\n        WHERE created_at >= %s AND created_at <= %s\n        GROUP BY customer_id\n        HAVING COUNT(*) >= %s\n    \"\"\", (start_date, end_date, min_order_count))\n    \n    customer_ids = [s['customer_id'] for s in summaries_raw]\n    \n    if not customer_ids:\n        await db.insert(order_summaries_cache, cached_at=datetime.utcnow(), summaries='[]')\n        logger.info(\"summarize_orders_by_customer: no summaries generated\")\n        return OrderSummaryResult(summaries=[], count=0)\n    \n    customer_details = await db.query(\"\"\"\n        SELECT id, name FROM customers_table WHERE id = ANY(%s)\n    \"\"\", (customer_ids,))\n    \n    customer_map = {c['id']: c['name'] for c in customer_details}\n    \n    results = []\n    for summary in summaries_raw:\n        result_item = OrderSummary(\n            customer_id=summary['customer_id'],\n            customer_name=customer_map.get(summary['customer_id'], 'Unknown'),\n            total_orders=summary['total_orders'],\n            total_revenue=float(summary['total_revenue'] or 0),\n            avg_order_value=float(summary['avg_order_value'] or 0),\n            last_order_date=summary['last_order_date']\n        )\n        results.append(result_item)\n    \n    import json\n    summaries_json = json.dumps([{\n        'customer_id': r.customer_id,\n        'customer_name': r.customer_name,\n        'total_orders': r.total_orders,\n        'total_revenue': r.total_revenue,\n        'avg_order_value': r.avg_order_value,\n        'last_order_date': r.last_order_date.isoformat() if r.last_order_date else None\n    } for r in results])\n    \n    await db.insert(order_summaries_cache, cached_at=datetime.utcnow(), summaries=summaries_json)\n    \n    logger.info(\"summarize_orders_by_customer: generated %d summaries for period %s to %s\", \n                len(results), start_date, end_date)\n    \n    assert len(results) >= 0, \"results must be non-negative\"\n    return OrderSummaryResult(summaries=results, count=len(results))\n# ═══ INTL:END   [id=f0855] summarize_orders_by_customer ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "AGGREGATE", "idx": 855}}
{"system": "You are the INTL compiler for Sql Postgres in PATCH mode. Apply the PATCH block to the existing code with minimal diff. Wrap output in INTL:BEGIN / INTL:END sentinels.", "prompt": "INTL:BEGIN\nSELECT u.user_id, u.username, s.search_query, s.created_at\nFROM users u\nJOIN searches s ON u.user_id = s.user_id\nWHERE s.created_at > NOW() - INTERVAL '7 days'\nORDER BY s.created_at DESC;\nINTL:END\n\nPATCH f0166 [target=f0166]\n  INTENT \"add validation check to fail gracefully if user_id is null\"\n  ADD BEFORE: WHERE\n    AND u.user_id IS NOT NULL\nEND PATCH f0166", "completion": "INTL:BEGIN\nSELECT u.user_id, u.username, s.search_query, s.created_at\nFROM users u\nJOIN searches s ON u.user_id = s.user_id\nWHERE u.user_id IS NOT NULL\n  AND s.created_at > NOW() - INTERVAL '7 days'\nORDER BY s.created_at DESC;\nINTL:END", "metadata": {"category": "B", "adapter": "sql_postgres", "construct": "FAIL", "idx": 166}}
{"system": "You are the INTL compiler for Sql Postgres in PATCH mode. Apply the PATCH block to the existing code with minimal diff. Wrap output in INTL:BEGIN / INTL:END sentinels.", "prompt": "INTL:BEGIN\nSELECT id, name, email FROM users WHERE id = $1;\nINTL:END\n\nPATCH f1545 [target=f1545]\n  INTENT \"add cache retrieval before database query\"\n  ADD BEFORE: GET cache_key FROM search_cache\nEND PATCH f1545", "completion": "INTL:BEGIN\nGET cache_key FROM search_cache;\nSELECT id, name, email FROM users WHERE id = $1;\nINTL:END", "metadata": {"category": "B", "adapter": "sql_postgres", "construct": "CACHE GET", "idx": 1545}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION grant_permission [id=f0534]\n  INTENT       \"grant or revoke user permission based on role and resource\"\n  PRECONDITION user_id.length > 0\n  PRECONDITION resource_id.length > 0\n  PRECONDITION action IN [\"read\", \"write\", \"delete\", \"admin\"]\n  POSTCONDITION result.success IS TRUE\n  READS        users_table, roles_table, permissions_table\n  MUTATES      permissions_table\n  OBSERVABLE\n\n  user = QUERY users_table WHERE id == user_id LIMIT 1\n  IF user IS NULL THEN FAIL PermissionError(\"user_not_found\")\n  \n  role = QUERY roles_table WHERE user_id == user_id LIMIT 1\n  IF role IS NULL THEN FAIL PermissionError(\"user_has_no_role\")\n  \n  IF role.level < 3 THEN\n    FAIL PermissionError(\"insufficient_role_level\")\n  ELSE\n    existing = QUERY permissions_table WHERE user_id == user_id AND resource_id == resource_id LIMIT 1\n    IF existing IS NOT NULL THEN\n      UPDATE permissions_table SET action = action, updated_at = now() WHERE id == existing.id\n    ELSE\n      PERSIST permissions_table (user_id: user_id, resource_id: resource_id, action: action, created_at: now())\n    END IF\n  END IF\n  \n  RETURN PermissionResult(success: TRUE, message: \"permission_granted\")\nEND FUNCTION grant_permission [id=f0534]", "completion": "# ═══ INTL:BEGIN [id=f0534] grant_permission ═══\nimport logging\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nasync def grant_permission(user_id: str, resource_id: str, action: str) -> PermissionResult:\n    assert len(user_id) > 0, \"user_id must not be empty\"\n    assert len(resource_id) > 0, \"resource_id must not be empty\"\n    assert action in [\"read\", \"write\", \"delete\", \"admin\"], \"action must be one of: read, write, delete, admin\"\n    \n    user = await db.query(users_table).filter(id=user_id).first()\n    if user is None:\n        raise PermissionError(\"user_not_found\")\n    \n    role = await db.query(roles_table).filter(user_id=user_id).first()\n    if role is None:\n        raise PermissionError(\"user_has_no_role\")\n    \n    if role.level < 3:\n        raise PermissionError(\"insufficient_role_level\")\n    else:\n        existing = await db.query(permissions_table).filter(\n            user_id=user_id, \n            resource_id=resource_id\n        ).first()\n        \n        if existing is not None:\n            await db.update(permissions_table).filter(\n                id=existing.id\n            ).set(action=action, updated_at=datetime.utcnow()).execute()\n        else:\n            await db.insert(permissions_table, \n                           user_id=user_id, \n                           resource_id=resource_id, \n                           action=action, \n                           created_at=datetime.utcnow())\n    \n    logger.info(\"grant_permission: user_id=%s resource_id=%s action=%s\", user_id, resource_id, action)\n    result = PermissionResult(success=True, message=\"permission_granted\")\n    assert result.success is True\n    return result\n# ═══ INTL:END   [id=f0534] grant_permission ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "IF/THEN/ELSE", "idx": 534}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION apply_coupon_discount [id=f0856]\n  INTENT       \"apply a discount coupon to an order and update coupon usage\"\n  PRECONDITION coupon_code.length > 0\n  PRECONDITION order_id > 0\n  PRECONDITION discount_amount >= 0\n  POSTCONDITION result.applied IS TRUE\n  READS        coupons_table, orders_table\n  MUTATES      orders_table, coupon_usage_table\n  OBSERVABLE\n\n  coupon = QUERY coupons_table WHERE code == coupon_code LIMIT 1\n  IF coupon IS NULL THEN FAIL CouponError(\"coupon_not_found\")\n  IF coupon.is_expired THEN FAIL CouponError(\"coupon_expired\")\n  IF coupon.remaining_uses <= 0 THEN FAIL CouponError(\"coupon_exhausted\")\n  \n  order = QUERY orders_table WHERE id == order_id LIMIT 1\n  IF order IS NULL THEN FAIL OrderError(\"order_not_found\")\n  IF order.status != \"pending\" THEN FAIL OrderError(\"order_not_pending\")\n  \n  PATCH orders_table SET total_price = order.total_price - discount_amount WHERE id == order_id\n  PATCH coupons_table SET remaining_uses = coupon.remaining_uses - 1 WHERE id == coupon.id\n  PERSIST coupon_usage_table (coupon_id: coupon.id, order_id: order_id, discount_applied: discount_amount, applied_at: now())\n  \n  RETURN CouponResult(applied: TRUE, new_total: order.total_price - discount_amount)\nEND FUNCTION apply_coupon_discount [id=f0856]", "completion": "# ═══ INTL:BEGIN [id=f0856] apply_coupon_discount ═══\nimport logging\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nasync def apply_coupon_discount(coupon_code: str, order_id: int, discount_amount: float) -> CouponResult:\n    assert len(coupon_code) > 0, \"coupon_code must not be empty\"\n    assert order_id > 0, \"order_id must be positive\"\n    assert discount_amount >= 0, \"discount_amount must be non-negative\"\n    \n    coupon = await db.query(coupons_table).filter(code=coupon_code).first()\n    if coupon is None:\n        raise CouponError(\"coupon_not_found\")\n    if coupon.is_expired:\n        raise CouponError(\"coupon_expired\")\n    if coupon.remaining_uses <= 0:\n        raise CouponError(\"coupon_exhausted\")\n    \n    order = await db.query(orders_table).filter(id=order_id).first()\n    if order is None:\n        raise OrderError(\"order_not_found\")\n    if order.status != \"pending\":\n        raise OrderError(\"order_not_pending\")\n    \n    new_total = order.total_price - discount_amount\n    await db.update(orders_table).set(total_price=new_total).where(id=order_id)\n    await db.update(coupons_table).set(remaining_uses=coupon.remaining_uses - 1).where(id=coupon.id)\n    await db.insert(coupon_usage_table, coupon_id=coupon.id, order_id=order_id,\n                    discount_applied=discount_amount, applied_at=datetime.utcnow())\n    \n    logger.info(\"apply_coupon_discount: coupon=%s order=%d discount=%.2f\", coupon_code, order_id, discount_amount)\n    result = CouponResult(applied=True, new_total=new_total)\n    assert result.applied is True\n    return result\n# ═══ INTL:END   [id=f0856] apply_coupon_discount ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "PATCH", "idx": 856}}
{"system": "You are the INTL compiler for Sql Postgres in PATCH mode. Apply the PATCH block to the existing code with minimal diff. Wrap output in INTL:BEGIN / INTL:END sentinels.", "prompt": "INTL:BEGIN\nCREATE FUNCTION validate_api_key(key_input TEXT)\nRETURNS TABLE(valid BOOLEAN, user_id INT) AS $$\nBEGIN\n  SELECT EXISTS(\n    SELECT 1 FROM api_keys \n    WHERE key = key_input AND is_active = true\n  ) AS valid,\n  (SELECT user_id FROM api_keys WHERE key = key_input) AS user_id;\nEND;\n$$ LANGUAGE plpgsql;\nINTL:END\n\nPATCH f1546 [target=f1546]\n  INTENT \"add statement timeout to prevent long-running api key validation queries\"\n  ADD BEFORE: SET statement_timeout TO '5s'\n  ADD AFTER: RESET statement_timeout\nEND PATCH f1546", "completion": "INTL:BEGIN\nCREATE FUNCTION validate_api_key(key_input TEXT)\nRETURNS TABLE(valid BOOLEAN, user_id INT) AS $$\nBEGIN\n  SET statement_timeout TO '5s';\n  SELECT EXISTS(\n    SELECT 1 FROM api_keys \n    WHERE key = key_input AND is_active = true\n  ) AS valid,\n  (SELECT user_id FROM api_keys WHERE key = key_input) AS user_id;\n  RESET statement_timeout;\nEND;\n$$ LANGUAGE plpgsql;\nINTL:END", "metadata": {"category": "B", "adapter": "sql_postgres", "construct": "TIMEOUT", "idx": 1546}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION generate_monthly_report [id=f0167]\n  INTENT       \"generate aggregated monthly sales report with confidence scores\"\n  PRECONDITION year > 0\n  PRECONDITION month >= 1 AND month <= 12\n  PRECONDITION confidence_threshold >= 0.0 AND confidence_threshold <= 1.0\n  POSTCONDITION result.report_id IS NOT NULL\n  POSTCONDITION result.confidence >= confidence_threshold\n  READS        sales_transactions\n  READS        product_catalog\n  MUTATES      reports_table\n  MUTATES      report_audit_log\n  OBSERVABLE\n\n  transactions = QUERY sales_transactions WHERE YEAR(date) == year AND MONTH(date) == month\n  total_revenue = SUM(transactions.amount)\n  transaction_count = COUNT(transactions)\n  CONFIDENCE data_quality = transaction_count / expected_monthly_transactions\n  \n  IF data_quality < confidence_threshold THEN FAIL ConfidenceError(\"insufficient_data_quality\")\n  \n  products = QUERY product_catalog\n  category_breakdown = GROUP transactions BY product_category SUM amount\n  average_order_value = total_revenue / transaction_count\n  \n  CONFIDENCE forecast_reliability = CALCULATE_RELIABILITY(data_quality, category_breakdown)\n  report_record = ReportRecord(\n    year: year,\n    month: month,\n    total_revenue: total_revenue,\n    transaction_count: transaction_count,\n    average_order_value: average_order_value,\n    category_breakdown: category_breakdown,\n    confidence: forecast_reliability,\n    generated_at: now()\n  )\n  \n  PERSIST reports_table (report_record)\n  PERSIST report_audit_log (report_id: report_record.id, action: \"GENERATED\", confidence: forecast_reliability, timestamp: now())\n  RETURN report_record\nEND FUNCTION generate_monthly_report [id=f0167]", "completion": "# ═══ INTL:BEGIN [id=f0167] generate_monthly_report ═══\nimport logging\nfrom datetime import datetime\nfrom decimal import Decimal\n\nlogger = logging.getLogger(__name__)\n\nasync def generate_monthly_report(\n    year: int,\n    month: int,\n    confidence_threshold: float\n) -> ReportRecord:\n    assert year > 0, \"year must be positive\"\n    assert 1 <= month <= 12, \"month must be between 1 and 12\"\n    assert 0.0 <= confidence_threshold <= 1.0, \"confidence_threshold must be between 0.0 and 1.0\"\n    \n    transactions = await db.query(sales_transactions).filter(\n        f\"EXTRACT(YEAR FROM date) = {year}\",\n        f\"EXTRACT(MONTH FROM date) = {month}\"\n    ).all()\n    \n    total_revenue = sum(t.amount for t in transactions) if transactions else Decimal(0)\n    transaction_count = len(transactions)\n    \n    expected_monthly_transactions = 1000\n    data_quality = transaction_count / expected_monthly_transactions if expected_monthly_transactions > 0 else 0.0\n    \n    if data_quality < confidence_threshold:\n        raise ConfidenceError(\"insufficient_data_quality\")\n    \n    products = await db.query(product_catalog).all()\n    \n    category_breakdown = {}\n    for transaction in transactions:\n        category = transaction.product_category\n        if category not in category_breakdown:\n            category_breakdown[category] = Decimal(0)\n        category_breakdown[category] += transaction.amount\n    \n    average_order_value = total_revenue / transaction_count if transaction_count > 0 else Decimal(0)\n    \n    forecast_reliability = data_quality * (1 - (0.1 * len(category_breakdown.keys()) / max(len(products), 1)))\n    forecast_reliability = max(0.0, min(1.0, forecast_reliability))\n    \n    report_record = ReportRecord(\n        year=year,\n        month=month,\n        total_revenue=total_revenue,\n        transaction_count=transaction_count,\n        average_order_value=average_order_value,\n        category_breakdown=category_breakdown,\n        confidence=forecast_reliability,\n        generated_at=datetime.utcnow()\n    )\n    \n    inserted_report = await db.insert(\n        reports_table,\n        year=report_record.year,\n        month=report_record.month,\n        total_revenue=report_record.total_revenue,\n        transaction_count=report_record.transaction_count,\n        average_order_value=report_record.average_order_value,\n        category_breakdown=report_record.category_breakdown,\n        confidence=report_record.confidence,\n        generated_at=report_record.generated_at\n    )\n    \n    report_record.id = inserted_report.id\n    \n    await db.insert(\n        report_audit_log,\n        report_id=report_record.id,\n        action=\"GENERATED\",\n        confidence=forecast_reliability,\n        timestamp=datetime.utcnow()\n    )\n    \n    logger.info(\"generate_monthly_report: report_id=%s year=%d month=%d confidence=%.2f transaction_count=%d\",\n                report_record.id, year, month, forecast_reliability, transaction_count)\n    \n    assert report_record.id is not None\n    assert report_record.confidence >= confidence_threshold\n    return report_record\n# ═══ INTL:END   [id=f0167] generate_monthly_report ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "CONFIDENCE", "idx": 167}}
{"system": "You are the INTL compiler for Sql Postgres in PATCH mode. Apply the PATCH block to the existing code with minimal diff. Wrap output in INTL:BEGIN / INTL:END sentinels.", "prompt": "INTL:BEGIN\nSELECT \n  sku,\n  product_name,\n  quantity_on_hand,\n  reorder_level\nFROM inventory\nWHERE warehouse_id = $1\nORDER BY sku ASC;\nINTL:END\n\nPATCH f0857 [target=f0857]\n  INTENT \"filter query to exclude discontinued items from inventory\"\n  ADD CONDITION: AND status != 'discontinued'\nEND PATCH f0857", "completion": "INTL:BEGIN\nSELECT \n  sku,\n  product_name,\n  quantity_on_hand,\n  reorder_level\nFROM inventory\nWHERE warehouse_id = $1\n  AND status != 'discontinued'\nORDER BY sku ASC;\nINTL:END", "metadata": {"category": "B", "adapter": "sql_postgres", "construct": "QUERY", "idx": 857}}
{"system": "You are the INTL compiler for Sql Postgres in PATCH mode. Apply the PATCH block to the existing code with minimal diff. Wrap output in INTL:BEGIN / INTL:END sentinels.", "prompt": "INTL:BEGIN\nCREATE OR REPLACE FUNCTION process_payment(\n  p_user_id INT,\n  p_amount DECIMAL,\n  p_currency VARCHAR\n) RETURNS TABLE(success BOOLEAN, message VARCHAR) AS $$\nBEGIN\n  INSERT INTO transactions (user_id, amount, currency, status)\n  VALUES (p_user_id, p_amount, p_currency, 'pending');\n  \n  RETURN QUERY SELECT true, 'Payment processed';\nEND;\n$$ LANGUAGE plpgsql;\nINTL:END\n\nPATCH f0536 [target=f0536]\n  INTENT \"add validation failure handling for insufficient balance\"\n  ADD BEFORE: INSERT INTO transactions\n    IF NOT EXISTS (\n      SELECT 1 FROM accounts \n      WHERE user_id = p_user_id AND balance >= p_amount\n    ) THEN\n      RETURN QUERY SELECT false, 'Insufficient balance';\n      RETURN;\n    END IF;\n  MODIFY: status to 'completed'\nEND PATCH f0536", "completion": "INTL:BEGIN\nCREATE OR REPLACE FUNCTION process_payment(\n  p_user_id INT,\n  p_amount DECIMAL,\n  p_currency VARCHAR\n) RETURNS TABLE(success BOOLEAN, message VARCHAR) AS $$\nBEGIN\n  IF NOT EXISTS (\n    SELECT 1 FROM accounts \n    WHERE user_id = p_user_id AND balance >= p_amount\n  ) THEN\n    RETURN QUERY SELECT false, 'Insufficient balance';\n    RETURN;\n  END IF;\n  \n  INSERT INTO transactions (user_id, amount, currency, status)\n  VALUES (p_user_id, p_amount, p_currency, 'completed');\n  \n  RETURN QUERY SELECT true, 'Payment processed';\nEND;\n$$ LANGUAGE plpgsql;\nINTL:END", "metadata": {"category": "B", "adapter": "sql_postgres", "construct": "FAIL", "idx": 536}}
{"system": "You are the INTL compiler for Sql Postgres in PATCH mode. Apply the PATCH block to the existing code with minimal diff. Wrap output in INTL:BEGIN / INTL:END sentinels.", "prompt": "INTL:BEGIN\nCREATE OR REPLACE FUNCTION calculate_shipping_cost(\n    order_id UUID,\n    weight_kg DECIMAL,\n    destination_country VARCHAR\n) RETURNS DECIMAL AS $$\nDECLARE\n    base_rate DECIMAL := 10.00;\n    distance_multiplier DECIMAL;\n    cost DECIMAL;\nBEGIN\n    SELECT multiplier INTO distance_multiplier\n    FROM shipping_zones\n    WHERE country_code = destination_country;\n    \n    cost := weight_kg * base_rate * distance_multiplier;\n    RETURN cost;\nEND;\n$$ LANGUAGE plpgsql;\nINTL:END\n\nPATCH f0168 [target=f0168]\n  INTENT \"transform cost calculation to round to 2 decimal places and apply currency conversion\"\n  TRANSFORM: cost := weight_kg * base_rate * distance_multiplier\n    TO: cost := ROUND((weight_kg * base_rate * distance_multiplier) * get_exchange_rate(destination_country), 2)\nEND PATCH f0168", "completion": "INTL:BEGIN\nCREATE OR REPLACE FUNCTION calculate_shipping_cost(\n    order_id UUID,\n    weight_kg DECIMAL,\n    destination_country VARCHAR\n) RETURNS DECIMAL AS $$\nDECLARE\n    base_rate DECIMAL := 10.00;\n    distance_multiplier DECIMAL;\n    cost DECIMAL;\nBEGIN\n    SELECT multiplier INTO distance_multiplier\n    FROM shipping_zones\n    WHERE country_code = destination_country;\n    \n    cost := ROUND((weight_kg * base_rate * distance_multiplier) * get_exchange_rate(destination_country), 2);\n    RETURN cost;\nEND;\n$$ LANGUAGE plpgsql;\nINTL:END", "metadata": {"category": "B", "adapter": "sql_postgres", "construct": "TRANSFORM", "idx": 168}}
{"system": "You are the INTL compiler for Sql Postgres in PATCH mode. Apply the PATCH block to the existing code with minimal diff. Wrap output in INTL:BEGIN / INTL:END sentinels.", "prompt": "INTL:BEGIN\nBEGIN TRANSACTION;\n\nUPDATE orders \nSET status = 'processing', updated_at = NOW()\nWHERE id = $1 AND status = 'pending';\n\nCOMMIT;\nINTL:END\n\nPATCH f0858 [target=f0858]\n  INTENT \"add row-level locking to prevent concurrent order updates\"\n  ADD BEFORE UPDATE: FOR UPDATE\nEND PATCH f0858", "completion": "INTL:BEGIN\nBEGIN TRANSACTION;\n\nSELECT id FROM orders \nWHERE id = $1 AND status = 'pending'\nFOR UPDATE;\n\nUPDATE orders \nSET status = 'processing', updated_at = NOW()\nWHERE id = $1 AND status = 'pending';\n\nCOMMIT;\nINTL:END", "metadata": {"category": "B", "adapter": "sql_postgres", "construct": "LOCK", "idx": 858}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION aggregate_daily_metrics [id=f1548]\n  INTENT       \"compute and persist daily aggregated analytics metrics from hourly event logs\"\n  PRECONDITION aggregation_date IS NOT NULL\n  PRECONDITION batch_size > 0\n  POSTCONDITION aggregated_count >= 0\n  READS        hourly_events\n  READS        event_categories\n  MUTATES      daily_metrics\n  OBSERVABLE\n\n  events = QUERY hourly_events WHERE event_date == aggregation_date\n  aggregated_count = 0\n  LOOP event IN events DO\n    category = QUERY event_categories WHERE id == event.category_id LIMIT 1\n    IF category IS NULL THEN CONTINUE\n    metric_key = CONCAT(aggregation_date, \"_\", category.name)\n    existing = QUERY daily_metrics WHERE metric_key == metric_key LIMIT 1\n    IF existing IS NOT NULL THEN\n      PERSIST daily_metrics (metric_key: metric_key, event_count: existing.event_count + event.count, last_updated: now())\n    ELSE\n      PERSIST daily_metrics (metric_key: metric_key, event_count: event.count, created_at: now(), last_updated: now())\n    END IF\n    aggregated_count = aggregated_count + event.count\n  END LOOP\n  LOG INFO \"daily_metrics aggregation complete for date=%s, total_events=%d\" (aggregation_date, aggregated_count)\n  RETURN AggregationResult(date: aggregation_date, events_processed: aggregated_count)\nEND FUNCTION aggregate_daily_metrics [id=f1548]", "completion": "# ═══ INTL:BEGIN [id=f1548] aggregate_daily_metrics ═══\nimport logging\nfrom datetime import datetime\nfrom typing import Optional\n\nlogger = logging.getLogger(__name__)\n\nasync def aggregate_daily_metrics(aggregation_date: str, batch_size: int) -> AggregationResult:\n    assert aggregation_date is not None, \"aggregation_date must not be null\"\n    assert batch_size > 0, \"batch_size must be greater than 0\"\n    \n    events = await db.query(hourly_events).filter(event_date=aggregation_date).all()\n    aggregated_count = 0\n    \n    for event in events:\n        category = await db.query(event_categories).filter(id=event.category_id).first()\n        if category is None:\n            continue\n        \n        metric_key = f\"{aggregation_date}_{category.name}\"\n        existing = await db.query(daily_metrics).filter(metric_key=metric_key).first()\n        \n        if existing is not None:\n            await db.update(\n                daily_metrics,\n                where={\"metric_key\": metric_key},\n                values={\n                    \"event_count\": existing.event_count + event.count,\n                    \"last_updated\": datetime.utcnow()\n                }\n            )\n        else:\n            await db.insert(\n                daily_metrics,\n                metric_key=metric_key,\n                event_count=event.count,\n                created_at=datetime.utcnow(),\n                last_updated=datetime.utcnow()\n            )\n        \n        aggregated_count += event.count\n    \n    logger.info(\"daily_metrics aggregation complete for date=%s, total_events=%d\", \n                aggregation_date, aggregated_count)\n    \n    result = AggregationResult(date=aggregation_date, events_processed=aggregated_count)\n    assert result.events_processed >= 0\n    return result\n# ═══ INTL:END   [id=f1548] aggregate_daily_metrics ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "LOOP", "idx": 1548}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION aggregate_review_sentiment [id=f0169]\n  INTENT       \"transform raw reviews into sentiment-scored aggregates grouped by product\"\n  PRECONDITION reviews_count > 0\n  PRECONDITION min_confidence >= 0.0\n  PRECONDITION min_confidence <= 1.0\n  POSTCONDITION result.total_reviews > 0\n  READS        reviews_table\n  READS        sentiment_scores_table\n  MUTATES      product_sentiment_cache\n  OBSERVABLE\n\n  raw_reviews = QUERY reviews_table WHERE rating > 0 LIMIT reviews_count\n  IF raw_reviews IS NULL THEN FAIL ValidationError(\"no_reviews_found\")\n  \n  TRANSFORM raw_reviews INTO sentiment_data:\n    FOR EACH review IN raw_reviews:\n      score = QUERY sentiment_scores_table WHERE review_id == review.id\n      IF score.confidence < min_confidence THEN SKIP\n      sentiment_data += {\n        product_id: review.product_id,\n        sentiment_value: score.value,\n        confidence: score.confidence,\n        review_text: review.text,\n        reviewer_id: review.reviewer_id,\n        timestamp: review.created_at\n      }\n  \n  aggregated = GROUP sentiment_data BY product_id AGGREGATE {\n    avg_sentiment: MEAN(sentiment_value),\n    confidence_weighted: SUM(sentiment_value * confidence) / SUM(confidence),\n    review_count: COUNT(*),\n    last_updated: NOW()\n  }\n  \n  PERSIST product_sentiment_cache FROM aggregated\n  RETURN AggregationResult(processed: COUNT(aggregated), status: \"success\")\nEND FUNCTION aggregate_review_sentiment [id=f0169]", "completion": "# ═══ INTL:BEGIN [id=f0169] aggregate_review_sentiment ═══\nimport logging\nfrom datetime import datetime\nfrom typing import List, Dict, Any\n\nlogger = logging.getLogger(__name__)\n\nasync def aggregate_review_sentiment(reviews_count: int, min_confidence: float) -> AggregationResult:\n    assert reviews_count > 0, \"reviews_count must be greater than 0\"\n    assert min_confidence >= 0.0, \"min_confidence must be >= 0.0\"\n    assert min_confidence <= 1.0, \"min_confidence must be <= 1.0\"\n    \n    raw_reviews = await db.query(reviews_table).filter(rating__gt=0).limit(reviews_count).all()\n    if not raw_reviews:\n        raise ValidationError(\"no_reviews_found\")\n    \n    sentiment_data: List[Dict[str, Any]] = []\n    for review in raw_reviews:\n        score = await db.query(sentiment_scores_table).filter(review_id=review.id).first()\n        if score is None or score.confidence < min_confidence:\n            continue\n        sentiment_data.append({\n            \"product_id\": review.product_id,\n            \"sentiment_value\": score.value,\n            \"confidence\": score.confidence,\n            \"review_text\": review.text,\n            \"reviewer_id\": review.reviewer_id,\n            \"timestamp\": review.created_at\n        })\n    \n    aggregated = {}\n    for item in sentiment_data:\n        product_id = item[\"product_id\"]\n        if product_id not in aggregated:\n            aggregated[product_id] = {\n                \"sentiment_values\": [],\n                \"weighted_sum\": 0.0,\n                \"confidence_sum\": 0.0,\n                \"review_count\": 0\n            }\n        aggregated[product_id][\"sentiment_values\"].append(item[\"sentiment_value\"])\n        aggregated[product_id][\"weighted_sum\"] += item[\"sentiment_value\"] * item[\"confidence\"]\n        aggregated[product_id][\"confidence_sum\"] += item[\"confidence\"]\n        aggregated[product_id][\"review_count\"] += 1\n    \n    for product_id, data in aggregated.items():\n        avg_sentiment = sum(data[\"sentiment_values\"]) / len(data[\"sentiment_values\"])\n        confidence_weighted = data[\"weighted_sum\"] / data[\"confidence_sum\"] if data[\"confidence_sum\"] > 0 else 0.0\n        \n        await db.upsert(\n            product_sentiment_cache,\n            keys={\"product_id\": product_id},\n            values={\n                \"avg_sentiment\": avg_sentiment,\n                \"confidence_weighted\": confidence_weighted,\n                \"review_count\": data[\"review_count\"],\n                \"last_updated\": datetime.utcnow()\n            }\n        )\n    \n    processed_count = len(aggregated)\n    logger.info(\"aggregate_review_sentiment: processed=%d products, total_items=%d\", processed_count, len(sentiment_data))\n    \n    result = AggregationResult(processed=processed_count, status=\"success\")\n    assert result.processed > 0, \"at least one product must be aggregated\"\n    return result\n# ═══ INTL:END   [id=f0169] aggregate_review_sentiment ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "TRANSFORM", "idx": 169}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION get_product_review_score [id=f0859]\n  INTENT       \"retrieve cached review aggregation score for product, compute if missing\"\n  PRECONDITION product_id > 0\n  PRECONDITION min_rating >= 1\n  PRECONDITION min_rating <= 5\n  POSTCONDITION result.score >= 0.0\n  POSTCONDITION result.score <= 5.0\n  READS        reviews_table\n  READS        review_cache_table\n  MUTATES      review_cache_table\n  OBSERVABLE\n\n  cached_score = CACHE GET review_cache_table WHERE product_id == product_id\n  IF cached_score IS NOT NULL AND cached_score.expires_at > now() THEN\n    RETURN CachedReviewScore(score: cached_score.score, is_cached: true)\n  END IF\n  \n  avg_rating = QUERY reviews_table WHERE product_id == product_id AND rating >= min_rating\n               AGGREGATE AVG(rating)\n  \n  IF avg_rating IS NULL THEN\n    avg_rating = 0.0\n  END IF\n  \n  PERSIST review_cache_table (\n    product_id: product_id,\n    score: avg_rating,\n    expires_at: now() + 7d,\n    updated_at: now()\n  )\n  \n  RETURN CachedReviewScore(score: avg_rating, is_cached: false)\nEND FUNCTION get_product_review_score [id=f0859]", "completion": "# ═══ INTL:BEGIN [id=f0859] get_product_review_score ═══\nimport logging\nfrom datetime import datetime, timedelta\n\nlogger = logging.getLogger(__name__)\n\nasync def get_product_review_score(product_id: int, min_rating: int) -> CachedReviewScore:\n    assert product_id > 0, \"product_id must be greater than 0\"\n    assert min_rating >= 1, \"min_rating must be at least 1\"\n    assert min_rating <= 5, \"min_rating must be at most 5\"\n    \n    cached_score = await db.query(review_cache_table).filter(product_id=product_id).first()\n    \n    if cached_score is not None and cached_score.expires_at > datetime.utcnow():\n        logger.info(\"get_product_review_score: cache_hit product_id=%d score=%.2f\", product_id, cached_score.score)\n        return CachedReviewScore(score=cached_score.score, is_cached=True)\n    \n    avg_rating = await db.query(reviews_table).filter(\n        product_id=product_id,\n        rating__gte=min_rating\n    ).aggregate(avg_rating=db.Avg('rating')).first()\n    \n    computed_score = avg_rating.get('avg_rating') or 0.0\n    \n    await db.insert_or_update(\n        review_cache_table,\n        filter={\"product_id\": product_id},\n        values={\n            \"product_id\": product_id,\n            \"score\": computed_score,\n            \"expires_at\": datetime.utcnow() + timedelta(days=7),\n            \"updated_at\": datetime.utcnow()\n        }\n    )\n    \n    logger.info(\"get_product_review_score: cache_miss product_id=%d score=%.2f\", product_id, computed_score)\n    \n    result = CachedReviewScore(score=computed_score, is_cached=False)\n    assert result.score >= 0.0, \"score must be non-negative\"\n    assert result.score <= 5.0, \"score must not exceed 5.0\"\n    return result\n# ═══ INTL:END   [id=f0859] get_product_review_score ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "CACHE GET", "idx": 859}}
{"system": "You are the INTL compiler for Sql Postgres. Given an INTL block, produce idiomatic, production-ready Sql Postgres code. Wrap output in INTL:BEGIN / INTL:END sentinels with the correct block ID.", "prompt": "FUNCTION update_user_profile [id=f0537]\n  INTENT       \"validate and update user profile information with audit logging\"\n  PRECONDITION user_id > 0\n  PRECONDITION full_name.length > 0\n  PRECONDITION full_name.length <= 255\n  PRECONDITION phone_number IS NULL OR phone_number.length >= 10\n  POSTCONDITION result.updated_at > BEFORE\n  READS        users_table\n  MUTATES      users_table, audit_log_table\n  OBSERVABLE\n\n  VALIDATE full_name MATCHES /^[a-zA-Z\\s'-]+$/ ELSE FAIL ValidationError(\"invalid_name_format\")\n  VALIDATE phone_number IS NULL OR phone_number MATCHES /^\\+?[0-9\\-\\(\\) ]{10,}$/ ELSE FAIL ValidationError(\"invalid_phone_format\")\n  \n  user = QUERY users_table WHERE id == user_id LIMIT 1\n  IF user IS NULL THEN FAIL NotFoundError(\"user_not_found\")\n  \n  old_name = user.full_name\n  old_phone = user.phone_number\n  \n  UPDATE users_table SET full_name = full_name, phone_number = phone_number, updated_at = now() WHERE id = user_id\n  PERSIST audit_log_table (user_id: user_id, action: \"profile_update\", old_values: {full_name: old_name, phone_number: old_phone}, new_values: {full_name: full_name, phone_number: phone_number}, timestamp: now())\n  \n  updated_user = QUERY users_table WHERE id == user_id LIMIT 1\n  RETURN UserProfile(id: updated_user.id, full_name: updated_user.full_name, phone_number: updated_user.phone_number, updated_at: updated_user.updated_at)\nEND FUNCTION update_user_profile [id=f0537]", "completion": "# ═══ INTL:BEGIN [id=f0537] update_user_profile ═══\nimport logging\nimport re\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nasync def update_user_profile(user_id: int, full_name: str, phone_number: str = None) -> UserProfile:\n    assert user_id > 0, \"user_id must be greater than 0\"\n    assert len(full_name) > 0, \"full_name must not be empty\"\n    assert len(full_name) <= 255, \"full_name must not exceed 255 characters\"\n    assert phone_number is None or len(phone_number) >= 10, \"phone_number must be at least 10 characters\"\n    \n    if not re.match(r\"^[a-zA-Z\\s'-]+$\", full_name):\n        raise ValidationError(\"invalid_name_format\")\n    \n    if phone_number is not None and not re.match(r\"^\\+?[0-9\\-\\(\\) ]{10,}$\", phone_number):\n        raise ValidationError(\"invalid_phone_format\")\n    \n    user = await db.query(users_table).filter(id=user_id).first()\n    if user is None:\n        raise NotFoundError(\"user_not_found\")\n    \n    old_name = user.full_name\n    old_phone = user.phone_number\n    \n    await db.update(users_table).filter(id=user_id).set(\n        full_name=full_name,\n        phone_number=phone_number,\n        updated_at=datetime.utcnow()\n    )\n    \n    await db.insert(audit_log_table,\n                    user_id=user_id,\n                    action=\"profile_update\",\n                    old_values={\"full_name\": old_name, \"phone_number\": old_phone},\n                    new_values={\"full_name\": full_name, \"phone_number\": phone_number},\n                    timestamp=datetime.utcnow())\n    \n    logger.info(\"update_user_profile: user_id=%s, old_name=%s, new_name=%s\", user_id, old_name, full_name)\n    \n    updated_user = await db.query(users_table).filter(id=user_id).first()\n    result = UserProfile(id=updated_user.id, full_name=updated_user.full_name, \n                        phone_number=updated_user.phone_number, updated_at=updated_user.updated_at)\n    assert result.updated_at > user.updated_at\n    return result\n# ═══ INTL:END   [id=f0537] update_user_profile ═══", "metadata": {"category": "A", "adapter": "sql_postgres", "construct": "VALIDATE", "idx": 537}}
{"system": "You are the INTL compiler for Sql Postgres in PATCH mode. Apply the PATCH block to the existing code with minimal diff. Wrap output in INTL:BEGIN / INTL:END sentinels.", "prompt": "INTL:BEGIN\nCREATE FUNCTION send_notification(user_id INT, message TEXT)\nRETURNS TABLE(success BOOLEAN, notification_id INT) AS $$\nBEGIN\n  INSERT INTO notifications (user_id, message, created_at)\n  VALUES (user_id, message, NOW())\n  RETURNING id INTO notification_id;\n  \n  RETURN QUERY SELECT TRUE, notification_id;\nEND;\n$$ LANGUAGE plpgsql;\nINTL:END\n\nPATCH f0538 [target=f0538]\n  INTENT \"add notification status field to return result set\"\n  MODIFY RETURN: append status column from notifications table to query result\nEND PATCH f0538", "completion": "INTL:BEGIN\nCREATE FUNCTION send_notification(user_id INT, message TEXT)\nRETURNS TABLE(success BOOLEAN, notification_id INT, status TEXT) AS $$\nBEGIN\n  INSERT INTO notifications (user_id, message, created_at)\n  VALUES (user_id, message, NOW())\n  RETURNING id INTO notification_id;\n  \n  RETURN QUERY SELECT TRUE, notification_id, n.status FROM notifications n WHERE n.id = notification_id;\nEND;\n$$ LANGUAGE plpgsql;\nINTL:END", "metadata": {"category": "B", "adapter": "sql_postgres", "construct": "RETURN", "idx": 538}}
